python3   -m pytest   -m gpu 
============================= test session starts ==============================
platform linux -- Python 3.10.8, pytest-7.2.0, pluggy-1.0.0
rootdir: /home/shie44167/workspace/1_read_codes/composer, configfile: pyproject.toml
plugins: anyio-3.6.2, httpserver-1.0.6, pytest_codeblocks-0.16.1, cov-4.0.0
collected 3135 items / 2892 deselected / 243 selected

README.md .                                                              [  0%]
composer/algorithms/alibi/README.md .                                    [  0%]
composer/algorithms/augmix/README.md .                                   [  1%]
composer/algorithms/blurpool/README.md .                                 [  1%]
composer/algorithms/channels_last/README.md .                            [  2%]
composer/algorithms/colout/README.md .                                   [  2%]
composer/algorithms/cutmix/README.md .                                   [  2%]
composer/algorithms/cutout/README.md .                                   [  3%]
composer/algorithms/ema/README.md .                                      [  3%]
composer/algorithms/factorize/README.md .                                [  4%]
composer/algorithms/fused_layernorm/README.md F                          [  4%]
composer/algorithms/gated_linear_units/README.md .                       [  4%]
composer/algorithms/ghost_batchnorm/README.md .                          [  5%]
composer/algorithms/label_smoothing/README.md .                          [  5%]
composer/algorithms/layer_freezing/README.md .                           [  6%]
composer/algorithms/low_precision_layernorm/README.md .                  [  6%]
composer/algorithms/mixup/README.md ..                                   [  7%]
composer/algorithms/progressive_resizing/README.md .                     [  7%]
composer/algorithms/randaugment/README.md .                              [  8%]
composer/algorithms/seq_length_warmup/README.md .                        [  8%]
composer/algorithms/squeeze_excite/README.md .                           [  9%]
composer/algorithms/stochastic_depth/README.md ..                        [  9%]
composer/algorithms/swa/README.md .                                      [ 10%]
composer/algorithms/weight_standardization/README.md ..                  [ 11%]
docs/source/method_cards/alibi.md .                                      [ 11%]
docs/source/method_cards/augmix.md .                                     [ 11%]
docs/source/method_cards/blurpool.md .                                   [ 12%]
docs/source/method_cards/channels_last.md .                              [ 12%]
docs/source/method_cards/colout.md .                                     [ 13%]
docs/source/method_cards/cutmix.md .                                     [ 13%]
docs/source/method_cards/cutout.md .                                     [ 13%]
docs/source/method_cards/decoupled_weight_decay.md .                     [ 14%]
docs/source/method_cards/ema.md .                                        [ 14%]
docs/source/method_cards/factorize.md .                                  [ 15%]
docs/source/method_cards/fused_layernorm.md F                            [ 15%]
docs/source/method_cards/gated_linear_units.md .                         [ 16%]
docs/source/method_cards/ghost_batchnorm.md .                            [ 16%]
docs/source/method_cards/label_smoothing.md .                            [ 16%]
docs/source/method_cards/layer_freezing.md .                             [ 17%]
docs/source/method_cards/low_precision_layernorm.md .                    [ 17%]
docs/source/method_cards/mixup.md ..                                     [ 18%]
docs/source/method_cards/progressive_resizing.md .                       [ 18%]
docs/source/method_cards/randaugment.md .                                [ 19%]
docs/source/method_cards/seq_length_warmup.md .                          [ 19%]
docs/source/method_cards/squeeze_excite.md .                             [ 20%]
docs/source/method_cards/stochastic_depth.md ..                          [ 20%]
docs/source/method_cards/stochastic_depth_samplewise.md ..               [ 21%]
docs/source/method_cards/swa.md .                                        [ 22%]
docs/source/method_cards/weight_standardization.md ..                    [ 23%]
docs/source/notes/numerics.md .                                          [ 23%]
tests/test_device.py ......                                              [ 25%]
tests/test_docker.py s                                                   [ 26%]
tests/test_events.py F.F.                                                [ 27%]
tests/test_notebooks.py sF..F.F.FFx.sE                                   [ 33%]
tests/test_precision.py FFFF.F                                           [ 36%]
tests/algorithms/test_algorithm_resumption.py x........F...x.x.....x.x.x [ 46%]
..                                                                       [ 47%]
tests/algorithms/test_algorithms_train.py x........F...x.x.......x....   [ 59%]
tests/algorithms/test_channels_last.py .                                 [ 59%]
tests/algorithms/test_fused_layernorm.py FF                              [ 60%]
tests/algorithms/test_low_precision_layernorm.py ..                      [ 61%]
tests/callbacks/test_early_stopper.py ....                               [ 62%]
tests/callbacks/test_memory_monitor.py ..                                [ 63%]
tests/callbacks/test_mlperf_callback.py ssss                             [ 65%]
tests/callbacks/test_threshold_stopper.py ....                           [ 67%]
tests/models/test_efficientnet.py .                                      [ 67%]
tests/trainer/test_checkpoint.py ..sss.....FF..FF..FF..FF..FF..FF.FFF    [ 82%]
tests/trainer/test_ddp.py .F.                                            [ 83%]
tests/trainer/test_predict.py ..                                         [ 84%]
tests/trainer/test_trainer.py ..FFFF...F.....F..................E        [ 98%]
tests/utils/test_inference.py F.                                         [ 99%]
tests/utils/test_module_surgery.py .                                     [100%]

==================================== ERRORS ====================================
_ ERROR at setup of test_notebook[gpu-/home/shie44167/workspace/1_read_codes/composer/composer/../examples/training_without_local_storage.ipynb] _

request = <SubRequest 's3_bucket' for <Function test_notebook[gpu-/home/shie44167/workspace/1_read_codes/composer/composer/../examples/training_without_local_storage.ipynb]>>

    @pytest.fixture
    def s3_bucket(request: pytest.FixtureRequest):
        if request.node.get_closest_marker('remote') is None:
            return 'my-bucket'
        else:
>           return _get_option(request.config, 's3_bucket')

/home/shie44167/workspace/1_read_codes/composer/tests/conftest.py:161: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

config = <_pytest.config.Config object at 0x7f69a8278820>, name = 's3_bucket'
default = None

    def _get_option(config: pytest.Config, name: str, default: Optional[str] = None) -> str:
        val = config.getoption(name)
        if val is not None:
            assert isinstance(val, str)
            return val
        val = config.getini(name)
        if val == []:
            val = None
        if val is None:
            if default is None:
>               pytest.fail(f'Config option {name} is not specified but is required')
E               Failed: Config option s3_bucket is not specified but is required

/home/shie44167/workspace/1_read_codes/composer/tests/conftest.py:57: Failed
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
___________ ERROR at setup of TestFFCVDataloaders.test_ffcv[gpu-amp] ___________

self = <tests.trainer.test_trainer.TestFFCVDataloaders object at 0x7f69217d0400>
tmp_path_factory = TempPathFactory(_given_basetemp=None, _trace=<pluggy._tracing.TagTracerSub object at 0x7f6927fdc730>, _basetemp=PosixPath('/tmp/pytest-of-shie44167/pytest-3'))

    @pytest.fixture(autouse=True)
    def create_dataset(self, tmp_path_factory: pytest.TempPathFactory):
        dataset_train = RandomImageDataset(size=16, is_PIL=True)
        self.tmp_path = tmp_path_factory.mktemp('ffcv')
        output_train_file = str(self.tmp_path / 'train.ffcv')
>       write_ffcv_dataset(dataset_train, write_path=output_train_file, num_workers=1, write_mode='proportion')

/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_trainer.py:1092: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/composer/datasets/ffcv_utils.py:66: in write_ffcv_dataset
    _require_ffcv()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _require_ffcv():
        if not ffcv_installed:
>           raise MissingConditionalImportError(extra_deps_group='ffcv', conda_package='ffcv')
E           composer.utils.import_helpers.MissingConditionalImportError: Composer was installed without ffcv support. To use ffcv related packages, with Composer, run `pip install 'mosaicml[ffcv]'` if using pip or `conda install -c conda-forge ffcv` if using Anaconda.

/home/shie44167/workspace/1_read_codes/composer/composer/datasets/ffcv_utils.py:25: MissingConditionalImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
=================================== FAILURES ===================================
_______________________________ code block check _______________________________
line 42, line 42:
```
from tests.fixtures.synthetic_hf_state import make_dataset_configs, synthetic_hf_state_maker

synthetic_config = make_dataset_configs(model_family=['bert'])[0]
_, model, train_dataloader = synthetic_hf_state_maker(synthetic_config)
_, _, eval_dataloader = synthetic_hf_state_maker(synthetic_config)
from composer.trainer import Trainer
from composer.algorithms import FusedLayerNorm

trainer = Trainer(model=model,
                  train_dataloader=train_dataloader,
                  eval_dataloader=eval_dataloader,
                  max_duration='1ep',
                  algorithms=[FusedLayerNorm()])

trainer.fit()
```

https://github.com/NVIDIA/apex is not installed. The Fused LayerNorm algorithm cannot be applied. The MosaicML Docker Images (https://hub.docker.com/r/mosaicml/pytorch) contain a copy of APEX for easy use.
----------------------------- Captured stdout call -----------------------------












----------------------------- Captured stderr call -----------------------------
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00, 23.04ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00, 24.46ba/s]
_______________________________ code block check _______________________________
line 42, line 42:
```
from tests.fixtures.synthetic_hf_state import make_dataset_configs, synthetic_hf_state_maker

synthetic_config = make_dataset_configs(model_family=['bert'])[0]
_, model, train_dataloader = synthetic_hf_state_maker(synthetic_config)
_, _, eval_dataloader = synthetic_hf_state_maker(synthetic_config)
from composer.trainer import Trainer
from composer.algorithms import FusedLayerNorm

trainer = Trainer(model=model,
                  train_dataloader=train_dataloader,
                  eval_dataloader=eval_dataloader,
                  max_duration='1ep',
                  algorithms=[FusedLayerNorm()])

trainer.fit()
```

https://github.com/NVIDIA/apex is not installed. The Fused LayerNorm algorithm cannot be applied. The MosaicML Docker Images (https://hub.docker.com/r/mosaicml/pytorch) contain a copy of APEX for easy use.
----------------------------- Captured stdout call -----------------------------












----------------------------- Captured stderr call -----------------------------
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00, 24.49ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00,  8.38ba/s]100%|██████████| 1/1 [00:00<00:00,  8.37ba/s]
________________ TestEventCalls.test_event_calls[1ep-gpu-ddp-1] ________________

self = <composer.trainer.trainer.Trainer object at 0x7f685dcdbd00>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
>               import deepspeed
E               ModuleNotFoundError: No module named 'deepspeed'

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1159: ModuleNotFoundError

The above exception was the direct cause of the following exception:

self = <tests.test_events.TestEventCalls object at 0x7f692738fa00>
world_size = 1, device = 'gpu', deepspeed_zero_stage = True, use_fsdp = False
save_interval = Time(1, TimeUnit.EPOCH)

    @pytest.mark.parametrize('world_size', [
        pytest.param(1),
        pytest.param(2, marks=pytest.mark.world_size(2)),
    ])
    @pytest.mark.parametrize('device,deepspeed_zero_stage,use_fsdp', [
        pytest.param('cpu', None, False, id='cpu-ddp'),
        pytest.param('gpu', True, False, id='gpu-ddp', marks=pytest.mark.gpu),
        pytest.param('gpu',
                     None,
                     True,
                     id='gpu-fsdp',
                     marks=[
                         pytest.mark.gpu,
                         pytest.mark.skipif(version.parse(torch.__version__) < version.parse('1.12.0'),
                                            reason='requires PyTorch 1.12 or higher')
                     ]),
    ])
    @pytest.mark.parametrize('save_interval', ['1ep', '1ba'])
    def test_event_calls(self, world_size, device, deepspeed_zero_stage, use_fsdp, save_interval):
        save_interval = Time.from_timestring(save_interval)
    
        deepspeed_config = None
        if deepspeed_zero_stage:
            deepspeed_config = {'zero_optimization': {'stage': deepspeed_zero_stage}}
    
        fsdp_config = None
        if use_fsdp:
            fsdp_config = {
                'sharding_strategy': 'FULL_SHARD',
                'min_params': 1e8,
                'cpu_offload': False,
                'mixed_precision': 'DEFAULT',
                'backward_prefetch': 'BACKWARD_PRE',
                'activation_checkpointing': False,
                'activation_ocpu_offload': False,
                'verbose': False
            }
    
>       trainer = self.get_trainer(
            device=device,
            deepspeed_config=deepspeed_config,
            fsdp_config=fsdp_config,
            save_interval=save_interval,
            eval_interval=save_interval,
        )

/home/shie44167/workspace/1_read_codes/composer/tests/test_events.py:94: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/tests/test_events.py:34: in get_trainer
    return Trainer(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.trainer.trainer.Trainer object at 0x7f685dcdbd00>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
                import deepspeed
            except ImportError as e:
>               raise MissingConditionalImportError(
                    extra_deps_group='deepspeed',
                    conda_package='deepspeed>=0.5.5',
                    conda_channel=None,
                ) from e
E               composer.utils.import_helpers.MissingConditionalImportError: Composer was installed without deepspeed support. To use deepspeed related packages, with Composer, run `pip install 'mosaicml[deepspeed]'` if using pip or `pip install deepspeed>=0.5.5` if using Anaconda.

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1161: MissingConditionalImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
------------------------------ Captured log call -------------------------------
DEBUG    composer.utils.dist:dist.py:409 Initializing torch.dist: global_rank=0, local_rank=0, world_size=1, local_world_size=1, node_rank=0
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669808883-quaint-muskrat
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
________________ TestEventCalls.test_event_calls[1ba-gpu-ddp-1] ________________

self = <composer.trainer.trainer.Trainer object at 0x7f685c294220>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
>               import deepspeed
E               ModuleNotFoundError: No module named 'deepspeed'

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1159: ModuleNotFoundError

The above exception was the direct cause of the following exception:

self = <tests.test_events.TestEventCalls object at 0x7f692738fd60>
world_size = 1, device = 'gpu', deepspeed_zero_stage = True, use_fsdp = False
save_interval = Time(1, TimeUnit.BATCH)

    @pytest.mark.parametrize('world_size', [
        pytest.param(1),
        pytest.param(2, marks=pytest.mark.world_size(2)),
    ])
    @pytest.mark.parametrize('device,deepspeed_zero_stage,use_fsdp', [
        pytest.param('cpu', None, False, id='cpu-ddp'),
        pytest.param('gpu', True, False, id='gpu-ddp', marks=pytest.mark.gpu),
        pytest.param('gpu',
                     None,
                     True,
                     id='gpu-fsdp',
                     marks=[
                         pytest.mark.gpu,
                         pytest.mark.skipif(version.parse(torch.__version__) < version.parse('1.12.0'),
                                            reason='requires PyTorch 1.12 or higher')
                     ]),
    ])
    @pytest.mark.parametrize('save_interval', ['1ep', '1ba'])
    def test_event_calls(self, world_size, device, deepspeed_zero_stage, use_fsdp, save_interval):
        save_interval = Time.from_timestring(save_interval)
    
        deepspeed_config = None
        if deepspeed_zero_stage:
            deepspeed_config = {'zero_optimization': {'stage': deepspeed_zero_stage}}
    
        fsdp_config = None
        if use_fsdp:
            fsdp_config = {
                'sharding_strategy': 'FULL_SHARD',
                'min_params': 1e8,
                'cpu_offload': False,
                'mixed_precision': 'DEFAULT',
                'backward_prefetch': 'BACKWARD_PRE',
                'activation_checkpointing': False,
                'activation_ocpu_offload': False,
                'verbose': False
            }
    
>       trainer = self.get_trainer(
            device=device,
            deepspeed_config=deepspeed_config,
            fsdp_config=fsdp_config,
            save_interval=save_interval,
            eval_interval=save_interval,
        )

/home/shie44167/workspace/1_read_codes/composer/tests/test_events.py:94: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/tests/test_events.py:34: in get_trainer
    return Trainer(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.trainer.trainer.Trainer object at 0x7f685c294220>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
                import deepspeed
            except ImportError as e:
>               raise MissingConditionalImportError(
                    extra_deps_group='deepspeed',
                    conda_package='deepspeed>=0.5.5',
                    conda_channel=None,
                ) from e
E               composer.utils.import_helpers.MissingConditionalImportError: Composer was installed without deepspeed support. To use deepspeed related packages, with Composer, run `pip install 'mosaicml[deepspeed]'` if using pip or `pip install deepspeed>=0.5.5` if using Anaconda.

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1161: MissingConditionalImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669808883-dramatic-penguin
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
_ test_notebook[gpu-/home/shie44167/workspace/1_read_codes/composer/composer/../examples/auto_grad_accum.ipynb] _

self = <testbook.client.TestbookNotebookClient object at 0x7f683c64b7c0>
cell = [9], kwargs = {}, cell_indexes = [9], executed_cells = [], idx = 9

    def execute_cell(self, cell, **kwargs) -> Union[Dict, List[Dict]]:
        """
        Executes a cell or list of cells
        """
        if isinstance(cell, slice):
            start, stop = self._cell_index(cell.start), self._cell_index(cell.stop)
            if cell.step is not None:
                raise TestbookError('testbook does not support step argument')
    
            cell = range(start, stop + 1)
        elif isinstance(cell, str) or isinstance(cell, int):
            cell = [cell]
    
        cell_indexes = cell
    
        if all(isinstance(x, str) for x in cell):
            cell_indexes = [self._cell_index(tag) for tag in cell]
    
        executed_cells = []
        for idx in cell_indexes:
            try:
>               cell = super().execute_cell(self.nb['cells'][idx], idx, **kwargs)

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/testbook/client.py:133: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<testbook.client.TestbookNotebookClient object at 0x7f683c64b7c0>, {'cell_type': 'code', 'execution_count': 6, 'metad...1ep",\n    grad_accum=\'auto\',  # <--- Activate Composer magic!\n    device=\'gpu\'\n)\n\n# Train\ntrainer.fit()'}, 9)
kwargs = {}, name = 'MainThread'
inner = <coroutine object NotebookClient.async_execute_cell at 0x7f6796f0b7d0>
loop = <_UnixSelectorEventLoop running=False closed=False debug=False>

    def wrapped(*args, **kwargs):
        name = threading.current_thread().name
        inner = coro(*args, **kwargs)
        try:
            # If a loop is currently running in this thread,
            # use a task runner.
            asyncio.get_running_loop()
            if name not in _runner_map:
                _runner_map[name] = _TaskRunner()
            return _runner_map[name].run(inner)
        except RuntimeError:
            pass
    
        # Run the loop for this thread.
        if name not in _loop_map:
            _loop_map[name] = asyncio.new_event_loop()
        loop = _loop_map[name]
>       return loop.run_until_complete(inner)

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/jupyter_core/utils/__init__.py:160: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_UnixSelectorEventLoop running=False closed=False debug=False>
future = <Task finished name='Task-24' coro=<NotebookClient.async_execute_cell() done, defined at /home/shie44167/miniconda3/en...meError\x1b[0m: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED\nRuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED\n')>

    def run_until_complete(self, future):
        """Run until the Future is done.
    
        If the argument is a coroutine, it is wrapped in a Task.
    
        WARNING: It would be disastrous to call run_until_complete()
        with the same coroutine twice -- it would wrap it in two
        different Tasks and that can't be good.
    
        Return the Future's result, or raise its exception.
        """
        self._check_closed()
        self._check_running()
    
        new_task = not futures.isfuture(future)
        future = tasks.ensure_future(future, loop=self)
        if new_task:
            # An exception is raised if the future didn't complete, so there
            # is no need to log the "destroy pending task" message
            future._log_destroy_pending = False
    
        future.add_done_callback(_run_until_complete_cb)
        try:
            self.run_forever()
        except:
            if new_task and future.done() and not future.cancelled():
                # The coroutine raised a BaseException. Consume the exception
                # to not log a warning, the caller doesn't have access to the
                # local task.
                future.exception()
            raise
        finally:
            future.remove_done_callback(_run_until_complete_cb)
        if not future.done():
            raise RuntimeError('Event loop stopped before Future completed.')
    
>       return future.result()

/home/shie44167/miniconda3/envs/composer/lib/python3.10/asyncio/base_events.py:649: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <testbook.client.TestbookNotebookClient object at 0x7f683c64b7c0>
cell = {'cell_type': 'code', 'execution_count': 6, 'metadata': {'execution': {'iopub.status.busy': '2022-11-30T11:48:25.53724...on="1ep",\n    grad_accum=\'auto\',  # <--- Activate Composer magic!\n    device=\'gpu\'\n)\n\n# Train\ntrainer.fit()'}
cell_index = 9, execution_count = None, store_history = True

    async def async_execute_cell(
        self,
        cell: NotebookNode,
        cell_index: int,
        execution_count: t.Optional[int] = None,
        store_history: bool = True,
    ) -> NotebookNode:
        """
        Executes a single code cell.
    
        To execute all cells see :meth:`execute`.
    
        Parameters
        ----------
        cell : nbformat.NotebookNode
            The cell which is currently being processed.
        cell_index : int
            The position of the cell within the notebook object.
        execution_count : int
            The execution count to be assigned to the cell (default: Use kernel response)
        store_history : bool
            Determines if history should be stored in the kernel (default: False).
            Specific to ipython kernels, which can store command histories.
    
        Returns
        -------
        output : dict
            The execution output payload (or None for no output).
    
        Raises
        ------
        CellExecutionError
            If execution failed and should raise an exception, this will be raised
            with defaults about the failure.
    
        Returns
        -------
        cell : NotebookNode
            The cell which was just processed.
        """
        assert self.kc is not None
    
        await run_hook(self.on_cell_start, cell=cell, cell_index=cell_index)
    
        if cell.cell_type != 'code' or not cell.source.strip():
            self.log.debug("Skipping non-executing cell %s", cell_index)
            return cell
    
        if self.skip_cells_with_tag in cell.metadata.get("tags", []):
            self.log.debug("Skipping tagged cell %s", cell_index)
            return cell
    
        if self.record_timing:  # clear execution metadata prior to execution
            cell['metadata']['execution'] = {}
    
        self.log.debug("Executing cell:\n%s", cell.source)
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors or "raises-exception" in cell.metadata.get("tags", [])
        )
    
        await run_hook(self.on_cell_execute, cell=cell, cell_index=cell_index)
        parent_msg_id = await ensure_async(
            self.kc.execute(
                cell.source, store_history=store_history, stop_on_error=not cell_allows_errors
            )
        )
        await run_hook(self.on_cell_complete, cell=cell, cell_index=cell_index)
        # We launched a code cell to execute
        self.code_cells_executed += 1
        exec_timeout = self._get_timeout(cell)
    
        cell.outputs = []
        self.clear_before_next_output = False
    
        task_poll_kernel_alive = asyncio.ensure_future(self._async_poll_kernel_alive())
        task_poll_output_msg = asyncio.ensure_future(
            self._async_poll_output_msg(parent_msg_id, cell, cell_index)
        )
        self.task_poll_for_reply = asyncio.ensure_future(
            self._async_poll_for_reply(
                parent_msg_id, cell, exec_timeout, task_poll_output_msg, task_poll_kernel_alive
            )
        )
        try:
            exec_reply = await self.task_poll_for_reply
        except asyncio.CancelledError:
            # can only be cancelled by task_poll_kernel_alive when the kernel is dead
            task_poll_output_msg.cancel()
            raise DeadKernelError("Kernel died")
        except Exception as e:
            # Best effort to cancel request if it hasn't been resolved
            try:
                # Check if the task_poll_output is doing the raising for us
                if not isinstance(e, CellControlSignal):
                    task_poll_output_msg.cancel()
            finally:
                raise
    
        if execution_count:
            cell['execution_count'] = execution_count
        await run_hook(
            self.on_cell_executed, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
>       await self._check_raise_for_error(cell, cell_index, exec_reply)

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/nbclient/client.py:1021: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <testbook.client.TestbookNotebookClient object at 0x7f683c64b7c0>
cell = {'cell_type': 'code', 'execution_count': 6, 'metadata': {'execution': {'iopub.status.busy': '2022-11-30T11:48:25.53724...on="1ep",\n    grad_accum=\'auto\',  # <--- Activate Composer magic!\n    device=\'gpu\'\n)\n\n# Train\ntrainer.fit()'}
cell_index = 9
exec_reply = {'buffers': [], 'content': {'ename': 'RuntimeError', 'engine_info': {'engine_id': -1, 'engine_uuid': 'f2c4b2a3-0dff-43...e, 'engine': 'f2c4b2a3-0dff-4334-9812-58879e08527b', 'started': '2022-11-30T11:48:25.537378Z', 'status': 'error'}, ...}

    async def _check_raise_for_error(
        self, cell: NotebookNode, cell_index: int, exec_reply: t.Optional[t.Dict]
    ) -> None:
    
        if exec_reply is None:
            return None
    
        exec_reply_content = exec_reply['content']
        if exec_reply_content['status'] != 'error':
            return None
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors
            or exec_reply_content.get('ename') in self.allow_error_names
            or "raises-exception" in cell.metadata.get("tags", [])
        )
        await run_hook(
            self.on_cell_error, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
        if not cell_allows_errors:
>           raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
E           nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
E           ------------------
E           assert torch.cuda.is_available(), "Demonstrating automatic gradient accumulation requires a GPU."
E           
E           trainer = composer.trainer.Trainer(
E               model=model,
E               train_dataloader=train_dataloader,
E               eval_dataloader=test_dataloader,
E               optimizers=optimizer,
E               max_duration="1ep",
E               grad_accum='auto',  # <--- Activate Composer magic!
E               device='gpu'
E           )
E           
E           # Train
E           trainer.fit()
E           ------------------
E           
E           [0;31m---------------------------------------------------------------------------[0m
E           [0;31mRuntimeError[0m                              Traceback (most recent call last)
E           [0;32m/tmp/ipykernel_312861/1804393140.py[0m in [0;36m<cell line: 14>[0;34m()[0m
E           [1;32m     12[0m [0;34m[0m[0m
E           [1;32m     13[0m [0;31m# Train[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m---> 14[0;31m [0mtrainer[0m[0;34m.[0m[0mfit[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m
E           [0;32m/tmp/ipykernel_312861/4080402710.py[0m in [0;36mnew_fit[0;34m(self, *args, **kwargs)[0m
E           [1;32m     19[0m             [0;32mif[0m [0;34m'eval_subset_num_batches'[0m [0;32mnot[0m [0;32min[0m [0mkwargs[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m     20[0m                 [0mkwargs[0m[0;34m[[0m[0;34m'eval_subset_num_batches'[0m[0;34m][0m [0;34m=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m---> 21[0;31m         [0moriginal_fit[0m[0;34m([0m[0mself[0m[0;34m,[0m [0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m     22[0m [0;34m[0m[0m
E           [1;32m     23[0m     [0mTrainer[0m[0;34m.[0m[0mfit[0m [0;34m=[0m [0mnew_fit[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36mfit[0;34m(self, train_dataloader, train_dataloader_label, train_subset_num_batches, duration, reset_time, schedulers, scale_schedule_ratio, step_schedulers_every_batch, eval_dataloader, eval_subset_num_batches, eval_interval, grad_accum, precision)[0m
E           [1;32m   1632[0m             [0;31m# update scaler since precision was provided[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1633[0m             [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mscaler[0m [0;34m=[0m [0mClosureGradScaler[0m[0;34m([0m[0;34m)[0m [0;32mif[0m [0mself[0m[0;34m.[0m[0m_use_closures[0m[0;34m([0m[0;34m)[0m [0;32melse[0m [0mGradScaler[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m-> 1634[0;31m         [0mself[0m[0;34m.[0m[0m_train_loop[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1635[0m [0;34m[0m[0m
E           [1;32m   1636[0m     [0;32mdef[0m [0mclose[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_loop[0;34m(self)[0m
E           [1;32m   1790[0m                     })
E           [1;32m   1791[0m [0;34m[0m[0m
E           [0;32m-> 1792[0;31m                     [0mtotal_loss_dict[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_train_batch[0m[0;34m([0m[0muse_grad_scaling[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1793[0m [0;34m[0m[0m
E           [1;32m   1794[0m                     [0;32mif[0m [0muse_grad_scaling[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_batch[0;34m(self, use_grad_scaling)[0m
E           [1;32m   1961[0m                                 microbatches, total_loss_dict, **kwargs).item())
E           [1;32m   1962[0m                 [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m-> 1963[0;31m                     [0mself[0m[0;34m.[0m[0m_train_microbatches[0m[0;34m([0m[0mmicrobatches[0m[0;34m,[0m [0mtotal_loss_dict[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1964[0m                     [0;32mif[0m [0;32mnot[0m [0mself[0m[0;34m.[0m[0mdeepspeed_enabled[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1965[0m                         [0;32mfor[0m [0moptimizer[0m [0;32min[0m [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0moptimizers[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_microbatches[0;34m(self, microbatches, total_loss_dict, ddp_sync)[0m
E           [1;32m   2032[0m             [0;32mfor[0m [0mmicrobatch_idx[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mbatch[0m [0;32min[0m [0menumerate[0m[0;34m([0m[0mmicrobatches[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   2033[0m                 [0mis_final_microbatch[0m [0;34m=[0m [0mmicrobatch_idx[0m [0;34m+[0m [0;36m1[0m [0;34m==[0m [0mlen[0m[0;34m([0m[0mmicrobatches[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m-> 2034[0;31m                 [0mmicrobatch_loss_dict[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_train_microbatch[0m[0;34m([0m[0muse_grad_scaling[0m[0;34m,[0m [0mcurrent_batch_size[0m[0;34m,[0m [0mis_final_microbatch[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   2035[0m [0;34m[0m[0m
E           [1;32m   2036[0m                 [0;31m# Aggregate each loss in microbatch_loss_dict into total_loss_dict[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_microbatch[0;34m(self, use_grad_scaling, current_batch_size, is_final_microbatch)[0m
E           [1;32m   2128[0m                 [0;31m# Scale loss based on the number of samples in the microbatch to maintain gradient numerics[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   2129[0m                 [0mmicrobatch_loss[0m[0;34m.[0m[0mmul_[0m[0;34m([0m[0mmicrobatch_num_samples[0m [0;34m/[0m [0mcurrent_batch_size[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m-> 2130[0;31m                 [0mmicrobatch_loss[0m[0;34m.[0m[0mbackward[0m[0;34m([0m[0mcreate_graph[0m[0;34m=[0m[0mself[0m[0;34m.[0m[0m_backwards_create_graph[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   2131[0m [0;34m[0m[0m
E           [1;32m   2132[0m             [0mself[0m[0;34m.[0m[0mengine[0m[0;34m.[0m[0mrun_event[0m[0;34m([0m[0mEvent[0m[0;34m.[0m[0mAFTER_BACKWARD[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/_tensor.py[0m in [0;36mbackward[0;34m(self, gradient, retain_graph, create_graph, inputs)[0m
E           [1;32m    394[0m                 [0mcreate_graph[0m[0;34m=[0m[0mcreate_graph[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m    395[0m                 inputs=inputs)
E           [0;32m--> 396[0;31m         [0mtorch[0m[0;34m.[0m[0mautograd[0m[0;34m.[0m[0mbackward[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mgradient[0m[0;34m,[0m [0mretain_graph[0m[0;34m,[0m [0mcreate_graph[0m[0;34m,[0m [0minputs[0m[0;34m=[0m[0minputs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m    397[0m [0;34m[0m[0m
E           [1;32m    398[0m     [0;32mdef[0m [0mregister_hook[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mhook[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/autograd/__init__.py[0m in [0;36mbackward[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)[0m
E           [1;32m    171[0m     [0;31m# some Python versions print out the first line of a multi-line function[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m    172[0m     [0;31m# calls in the traceback and some print out the last line[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m--> 173[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
E           [0m[1;32m    174[0m         [0mtensors[0m[0;34m,[0m [0mgrad_tensors_[0m[0;34m,[0m [0mretain_graph[0m[0;34m,[0m [0mcreate_graph[0m[0;34m,[0m [0minputs[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m    175[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
E           
E           [0;31mRuntimeError[0m: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED
E           RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/nbclient/client.py:915: CellExecutionError

During handling of the above exception, another exception occurred:

notebook = '/home/shie44167/workspace/1_read_codes/composer/composer/../examples/auto_grad_accum.ipynb'
device = 'gpu', s3_bucket = 'my-bucket'

    @pytest.mark.parametrize('notebook', [_to_pytest_param(notebook) for notebook in NOTEBOOKS])
    @device('cpu', 'gpu')
    @pytest.mark.daily
    def test_notebook(notebook: str, device: str, s3_bucket: str):
        trainer_monkeypatch_code = inspect.getsource(patch_notebooks)
        notebook_name = os.path.split(notebook)[-1][:-len('.ipynb')]
        if notebook_name == 'medical_image_segmentation':
            pytest.xfail('Dataset is only available via kaggle; need to authenticate on ci/cd')
        if notebook_name == 'auto_grad_accum' and device == 'cpu':
            pytest.skip('auto_grad_accum notebook only runs with a gpu')
        if notebook_name == 'TPU_Training_in_composer':
            pytest.skip('The CI does not support tpus')
        if notebook_name == 'ffcv_dataloaders' and device == 'cpu':
            pytest.skip('The FFCV notebook requires CUDA')
        if notebook_name == 'streaming_dataloader_facesynthetics':
            pytest.skip('Jenkins is killing this notebook for some reason, it should work locally')
        if notebook_name == 'training_without_local_storage':
            pytest.skip('Jenkins is not getting the S3 credentials set up properly, it should work locally')
        with testbook.testbook(notebook) as tb:
            tb.inject(trainer_monkeypatch_code)
            tb.inject('patch_notebooks()')
            for i, cell in enumerate(tb.cells):
                if cell['cell_type'] != 'code':
                    continue
                cell['source'] = modify_cell_source(tb,
                                                    notebook_name=notebook_name,
                                                    cell_source=cell['source'],
                                                    s3_bucket=s3_bucket)
>               tb.execute_cell(i)

/home/shie44167/workspace/1_read_codes/composer/tests/test_notebooks.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <testbook.client.TestbookNotebookClient object at 0x7f683c64b7c0>
cell = [9], kwargs = {}, cell_indexes = [9], executed_cells = [], idx = 9

    def execute_cell(self, cell, **kwargs) -> Union[Dict, List[Dict]]:
        """
        Executes a cell or list of cells
        """
        if isinstance(cell, slice):
            start, stop = self._cell_index(cell.start), self._cell_index(cell.stop)
            if cell.step is not None:
                raise TestbookError('testbook does not support step argument')
    
            cell = range(start, stop + 1)
        elif isinstance(cell, str) or isinstance(cell, int):
            cell = [cell]
    
        cell_indexes = cell
    
        if all(isinstance(x, str) for x in cell):
            cell_indexes = [self._cell_index(tag) for tag in cell]
    
        executed_cells = []
        for idx in cell_indexes:
            try:
                cell = super().execute_cell(self.nb['cells'][idx], idx, **kwargs)
            except CellExecutionError as ce:
>               raise TestbookRuntimeError(ce.evalue, ce, self._get_error_class(ce.ename))
E               testbook.exceptions.TestbookRuntimeError: An error occurred while executing the following cell:
E               ------------------
E               assert torch.cuda.is_available(), "Demonstrating automatic gradient accumulation requires a GPU."
E               
E               trainer = composer.trainer.Trainer(
E                   model=model,
E                   train_dataloader=train_dataloader,
E                   eval_dataloader=test_dataloader,
E                   optimizers=optimizer,
E                   max_duration="1ep",
E                   grad_accum='auto',  # <--- Activate Composer magic!
E                   device='gpu'
E               )
E               
E               # Train
E               trainer.fit()
E               ------------------
E               
E               [0;31m---------------------------------------------------------------------------[0m
E               [0;31mRuntimeError[0m                              Traceback (most recent call last)
E               [0;32m/tmp/ipykernel_312861/1804393140.py[0m in [0;36m<cell line: 14>[0;34m()[0m
E               [1;32m     12[0m [0;34m[0m[0m
E               [1;32m     13[0m [0;31m# Train[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m---> 14[0;31m [0mtrainer[0m[0;34m.[0m[0mfit[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m
E               [0;32m/tmp/ipykernel_312861/4080402710.py[0m in [0;36mnew_fit[0;34m(self, *args, **kwargs)[0m
E               [1;32m     19[0m             [0;32mif[0m [0;34m'eval_subset_num_batches'[0m [0;32mnot[0m [0;32min[0m [0mkwargs[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m     20[0m                 [0mkwargs[0m[0;34m[[0m[0;34m'eval_subset_num_batches'[0m[0;34m][0m [0;34m=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m---> 21[0;31m         [0moriginal_fit[0m[0;34m([0m[0mself[0m[0;34m,[0m [0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m     22[0m [0;34m[0m[0m
E               [1;32m     23[0m     [0mTrainer[0m[0;34m.[0m[0mfit[0m [0;34m=[0m [0mnew_fit[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36mfit[0;34m(self, train_dataloader, train_dataloader_label, train_subset_num_batches, duration, reset_time, schedulers, scale_schedule_ratio, step_schedulers_every_batch, eval_dataloader, eval_subset_num_batches, eval_interval, grad_accum, precision)[0m
E               [1;32m   1632[0m             [0;31m# update scaler since precision was provided[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1633[0m             [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mscaler[0m [0;34m=[0m [0mClosureGradScaler[0m[0;34m([0m[0;34m)[0m [0;32mif[0m [0mself[0m[0;34m.[0m[0m_use_closures[0m[0;34m([0m[0;34m)[0m [0;32melse[0m [0mGradScaler[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m-> 1634[0;31m         [0mself[0m[0;34m.[0m[0m_train_loop[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1635[0m [0;34m[0m[0m
E               [1;32m   1636[0m     [0;32mdef[0m [0mclose[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_loop[0;34m(self)[0m
E               [1;32m   1790[0m                     })
E               [1;32m   1791[0m [0;34m[0m[0m
E               [0;32m-> 1792[0;31m                     [0mtotal_loss_dict[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_train_batch[0m[0;34m([0m[0muse_grad_scaling[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1793[0m [0;34m[0m[0m
E               [1;32m   1794[0m                     [0;32mif[0m [0muse_grad_scaling[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_batch[0;34m(self, use_grad_scaling)[0m
E               [1;32m   1961[0m                                 microbatches, total_loss_dict, **kwargs).item())
E               [1;32m   1962[0m                 [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m-> 1963[0;31m                     [0mself[0m[0;34m.[0m[0m_train_microbatches[0m[0;34m([0m[0mmicrobatches[0m[0;34m,[0m [0mtotal_loss_dict[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1964[0m                     [0;32mif[0m [0;32mnot[0m [0mself[0m[0;34m.[0m[0mdeepspeed_enabled[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1965[0m                         [0;32mfor[0m [0moptimizer[0m [0;32min[0m [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0moptimizers[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_microbatches[0;34m(self, microbatches, total_loss_dict, ddp_sync)[0m
E               [1;32m   2032[0m             [0;32mfor[0m [0mmicrobatch_idx[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mbatch[0m [0;32min[0m [0menumerate[0m[0;34m([0m[0mmicrobatches[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   2033[0m                 [0mis_final_microbatch[0m [0;34m=[0m [0mmicrobatch_idx[0m [0;34m+[0m [0;36m1[0m [0;34m==[0m [0mlen[0m[0;34m([0m[0mmicrobatches[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m-> 2034[0;31m                 [0mmicrobatch_loss_dict[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_train_microbatch[0m[0;34m([0m[0muse_grad_scaling[0m[0;34m,[0m [0mcurrent_batch_size[0m[0;34m,[0m [0mis_final_microbatch[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   2035[0m [0;34m[0m[0m
E               [1;32m   2036[0m                 [0;31m# Aggregate each loss in microbatch_loss_dict into total_loss_dict[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_microbatch[0;34m(self, use_grad_scaling, current_batch_size, is_final_microbatch)[0m
E               [1;32m   2128[0m                 [0;31m# Scale loss based on the number of samples in the microbatch to maintain gradient numerics[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   2129[0m                 [0mmicrobatch_loss[0m[0;34m.[0m[0mmul_[0m[0;34m([0m[0mmicrobatch_num_samples[0m [0;34m/[0m [0mcurrent_batch_size[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m-> 2130[0;31m                 [0mmicrobatch_loss[0m[0;34m.[0m[0mbackward[0m[0;34m([0m[0mcreate_graph[0m[0;34m=[0m[0mself[0m[0;34m.[0m[0m_backwards_create_graph[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   2131[0m [0;34m[0m[0m
E               [1;32m   2132[0m             [0mself[0m[0;34m.[0m[0mengine[0m[0;34m.[0m[0mrun_event[0m[0;34m([0m[0mEvent[0m[0;34m.[0m[0mAFTER_BACKWARD[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/_tensor.py[0m in [0;36mbackward[0;34m(self, gradient, retain_graph, create_graph, inputs)[0m
E               [1;32m    394[0m                 [0mcreate_graph[0m[0;34m=[0m[0mcreate_graph[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m    395[0m                 inputs=inputs)
E               [0;32m--> 396[0;31m         [0mtorch[0m[0;34m.[0m[0mautograd[0m[0;34m.[0m[0mbackward[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mgradient[0m[0;34m,[0m [0mretain_graph[0m[0;34m,[0m [0mcreate_graph[0m[0;34m,[0m [0minputs[0m[0;34m=[0m[0minputs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m    397[0m [0;34m[0m[0m
E               [1;32m    398[0m     [0;32mdef[0m [0mregister_hook[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mhook[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/autograd/__init__.py[0m in [0;36mbackward[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)[0m
E               [1;32m    171[0m     [0;31m# some Python versions print out the first line of a multi-line function[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m    172[0m     [0;31m# calls in the traceback and some print out the last line[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m--> 173[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
E               [0m[1;32m    174[0m         [0mtensors[0m[0;34m,[0m [0mgrad_tensors_[0m[0;34m,[0m [0mretain_graph[0m[0;34m,[0m [0mcreate_graph[0m[0;34m,[0m [0minputs[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m    175[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
E               
E               [0;31mRuntimeError[0m: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED
E               RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/testbook/client.py:135: TestbookRuntimeError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
----------------------------- Captured stderr call -----------------------------


------------------------------ Captured log call -------------------------------
DEBUG    composer.core.engine:engine.py:484 Closing the engine
DEBUG    composer.core.engine:engine.py:488 Closing callback ProgressBarLogger
DEBUG    composer.core.engine:engine.py:488 Closing callback EventCounterCallback
DEBUG    composer.core.engine:engine.py:502 Post-closing callback ProgressBarLogger
DEBUG    composer.core.engine:engine.py:502 Post-closing callback EventCounterCallback
DEBUG    composer.core.engine:engine.py:484 Closing the engine
DEBUG    composer.core.engine:engine.py:488 Closing callback ProgressBarLogger
DEBUG    composer.core.engine:engine.py:488 Closing callback EventCounterCallback
DEBUG    composer.core.engine:engine.py:502 Post-closing callback ProgressBarLogger
DEBUG    composer.core.engine:engine.py:502 Post-closing callback EventCounterCallback
_ test_notebook[gpu-/home/shie44167/workspace/1_read_codes/composer/composer/../examples/early_stopping.ipynb] _

self = <testbook.client.TestbookNotebookClient object at 0x7f6796a64eb0>
cell = [14], kwargs = {}, cell_indexes = [14], executed_cells = [], idx = 14

    def execute_cell(self, cell, **kwargs) -> Union[Dict, List[Dict]]:
        """
        Executes a cell or list of cells
        """
        if isinstance(cell, slice):
            start, stop = self._cell_index(cell.start), self._cell_index(cell.stop)
            if cell.step is not None:
                raise TestbookError('testbook does not support step argument')
    
            cell = range(start, stop + 1)
        elif isinstance(cell, str) or isinstance(cell, int):
            cell = [cell]
    
        cell_indexes = cell
    
        if all(isinstance(x, str) for x in cell):
            cell_indexes = [self._cell_index(tag) for tag in cell]
    
        executed_cells = []
        for idx in cell_indexes:
            try:
>               cell = super().execute_cell(self.nb['cells'][idx], idx, **kwargs)

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/testbook/client.py:133: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<testbook.client.TestbookNotebookClient object at 0x7f6796a64eb0>, {'cell_type': 'code', 'execution_count': 8, 'metad...atches=10,  # Only training on a subset of the data to trigger the callback sooner\n)\n\n# Train!\ntrainer.fit()'}, 14)
kwargs = {}, name = 'MainThread'
inner = <coroutine object NotebookClient.async_execute_cell at 0x7f6796b81cb0>
loop = <_UnixSelectorEventLoop running=False closed=False debug=False>

    def wrapped(*args, **kwargs):
        name = threading.current_thread().name
        inner = coro(*args, **kwargs)
        try:
            # If a loop is currently running in this thread,
            # use a task runner.
            asyncio.get_running_loop()
            if name not in _runner_map:
                _runner_map[name] = _TaskRunner()
            return _runner_map[name].run(inner)
        except RuntimeError:
            pass
    
        # Run the loop for this thread.
        if name not in _loop_map:
            _loop_map[name] = asyncio.new_event_loop()
        loop = _loop_map[name]
>       return loop.run_until_complete(inner)

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/jupyter_core/utils/__init__.py:160: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_UnixSelectorEventLoop running=False closed=False debug=False>
future = <Task finished name='Task-202' coro=<NotebookClient.async_execute_cell() done, defined at /home/shie44167/miniconda3/e...ing max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n')>

    def run_until_complete(self, future):
        """Run until the Future is done.
    
        If the argument is a coroutine, it is wrapped in a Task.
    
        WARNING: It would be disastrous to call run_until_complete()
        with the same coroutine twice -- it would wrap it in two
        different Tasks and that can't be good.
    
        Return the Future's result, or raise its exception.
        """
        self._check_closed()
        self._check_running()
    
        new_task = not futures.isfuture(future)
        future = tasks.ensure_future(future, loop=self)
        if new_task:
            # An exception is raised if the future didn't complete, so there
            # is no need to log the "destroy pending task" message
            future._log_destroy_pending = False
    
        future.add_done_callback(_run_until_complete_cb)
        try:
            self.run_forever()
        except:
            if new_task and future.done() and not future.cancelled():
                # The coroutine raised a BaseException. Consume the exception
                # to not log a warning, the caller doesn't have access to the
                # local task.
                future.exception()
            raise
        finally:
            future.remove_done_callback(_run_until_complete_cb)
        if not future.done():
            raise RuntimeError('Event loop stopped before Future completed.')
    
>       return future.result()

/home/shie44167/miniconda3/envs/composer/lib/python3.10/asyncio/base_events.py:649: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <testbook.client.TestbookNotebookClient object at 0x7f6796a64eb0>
cell = {'cell_type': 'code', 'execution_count': 8, 'metadata': {'execution': {'iopub.status.busy': '2022-11-30T11:49:38.20926...num_batches=10,  # Only training on a subset of the data to trigger the callback sooner\n)\n\n# Train!\ntrainer.fit()'}
cell_index = 14, execution_count = None, store_history = True

    async def async_execute_cell(
        self,
        cell: NotebookNode,
        cell_index: int,
        execution_count: t.Optional[int] = None,
        store_history: bool = True,
    ) -> NotebookNode:
        """
        Executes a single code cell.
    
        To execute all cells see :meth:`execute`.
    
        Parameters
        ----------
        cell : nbformat.NotebookNode
            The cell which is currently being processed.
        cell_index : int
            The position of the cell within the notebook object.
        execution_count : int
            The execution count to be assigned to the cell (default: Use kernel response)
        store_history : bool
            Determines if history should be stored in the kernel (default: False).
            Specific to ipython kernels, which can store command histories.
    
        Returns
        -------
        output : dict
            The execution output payload (or None for no output).
    
        Raises
        ------
        CellExecutionError
            If execution failed and should raise an exception, this will be raised
            with defaults about the failure.
    
        Returns
        -------
        cell : NotebookNode
            The cell which was just processed.
        """
        assert self.kc is not None
    
        await run_hook(self.on_cell_start, cell=cell, cell_index=cell_index)
    
        if cell.cell_type != 'code' or not cell.source.strip():
            self.log.debug("Skipping non-executing cell %s", cell_index)
            return cell
    
        if self.skip_cells_with_tag in cell.metadata.get("tags", []):
            self.log.debug("Skipping tagged cell %s", cell_index)
            return cell
    
        if self.record_timing:  # clear execution metadata prior to execution
            cell['metadata']['execution'] = {}
    
        self.log.debug("Executing cell:\n%s", cell.source)
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors or "raises-exception" in cell.metadata.get("tags", [])
        )
    
        await run_hook(self.on_cell_execute, cell=cell, cell_index=cell_index)
        parent_msg_id = await ensure_async(
            self.kc.execute(
                cell.source, store_history=store_history, stop_on_error=not cell_allows_errors
            )
        )
        await run_hook(self.on_cell_complete, cell=cell, cell_index=cell_index)
        # We launched a code cell to execute
        self.code_cells_executed += 1
        exec_timeout = self._get_timeout(cell)
    
        cell.outputs = []
        self.clear_before_next_output = False
    
        task_poll_kernel_alive = asyncio.ensure_future(self._async_poll_kernel_alive())
        task_poll_output_msg = asyncio.ensure_future(
            self._async_poll_output_msg(parent_msg_id, cell, cell_index)
        )
        self.task_poll_for_reply = asyncio.ensure_future(
            self._async_poll_for_reply(
                parent_msg_id, cell, exec_timeout, task_poll_output_msg, task_poll_kernel_alive
            )
        )
        try:
            exec_reply = await self.task_poll_for_reply
        except asyncio.CancelledError:
            # can only be cancelled by task_poll_kernel_alive when the kernel is dead
            task_poll_output_msg.cancel()
            raise DeadKernelError("Kernel died")
        except Exception as e:
            # Best effort to cancel request if it hasn't been resolved
            try:
                # Check if the task_poll_output is doing the raising for us
                if not isinstance(e, CellControlSignal):
                    task_poll_output_msg.cancel()
            finally:
                raise
    
        if execution_count:
            cell['execution_count'] = execution_count
        await run_hook(
            self.on_cell_executed, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
>       await self._check_raise_for_error(cell, cell_index, exec_reply)

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/nbclient/client.py:1021: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <testbook.client.TestbookNotebookClient object at 0x7f6796a64eb0>
cell = {'cell_type': 'code', 'execution_count': 8, 'metadata': {'execution': {'iopub.status.busy': '2022-11-30T11:49:38.20926...num_batches=10,  # Only training on a subset of the data to trigger the callback sooner\n)\n\n# Train!\ntrainer.fit()'}
cell_index = 14
exec_reply = {'buffers': [], 'content': {'ename': 'RuntimeError', 'engine_info': {'engine_id': -1, 'engine_uuid': '572de8ef-43ba-44...e, 'engine': '572de8ef-43ba-4479-bf8c-1af9ffe2bada', 'started': '2022-11-30T11:49:38.209362Z', 'status': 'error'}, ...}

    async def _check_raise_for_error(
        self, cell: NotebookNode, cell_index: int, exec_reply: t.Optional[t.Dict]
    ) -> None:
    
        if exec_reply is None:
            return None
    
        exec_reply_content = exec_reply['content']
        if exec_reply_content['status'] != 'error':
            return None
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors
            or exec_reply_content.get('ename') in self.allow_error_names
            or "raises-exception" in cell.metadata.get("tags", [])
        )
        await run_hook(
            self.on_cell_error, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
        if not cell_allows_errors:
>           raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
E           nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
E           ------------------
E           from composer.trainer import Trainer
E           
E           # Early stopping should stop training before we reach 100 epochs!
E           train_epochs = "100ep"
E           
E           trainer = Trainer(
E               model=model,
E               train_dataloader=train_dataloader,
E               eval_dataloader=evaluator,
E               max_duration=train_epochs,
E               optimizers=optimizer,
E               schedulers=lr_scheduler,
E               callbacks=[early_stopper],    # Instruct the trainer to use our early stopping callback
E               train_subset_num_batches=10,  # Only training on a subset of the data to trigger the callback sooner
E           )
E           
E           # Train!
E           trainer.fit()
E           ------------------
E           
E           [0;31m---------------------------------------------------------------------------[0m
E           [0;31mRuntimeError[0m                              Traceback (most recent call last)
E           [0;32m/tmp/ipykernel_313059/1921538859.py[0m in [0;36m<cell line: 18>[0;34m()[0m
E           [1;32m     16[0m [0;34m[0m[0m
E           [1;32m     17[0m [0;31m# Train![0m[0;34m[0m[0;34m[0m[0m
E           [0;32m---> 18[0;31m [0mtrainer[0m[0;34m.[0m[0mfit[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m
E           [0;32m/tmp/ipykernel_313059/4080402710.py[0m in [0;36mnew_fit[0;34m(self, *args, **kwargs)[0m
E           [1;32m     19[0m             [0;32mif[0m [0;34m'eval_subset_num_batches'[0m [0;32mnot[0m [0;32min[0m [0mkwargs[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m     20[0m                 [0mkwargs[0m[0;34m[[0m[0;34m'eval_subset_num_batches'[0m[0;34m][0m [0;34m=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m---> 21[0;31m         [0moriginal_fit[0m[0;34m([0m[0mself[0m[0;34m,[0m [0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m     22[0m [0;34m[0m[0m
E           [1;32m     23[0m     [0mTrainer[0m[0;34m.[0m[0mfit[0m [0;34m=[0m [0mnew_fit[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36mfit[0;34m(self, train_dataloader, train_dataloader_label, train_subset_num_batches, duration, reset_time, schedulers, scale_schedule_ratio, step_schedulers_every_batch, eval_dataloader, eval_subset_num_batches, eval_interval, grad_accum, precision)[0m
E           [1;32m   1632[0m             [0;31m# update scaler since precision was provided[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1633[0m             [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mscaler[0m [0;34m=[0m [0mClosureGradScaler[0m[0;34m([0m[0;34m)[0m [0;32mif[0m [0mself[0m[0;34m.[0m[0m_use_closures[0m[0;34m([0m[0;34m)[0m [0;32melse[0m [0mGradScaler[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m-> 1634[0;31m         [0mself[0m[0;34m.[0m[0m_train_loop[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1635[0m [0;34m[0m[0m
E           [1;32m   1636[0m     [0;32mdef[0m [0mclose[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_loop[0;34m(self)[0m
E           [1;32m   1790[0m                     })
E           [1;32m   1791[0m [0;34m[0m[0m
E           [0;32m-> 1792[0;31m                     [0mtotal_loss_dict[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_train_batch[0m[0;34m([0m[0muse_grad_scaling[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1793[0m [0;34m[0m[0m
E           [1;32m   1794[0m                     [0;32mif[0m [0muse_grad_scaling[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_batch[0;34m(self, use_grad_scaling)[0m
E           [1;32m   1961[0m                                 microbatches, total_loss_dict, **kwargs).item())
E           [1;32m   1962[0m                 [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m-> 1963[0;31m                     [0mself[0m[0;34m.[0m[0m_train_microbatches[0m[0;34m([0m[0mmicrobatches[0m[0;34m,[0m [0mtotal_loss_dict[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1964[0m                     [0;32mif[0m [0;32mnot[0m [0mself[0m[0;34m.[0m[0mdeepspeed_enabled[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1965[0m                         [0;32mfor[0m [0moptimizer[0m [0;32min[0m [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0moptimizers[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_microbatches[0;34m(self, microbatches, total_loss_dict, ddp_sync)[0m
E           [1;32m   2032[0m             [0;32mfor[0m [0mmicrobatch_idx[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mbatch[0m [0;32min[0m [0menumerate[0m[0;34m([0m[0mmicrobatches[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   2033[0m                 [0mis_final_microbatch[0m [0;34m=[0m [0mmicrobatch_idx[0m [0;34m+[0m [0;36m1[0m [0;34m==[0m [0mlen[0m[0;34m([0m[0mmicrobatches[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m-> 2034[0;31m                 [0mmicrobatch_loss_dict[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_train_microbatch[0m[0;34m([0m[0muse_grad_scaling[0m[0;34m,[0m [0mcurrent_batch_size[0m[0;34m,[0m [0mis_final_microbatch[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   2035[0m [0;34m[0m[0m
E           [1;32m   2036[0m                 [0;31m# Aggregate each loss in microbatch_loss_dict into total_loss_dict[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_microbatch[0;34m(self, use_grad_scaling, current_batch_size, is_final_microbatch)[0m
E           [1;32m   2078[0m [0;34m[0m[0m
E           [1;32m   2079[0m             [0;32mwith[0m [0mget_precision_context[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mprecision[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m-> 2080[0;31m                 [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0moutputs[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mmodel[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mbatch[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   2081[0m [0;34m[0m[0m
E           [1;32m   2082[0m             [0mself[0m[0;34m.[0m[0mengine[0m[0;34m.[0m[0mrun_event[0m[0;34m([0m[0mEvent[0m[0;34m.[0m[0mAFTER_FORWARD[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E           [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E           [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E           [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/models/tasks/classification.py[0m in [0;36mforward[0;34m(self, batch)[0m
E           [1;32m    104[0m     [0;32mdef[0m [0mforward[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mbatch[0m[0;34m:[0m [0mTuple[0m[0;34m[[0m[0mTensor[0m[0;34m,[0m [0mAny[0m[0;34m][0m[0;34m)[0m [0;34m->[0m [0mTensor[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m    105[0m         [0minputs[0m[0;34m,[0m [0m_[0m [0;34m=[0m [0mbatch[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m--> 106[0;31m         [0moutputs[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mmodule[0m[0;34m([0m[0minputs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m    107[0m         [0;32mreturn[0m [0moutputs[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m    108[0m [0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E           [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E           [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E           [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/models/resnet_cifar/resnets.py[0m in [0;36mforward[0;34m(self, x)[0m
E           [1;32m     85[0m     [0;32mdef[0m [0mforward[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mx[0m[0;34m:[0m [0mtorch[0m[0;34m.[0m[0mTensor[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m     86[0m         [0mout[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mrelu[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mbn[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mconv[0m[0;34m([0m[0mx[0m[0;34m)[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m---> 87[0;31m         [0mout[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mblocks[0m[0;34m([0m[0mout[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m     88[0m         [0mout[0m [0;34m=[0m [0mF[0m[0;34m.[0m[0mavg_pool2d[0m[0;34m([0m[0mout[0m[0;34m,[0m [0mout[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;36m3[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m     89[0m         [0mout[0m [0;34m=[0m [0mout[0m[0;34m.[0m[0mview[0m[0;34m([0m[0mout[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;36m0[0m[0;34m)[0m[0;34m,[0m [0;34m-[0m[0;36m1[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E           [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E           [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E           [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/container.py[0m in [0;36mforward[0;34m(self, input)[0m
E           [1;32m    137[0m     [0;32mdef[0m [0mforward[0m[0;34m([0m[0mself[0m[0;34m,[0m [0minput[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m    138[0m         [0;32mfor[0m [0mmodule[0m [0;32min[0m [0mself[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m--> 139[0;31m             [0minput[0m [0;34m=[0m [0mmodule[0m[0;34m([0m[0minput[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m    140[0m         [0;32mreturn[0m [0minput[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m    141[0m [0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E           [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E           [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E           [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/models/resnet_cifar/resnets.py[0m in [0;36mforward[0;34m(self, x)[0m
E           [1;32m     49[0m         [0;32mdef[0m [0mforward[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mx[0m[0;34m:[0m [0mtorch[0m[0;34m.[0m[0mTensor[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m     50[0m             [0mout[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mrelu[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mbn1[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mconv1[0m[0;34m([0m[0mx[0m[0;34m)[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m---> 51[0;31m             [0mout[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mbn2[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mconv2[0m[0;34m([0m[0mout[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m     52[0m             [0mout[0m [0;34m+=[0m [0mself[0m[0;34m.[0m[0mshortcut[0m[0;34m([0m[0mx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m     53[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0mrelu[0m[0;34m([0m[0mout[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E           [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E           [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E           [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py[0m in [0;36mforward[0;34m(self, input)[0m
E           [1;32m    166[0m         [0mused[0m [0;32mfor[0m [0mnormalization[0m [0;34m([0m[0mi[0m[0;34m.[0m[0me[0m[0;34m.[0m [0;32min[0m [0meval[0m [0mmode[0m [0mwhen[0m [0mbuffers[0m [0mare[0m [0;32mnot[0m [0;32mNone[0m[0;34m)[0m[0;34m.[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m    167[0m         """
E           [0;32m--> 168[0;31m         return F.batch_norm(
E           [0m[1;32m    169[0m             [0minput[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m    170[0m             [0;31m# If buffers are not to be tracked, ensure that they won't be updated[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/functional.py[0m in [0;36mbatch_norm[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)[0m
E           [1;32m   2436[0m         [0m_verify_batch_size[0m[0;34m([0m[0minput[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   2437[0m [0;34m[0m[0m
E           [0;32m-> 2438[0;31m     return torch.batch_norm(
E           [0m[1;32m   2439[0m         [0minput[0m[0;34m,[0m [0mweight[0m[0;34m,[0m [0mbias[0m[0;34m,[0m [0mrunning_mean[0m[0;34m,[0m [0mrunning_var[0m[0;34m,[0m [0mtraining[0m[0;34m,[0m [0mmomentum[0m[0;34m,[0m [0meps[0m[0;34m,[0m [0mtorch[0m[0;34m.[0m[0mbackends[0m[0;34m.[0m[0mcudnn[0m[0;34m.[0m[0menabled[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   2440[0m     )
E           
E           [0;31mRuntimeError[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 7.76 GiB total capacity; 1.60 GiB already allocated; 26.31 MiB free; 1.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
E           RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 7.76 GiB total capacity; 1.60 GiB already allocated; 26.31 MiB free; 1.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/nbclient/client.py:915: CellExecutionError

During handling of the above exception, another exception occurred:

notebook = '/home/shie44167/workspace/1_read_codes/composer/composer/../examples/early_stopping.ipynb'
device = 'gpu', s3_bucket = 'my-bucket'

    @pytest.mark.parametrize('notebook', [_to_pytest_param(notebook) for notebook in NOTEBOOKS])
    @device('cpu', 'gpu')
    @pytest.mark.daily
    def test_notebook(notebook: str, device: str, s3_bucket: str):
        trainer_monkeypatch_code = inspect.getsource(patch_notebooks)
        notebook_name = os.path.split(notebook)[-1][:-len('.ipynb')]
        if notebook_name == 'medical_image_segmentation':
            pytest.xfail('Dataset is only available via kaggle; need to authenticate on ci/cd')
        if notebook_name == 'auto_grad_accum' and device == 'cpu':
            pytest.skip('auto_grad_accum notebook only runs with a gpu')
        if notebook_name == 'TPU_Training_in_composer':
            pytest.skip('The CI does not support tpus')
        if notebook_name == 'ffcv_dataloaders' and device == 'cpu':
            pytest.skip('The FFCV notebook requires CUDA')
        if notebook_name == 'streaming_dataloader_facesynthetics':
            pytest.skip('Jenkins is killing this notebook for some reason, it should work locally')
        if notebook_name == 'training_without_local_storage':
            pytest.skip('Jenkins is not getting the S3 credentials set up properly, it should work locally')
        with testbook.testbook(notebook) as tb:
            tb.inject(trainer_monkeypatch_code)
            tb.inject('patch_notebooks()')
            for i, cell in enumerate(tb.cells):
                if cell['cell_type'] != 'code':
                    continue
                cell['source'] = modify_cell_source(tb,
                                                    notebook_name=notebook_name,
                                                    cell_source=cell['source'],
                                                    s3_bucket=s3_bucket)
>               tb.execute_cell(i)

/home/shie44167/workspace/1_read_codes/composer/tests/test_notebooks.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <testbook.client.TestbookNotebookClient object at 0x7f6796a64eb0>
cell = [14], kwargs = {}, cell_indexes = [14], executed_cells = [], idx = 14

    def execute_cell(self, cell, **kwargs) -> Union[Dict, List[Dict]]:
        """
        Executes a cell or list of cells
        """
        if isinstance(cell, slice):
            start, stop = self._cell_index(cell.start), self._cell_index(cell.stop)
            if cell.step is not None:
                raise TestbookError('testbook does not support step argument')
    
            cell = range(start, stop + 1)
        elif isinstance(cell, str) or isinstance(cell, int):
            cell = [cell]
    
        cell_indexes = cell
    
        if all(isinstance(x, str) for x in cell):
            cell_indexes = [self._cell_index(tag) for tag in cell]
    
        executed_cells = []
        for idx in cell_indexes:
            try:
                cell = super().execute_cell(self.nb['cells'][idx], idx, **kwargs)
            except CellExecutionError as ce:
>               raise TestbookRuntimeError(ce.evalue, ce, self._get_error_class(ce.ename))
E               testbook.exceptions.TestbookRuntimeError: An error occurred while executing the following cell:
E               ------------------
E               from composer.trainer import Trainer
E               
E               # Early stopping should stop training before we reach 100 epochs!
E               train_epochs = "100ep"
E               
E               trainer = Trainer(
E                   model=model,
E                   train_dataloader=train_dataloader,
E                   eval_dataloader=evaluator,
E                   max_duration=train_epochs,
E                   optimizers=optimizer,
E                   schedulers=lr_scheduler,
E                   callbacks=[early_stopper],    # Instruct the trainer to use our early stopping callback
E                   train_subset_num_batches=10,  # Only training on a subset of the data to trigger the callback sooner
E               )
E               
E               # Train!
E               trainer.fit()
E               ------------------
E               
E               [0;31m---------------------------------------------------------------------------[0m
E               [0;31mRuntimeError[0m                              Traceback (most recent call last)
E               [0;32m/tmp/ipykernel_313059/1921538859.py[0m in [0;36m<cell line: 18>[0;34m()[0m
E               [1;32m     16[0m [0;34m[0m[0m
E               [1;32m     17[0m [0;31m# Train![0m[0;34m[0m[0;34m[0m[0m
E               [0;32m---> 18[0;31m [0mtrainer[0m[0;34m.[0m[0mfit[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m
E               [0;32m/tmp/ipykernel_313059/4080402710.py[0m in [0;36mnew_fit[0;34m(self, *args, **kwargs)[0m
E               [1;32m     19[0m             [0;32mif[0m [0;34m'eval_subset_num_batches'[0m [0;32mnot[0m [0;32min[0m [0mkwargs[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m     20[0m                 [0mkwargs[0m[0;34m[[0m[0;34m'eval_subset_num_batches'[0m[0;34m][0m [0;34m=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m---> 21[0;31m         [0moriginal_fit[0m[0;34m([0m[0mself[0m[0;34m,[0m [0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m     22[0m [0;34m[0m[0m
E               [1;32m     23[0m     [0mTrainer[0m[0;34m.[0m[0mfit[0m [0;34m=[0m [0mnew_fit[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36mfit[0;34m(self, train_dataloader, train_dataloader_label, train_subset_num_batches, duration, reset_time, schedulers, scale_schedule_ratio, step_schedulers_every_batch, eval_dataloader, eval_subset_num_batches, eval_interval, grad_accum, precision)[0m
E               [1;32m   1632[0m             [0;31m# update scaler since precision was provided[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1633[0m             [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mscaler[0m [0;34m=[0m [0mClosureGradScaler[0m[0;34m([0m[0;34m)[0m [0;32mif[0m [0mself[0m[0;34m.[0m[0m_use_closures[0m[0;34m([0m[0;34m)[0m [0;32melse[0m [0mGradScaler[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m-> 1634[0;31m         [0mself[0m[0;34m.[0m[0m_train_loop[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1635[0m [0;34m[0m[0m
E               [1;32m   1636[0m     [0;32mdef[0m [0mclose[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_loop[0;34m(self)[0m
E               [1;32m   1790[0m                     })
E               [1;32m   1791[0m [0;34m[0m[0m
E               [0;32m-> 1792[0;31m                     [0mtotal_loss_dict[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_train_batch[0m[0;34m([0m[0muse_grad_scaling[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1793[0m [0;34m[0m[0m
E               [1;32m   1794[0m                     [0;32mif[0m [0muse_grad_scaling[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_batch[0;34m(self, use_grad_scaling)[0m
E               [1;32m   1961[0m                                 microbatches, total_loss_dict, **kwargs).item())
E               [1;32m   1962[0m                 [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m-> 1963[0;31m                     [0mself[0m[0;34m.[0m[0m_train_microbatches[0m[0;34m([0m[0mmicrobatches[0m[0;34m,[0m [0mtotal_loss_dict[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1964[0m                     [0;32mif[0m [0;32mnot[0m [0mself[0m[0;34m.[0m[0mdeepspeed_enabled[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1965[0m                         [0;32mfor[0m [0moptimizer[0m [0;32min[0m [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0moptimizers[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_microbatches[0;34m(self, microbatches, total_loss_dict, ddp_sync)[0m
E               [1;32m   2032[0m             [0;32mfor[0m [0mmicrobatch_idx[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mbatch[0m [0;32min[0m [0menumerate[0m[0;34m([0m[0mmicrobatches[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   2033[0m                 [0mis_final_microbatch[0m [0;34m=[0m [0mmicrobatch_idx[0m [0;34m+[0m [0;36m1[0m [0;34m==[0m [0mlen[0m[0;34m([0m[0mmicrobatches[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m-> 2034[0;31m                 [0mmicrobatch_loss_dict[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_train_microbatch[0m[0;34m([0m[0muse_grad_scaling[0m[0;34m,[0m [0mcurrent_batch_size[0m[0;34m,[0m [0mis_final_microbatch[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   2035[0m [0;34m[0m[0m
E               [1;32m   2036[0m                 [0;31m# Aggregate each loss in microbatch_loss_dict into total_loss_dict[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_microbatch[0;34m(self, use_grad_scaling, current_batch_size, is_final_microbatch)[0m
E               [1;32m   2078[0m [0;34m[0m[0m
E               [1;32m   2079[0m             [0;32mwith[0m [0mget_precision_context[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mprecision[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m-> 2080[0;31m                 [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0moutputs[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mmodel[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mbatch[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   2081[0m [0;34m[0m[0m
E               [1;32m   2082[0m             [0mself[0m[0;34m.[0m[0mengine[0m[0;34m.[0m[0mrun_event[0m[0;34m([0m[0mEvent[0m[0;34m.[0m[0mAFTER_FORWARD[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E               [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E               [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E               [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/models/tasks/classification.py[0m in [0;36mforward[0;34m(self, batch)[0m
E               [1;32m    104[0m     [0;32mdef[0m [0mforward[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mbatch[0m[0;34m:[0m [0mTuple[0m[0;34m[[0m[0mTensor[0m[0;34m,[0m [0mAny[0m[0;34m][0m[0;34m)[0m [0;34m->[0m [0mTensor[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m    105[0m         [0minputs[0m[0;34m,[0m [0m_[0m [0;34m=[0m [0mbatch[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m--> 106[0;31m         [0moutputs[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mmodule[0m[0;34m([0m[0minputs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m    107[0m         [0;32mreturn[0m [0moutputs[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m    108[0m [0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E               [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E               [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E               [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/models/resnet_cifar/resnets.py[0m in [0;36mforward[0;34m(self, x)[0m
E               [1;32m     85[0m     [0;32mdef[0m [0mforward[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mx[0m[0;34m:[0m [0mtorch[0m[0;34m.[0m[0mTensor[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m     86[0m         [0mout[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mrelu[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mbn[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mconv[0m[0;34m([0m[0mx[0m[0;34m)[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m---> 87[0;31m         [0mout[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mblocks[0m[0;34m([0m[0mout[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m     88[0m         [0mout[0m [0;34m=[0m [0mF[0m[0;34m.[0m[0mavg_pool2d[0m[0;34m([0m[0mout[0m[0;34m,[0m [0mout[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;36m3[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m     89[0m         [0mout[0m [0;34m=[0m [0mout[0m[0;34m.[0m[0mview[0m[0;34m([0m[0mout[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;36m0[0m[0;34m)[0m[0;34m,[0m [0;34m-[0m[0;36m1[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E               [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E               [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E               [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/container.py[0m in [0;36mforward[0;34m(self, input)[0m
E               [1;32m    137[0m     [0;32mdef[0m [0mforward[0m[0;34m([0m[0mself[0m[0;34m,[0m [0minput[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m    138[0m         [0;32mfor[0m [0mmodule[0m [0;32min[0m [0mself[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m--> 139[0;31m             [0minput[0m [0;34m=[0m [0mmodule[0m[0;34m([0m[0minput[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m    140[0m         [0;32mreturn[0m [0minput[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m    141[0m [0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E               [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E               [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E               [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/models/resnet_cifar/resnets.py[0m in [0;36mforward[0;34m(self, x)[0m
E               [1;32m     49[0m         [0;32mdef[0m [0mforward[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mx[0m[0;34m:[0m [0mtorch[0m[0;34m.[0m[0mTensor[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m     50[0m             [0mout[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mrelu[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mbn1[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mconv1[0m[0;34m([0m[0mx[0m[0;34m)[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m---> 51[0;31m             [0mout[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mbn2[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mconv2[0m[0;34m([0m[0mout[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m     52[0m             [0mout[0m [0;34m+=[0m [0mself[0m[0;34m.[0m[0mshortcut[0m[0;34m([0m[0mx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m     53[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0mrelu[0m[0;34m([0m[0mout[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E               [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E               [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E               [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py[0m in [0;36mforward[0;34m(self, input)[0m
E               [1;32m    166[0m         [0mused[0m [0;32mfor[0m [0mnormalization[0m [0;34m([0m[0mi[0m[0;34m.[0m[0me[0m[0;34m.[0m [0;32min[0m [0meval[0m [0mmode[0m [0mwhen[0m [0mbuffers[0m [0mare[0m [0;32mnot[0m [0;32mNone[0m[0;34m)[0m[0;34m.[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m    167[0m         """
E               [0;32m--> 168[0;31m         return F.batch_norm(
E               [0m[1;32m    169[0m             [0minput[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m    170[0m             [0;31m# If buffers are not to be tracked, ensure that they won't be updated[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/functional.py[0m in [0;36mbatch_norm[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)[0m
E               [1;32m   2436[0m         [0m_verify_batch_size[0m[0;34m([0m[0minput[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   2437[0m [0;34m[0m[0m
E               [0;32m-> 2438[0;31m     return torch.batch_norm(
E               [0m[1;32m   2439[0m         [0minput[0m[0;34m,[0m [0mweight[0m[0;34m,[0m [0mbias[0m[0;34m,[0m [0mrunning_mean[0m[0;34m,[0m [0mrunning_var[0m[0;34m,[0m [0mtraining[0m[0;34m,[0m [0mmomentum[0m[0;34m,[0m [0meps[0m[0;34m,[0m [0mtorch[0m[0;34m.[0m[0mbackends[0m[0;34m.[0m[0mcudnn[0m[0;34m.[0m[0menabled[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   2440[0m     )
E               
E               [0;31mRuntimeError[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 7.76 GiB total capacity; 1.60 GiB already allocated; 26.31 MiB free; 1.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
E               RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 7.76 GiB total capacity; 1.60 GiB already allocated; 26.31 MiB free; 1.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/testbook/client.py:135: TestbookRuntimeError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
_ test_notebook[gpu-/home/shie44167/workspace/1_read_codes/composer/composer/../examples/ffcv_dataloaders.ipynb] _

self = <testbook.client.TestbookNotebookClient object at 0x7f67949bd4b0>
cell = [24], kwargs = {}, cell_indexes = [24], executed_cells = [], idx = 24

    def execute_cell(self, cell, **kwargs) -> Union[Dict, List[Dict]]:
        """
        Executes a cell or list of cells
        """
        if isinstance(cell, slice):
            start, stop = self._cell_index(cell.start), self._cell_index(cell.stop)
            if cell.step is not None:
                raise TestbookError('testbook does not support step argument')
    
            cell = range(start, stop + 1)
        elif isinstance(cell, str) or isinstance(cell, int):
            cell = [cell]
    
        cell_indexes = cell
    
        if all(isinstance(x, str) for x in cell):
            cell_indexes = [self._cell_index(tag) for tag in cell]
    
        executed_cells = []
        for idx in cell_indexes:
            try:
>               cell = super().execute_cell(self.nb['cells'][idx], idx, **kwargs)

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/testbook/client.py:133: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<testbook.client.TestbookNotebookClient object at 0x7f67949bd4b0>, {'cell_type': 'code', 'execution_count': 12, 'meta...ng Anaconda."]}], 'source': 'from composer.datasets.ffcv_utils import ffcv_monkey_patches\nffcv_monkey_patches()'}, 24)
kwargs = {}, name = 'MainThread'
inner = <coroutine object NotebookClient.async_execute_cell at 0x7f6796b9f4c0>
loop = <_UnixSelectorEventLoop running=False closed=False debug=False>

    def wrapped(*args, **kwargs):
        name = threading.current_thread().name
        inner = coro(*args, **kwargs)
        try:
            # If a loop is currently running in this thread,
            # use a task runner.
            asyncio.get_running_loop()
            if name not in _runner_map:
                _runner_map[name] = _TaskRunner()
            return _runner_map[name].run(inner)
        except RuntimeError:
            pass
    
        # Run the loop for this thread.
        if name not in _loop_map:
            _loop_map[name] = asyncio.new_event_loop()
        loop = _loop_map[name]
>       return loop.run_until_complete(inner)

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/jupyter_core/utils/__init__.py:160: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_UnixSelectorEventLoop running=False closed=False debug=False>
future = <Task finished name='Task-374' coro=<NotebookClient.async_execute_cell() done, defined at /home/shie44167/miniconda3/e...omposer, run `pip install 'mosaicml[ffcv]'` if using pip or `conda install -c conda-forge ffcv` if using Anaconda.\n")>

    def run_until_complete(self, future):
        """Run until the Future is done.
    
        If the argument is a coroutine, it is wrapped in a Task.
    
        WARNING: It would be disastrous to call run_until_complete()
        with the same coroutine twice -- it would wrap it in two
        different Tasks and that can't be good.
    
        Return the Future's result, or raise its exception.
        """
        self._check_closed()
        self._check_running()
    
        new_task = not futures.isfuture(future)
        future = tasks.ensure_future(future, loop=self)
        if new_task:
            # An exception is raised if the future didn't complete, so there
            # is no need to log the "destroy pending task" message
            future._log_destroy_pending = False
    
        future.add_done_callback(_run_until_complete_cb)
        try:
            self.run_forever()
        except:
            if new_task and future.done() and not future.cancelled():
                # The coroutine raised a BaseException. Consume the exception
                # to not log a warning, the caller doesn't have access to the
                # local task.
                future.exception()
            raise
        finally:
            future.remove_done_callback(_run_until_complete_cb)
        if not future.done():
            raise RuntimeError('Event loop stopped before Future completed.')
    
>       return future.result()

/home/shie44167/miniconda3/envs/composer/lib/python3.10/asyncio/base_events.py:649: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <testbook.client.TestbookNotebookClient object at 0x7f67949bd4b0>
cell = {'cell_type': 'code', 'execution_count': 12, 'metadata': {'execution': {'iopub.status.busy': '2022-11-30T11:50:23.4214...f using Anaconda."]}], 'source': 'from composer.datasets.ffcv_utils import ffcv_monkey_patches\nffcv_monkey_patches()'}
cell_index = 24, execution_count = None, store_history = True

    async def async_execute_cell(
        self,
        cell: NotebookNode,
        cell_index: int,
        execution_count: t.Optional[int] = None,
        store_history: bool = True,
    ) -> NotebookNode:
        """
        Executes a single code cell.
    
        To execute all cells see :meth:`execute`.
    
        Parameters
        ----------
        cell : nbformat.NotebookNode
            The cell which is currently being processed.
        cell_index : int
            The position of the cell within the notebook object.
        execution_count : int
            The execution count to be assigned to the cell (default: Use kernel response)
        store_history : bool
            Determines if history should be stored in the kernel (default: False).
            Specific to ipython kernels, which can store command histories.
    
        Returns
        -------
        output : dict
            The execution output payload (or None for no output).
    
        Raises
        ------
        CellExecutionError
            If execution failed and should raise an exception, this will be raised
            with defaults about the failure.
    
        Returns
        -------
        cell : NotebookNode
            The cell which was just processed.
        """
        assert self.kc is not None
    
        await run_hook(self.on_cell_start, cell=cell, cell_index=cell_index)
    
        if cell.cell_type != 'code' or not cell.source.strip():
            self.log.debug("Skipping non-executing cell %s", cell_index)
            return cell
    
        if self.skip_cells_with_tag in cell.metadata.get("tags", []):
            self.log.debug("Skipping tagged cell %s", cell_index)
            return cell
    
        if self.record_timing:  # clear execution metadata prior to execution
            cell['metadata']['execution'] = {}
    
        self.log.debug("Executing cell:\n%s", cell.source)
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors or "raises-exception" in cell.metadata.get("tags", [])
        )
    
        await run_hook(self.on_cell_execute, cell=cell, cell_index=cell_index)
        parent_msg_id = await ensure_async(
            self.kc.execute(
                cell.source, store_history=store_history, stop_on_error=not cell_allows_errors
            )
        )
        await run_hook(self.on_cell_complete, cell=cell, cell_index=cell_index)
        # We launched a code cell to execute
        self.code_cells_executed += 1
        exec_timeout = self._get_timeout(cell)
    
        cell.outputs = []
        self.clear_before_next_output = False
    
        task_poll_kernel_alive = asyncio.ensure_future(self._async_poll_kernel_alive())
        task_poll_output_msg = asyncio.ensure_future(
            self._async_poll_output_msg(parent_msg_id, cell, cell_index)
        )
        self.task_poll_for_reply = asyncio.ensure_future(
            self._async_poll_for_reply(
                parent_msg_id, cell, exec_timeout, task_poll_output_msg, task_poll_kernel_alive
            )
        )
        try:
            exec_reply = await self.task_poll_for_reply
        except asyncio.CancelledError:
            # can only be cancelled by task_poll_kernel_alive when the kernel is dead
            task_poll_output_msg.cancel()
            raise DeadKernelError("Kernel died")
        except Exception as e:
            # Best effort to cancel request if it hasn't been resolved
            try:
                # Check if the task_poll_output is doing the raising for us
                if not isinstance(e, CellControlSignal):
                    task_poll_output_msg.cancel()
            finally:
                raise
    
        if execution_count:
            cell['execution_count'] = execution_count
        await run_hook(
            self.on_cell_executed, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
>       await self._check_raise_for_error(cell, cell_index, exec_reply)

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/nbclient/client.py:1021: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <testbook.client.TestbookNotebookClient object at 0x7f67949bd4b0>
cell = {'cell_type': 'code', 'execution_count': 12, 'metadata': {'execution': {'iopub.status.busy': '2022-11-30T11:50:23.4214...f using Anaconda."]}], 'source': 'from composer.datasets.ffcv_utils import ffcv_monkey_patches\nffcv_monkey_patches()'}
cell_index = 24
exec_reply = {'buffers': [], 'content': {'ename': 'MissingConditionalImportError', 'engine_info': {'engine_id': -1, 'engine_uuid': ...e, 'engine': 'ca1d197a-75bb-42ea-9652-f504568f9432', 'started': '2022-11-30T11:50:23.421606Z', 'status': 'error'}, ...}

    async def _check_raise_for_error(
        self, cell: NotebookNode, cell_index: int, exec_reply: t.Optional[t.Dict]
    ) -> None:
    
        if exec_reply is None:
            return None
    
        exec_reply_content = exec_reply['content']
        if exec_reply_content['status'] != 'error':
            return None
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors
            or exec_reply_content.get('ename') in self.allow_error_names
            or "raises-exception" in cell.metadata.get("tags", [])
        )
        await run_hook(
            self.on_cell_error, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
        if not cell_allows_errors:
>           raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
E           nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
E           ------------------
E           from composer.datasets.ffcv_utils import ffcv_monkey_patches
E           ffcv_monkey_patches()
E           ------------------
E           
E           [0;31m---------------------------------------------------------------------------[0m
E           [0;31mMissingConditionalImportError[0m             Traceback (most recent call last)
E           [0;32m/tmp/ipykernel_313644/3997271273.py[0m in [0;36m<cell line: 2>[0;34m()[0m
E           [1;32m      1[0m [0;32mfrom[0m [0mcomposer[0m[0;34m.[0m[0mdatasets[0m[0;34m.[0m[0mffcv_utils[0m [0;32mimport[0m [0mffcv_monkey_patches[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m----> 2[0;31m [0mffcv_monkey_patches[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/datasets/ffcv_utils.py[0m in [0;36mffcv_monkey_patches[0;34m()[0m
E           [1;32m     27[0m [0;34m[0m[0m
E           [1;32m     28[0m [0;32mdef[0m [0mffcv_monkey_patches[0m[0;34m([0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m---> 29[0;31m     [0m_require_ffcv[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m     30[0m [0;34m[0m[0m
E           [1;32m     31[0m     [0;31m# ffcv's __len__ function is expensive as it always calls self.next_traversal_order which does shuffling.[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/datasets/ffcv_utils.py[0m in [0;36m_require_ffcv[0;34m()[0m
E           [1;32m     23[0m [0;32mdef[0m [0m_require_ffcv[0m[0;34m([0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m     24[0m     [0;32mif[0m [0;32mnot[0m [0mffcv_installed[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m---> 25[0;31m         [0;32mraise[0m [0mMissingConditionalImportError[0m[0;34m([0m[0mextra_deps_group[0m[0;34m=[0m[0;34m'ffcv'[0m[0;34m,[0m [0mconda_package[0m[0;34m=[0m[0;34m'ffcv'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m     26[0m [0;34m[0m[0m
E           [1;32m     27[0m [0;34m[0m[0m
E           
E           [0;31mMissingConditionalImportError[0m: Composer was installed without ffcv support. To use ffcv related packages, with Composer, run `pip install 'mosaicml[ffcv]'` if using pip or `conda install -c conda-forge ffcv` if using Anaconda.
E           MissingConditionalImportError: Composer was installed without ffcv support. To use ffcv related packages, with Composer, run `pip install 'mosaicml[ffcv]'` if using pip or `conda install -c conda-forge ffcv` if using Anaconda.

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/nbclient/client.py:915: CellExecutionError

During handling of the above exception, another exception occurred:

notebook = '/home/shie44167/workspace/1_read_codes/composer/composer/../examples/ffcv_dataloaders.ipynb'
device = 'gpu', s3_bucket = 'my-bucket'

    @pytest.mark.parametrize('notebook', [_to_pytest_param(notebook) for notebook in NOTEBOOKS])
    @device('cpu', 'gpu')
    @pytest.mark.daily
    def test_notebook(notebook: str, device: str, s3_bucket: str):
        trainer_monkeypatch_code = inspect.getsource(patch_notebooks)
        notebook_name = os.path.split(notebook)[-1][:-len('.ipynb')]
        if notebook_name == 'medical_image_segmentation':
            pytest.xfail('Dataset is only available via kaggle; need to authenticate on ci/cd')
        if notebook_name == 'auto_grad_accum' and device == 'cpu':
            pytest.skip('auto_grad_accum notebook only runs with a gpu')
        if notebook_name == 'TPU_Training_in_composer':
            pytest.skip('The CI does not support tpus')
        if notebook_name == 'ffcv_dataloaders' and device == 'cpu':
            pytest.skip('The FFCV notebook requires CUDA')
        if notebook_name == 'streaming_dataloader_facesynthetics':
            pytest.skip('Jenkins is killing this notebook for some reason, it should work locally')
        if notebook_name == 'training_without_local_storage':
            pytest.skip('Jenkins is not getting the S3 credentials set up properly, it should work locally')
        with testbook.testbook(notebook) as tb:
            tb.inject(trainer_monkeypatch_code)
            tb.inject('patch_notebooks()')
            for i, cell in enumerate(tb.cells):
                if cell['cell_type'] != 'code':
                    continue
                cell['source'] = modify_cell_source(tb,
                                                    notebook_name=notebook_name,
                                                    cell_source=cell['source'],
                                                    s3_bucket=s3_bucket)
>               tb.execute_cell(i)

/home/shie44167/workspace/1_read_codes/composer/tests/test_notebooks.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <testbook.client.TestbookNotebookClient object at 0x7f67949bd4b0>
cell = [24], kwargs = {}, cell_indexes = [24], executed_cells = [], idx = 24

    def execute_cell(self, cell, **kwargs) -> Union[Dict, List[Dict]]:
        """
        Executes a cell or list of cells
        """
        if isinstance(cell, slice):
            start, stop = self._cell_index(cell.start), self._cell_index(cell.stop)
            if cell.step is not None:
                raise TestbookError('testbook does not support step argument')
    
            cell = range(start, stop + 1)
        elif isinstance(cell, str) or isinstance(cell, int):
            cell = [cell]
    
        cell_indexes = cell
    
        if all(isinstance(x, str) for x in cell):
            cell_indexes = [self._cell_index(tag) for tag in cell]
    
        executed_cells = []
        for idx in cell_indexes:
            try:
                cell = super().execute_cell(self.nb['cells'][idx], idx, **kwargs)
            except CellExecutionError as ce:
>               raise TestbookRuntimeError(ce.evalue, ce, self._get_error_class(ce.ename))
E               testbook.exceptions.TestbookRuntimeError: An error occurred while executing the following cell:
E               ------------------
E               from composer.datasets.ffcv_utils import ffcv_monkey_patches
E               ffcv_monkey_patches()
E               ------------------
E               
E               [0;31m---------------------------------------------------------------------------[0m
E               [0;31mMissingConditionalImportError[0m             Traceback (most recent call last)
E               [0;32m/tmp/ipykernel_313644/3997271273.py[0m in [0;36m<cell line: 2>[0;34m()[0m
E               [1;32m      1[0m [0;32mfrom[0m [0mcomposer[0m[0;34m.[0m[0mdatasets[0m[0;34m.[0m[0mffcv_utils[0m [0;32mimport[0m [0mffcv_monkey_patches[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m----> 2[0;31m [0mffcv_monkey_patches[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/datasets/ffcv_utils.py[0m in [0;36mffcv_monkey_patches[0;34m()[0m
E               [1;32m     27[0m [0;34m[0m[0m
E               [1;32m     28[0m [0;32mdef[0m [0mffcv_monkey_patches[0m[0;34m([0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m---> 29[0;31m     [0m_require_ffcv[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m     30[0m [0;34m[0m[0m
E               [1;32m     31[0m     [0;31m# ffcv's __len__ function is expensive as it always calls self.next_traversal_order which does shuffling.[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/datasets/ffcv_utils.py[0m in [0;36m_require_ffcv[0;34m()[0m
E               [1;32m     23[0m [0;32mdef[0m [0m_require_ffcv[0m[0;34m([0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m     24[0m     [0;32mif[0m [0;32mnot[0m [0mffcv_installed[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m---> 25[0;31m         [0;32mraise[0m [0mMissingConditionalImportError[0m[0;34m([0m[0mextra_deps_group[0m[0;34m=[0m[0;34m'ffcv'[0m[0;34m,[0m [0mconda_package[0m[0;34m=[0m[0;34m'ffcv'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m     26[0m [0;34m[0m[0m
E               [1;32m     27[0m [0;34m[0m[0m
E               
E               [0;31mMissingConditionalImportError[0m: Composer was installed without ffcv support. To use ffcv related packages, with Composer, run `pip install 'mosaicml[ffcv]'` if using pip or `conda install -c conda-forge ffcv` if using Anaconda.
E               MissingConditionalImportError: Composer was installed without ffcv support. To use ffcv related packages, with Composer, run `pip install 'mosaicml[ffcv]'` if using pip or `conda install -c conda-forge ffcv` if using Anaconda.

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/testbook/client.py:135: TestbookRuntimeError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
----------------------------- Captured stderr call -----------------------------
[IPKernelApp] WARNING | WARNING: attempted to send message from fork
{'header': {'msg_id': 'fd7a326f-a48486fe62b30e2d1a6e2874_313865_131', 'msg_type': 'comm_msg', 'username': 'shie44167', 'session': 'fd7a326f-a48486fe62b30e2d1a6e2874', 'date': datetime.datetime(2022, 11, 30, 11, 50, 22, 868964, tzinfo=datetime.timezone.utc), 'version': '5.3'}, 'msg_id': 'fd7a326f-a48486fe62b30e2d1a6e2874_313865_131', 'msg_type': 'comm_msg', 'parent_header': {'msg_id': '9f675616-9c730a781d572d25d31e654f_311606_12', 'msg_type': 'execute_request', 'username': 'shie44167', 'session': '9f675616-9c730a781d572d25d31e654f', 'date': datetime.datetime(2022, 11, 30, 11, 50, 21, 187004, tzinfo=tzutc()), 'version': '5.3'}, 'content': {'data': {'method': 'update', 'state': {'value': 'train          Epoch   0:  100%|| 2/2 [00:01&lt;00:00,  3.43ba/s, loss/train/total=2.5453]         '}, 'buffer_paths': []}, 'comm_id': '150a78e67c724579a66c9f9953ffed1b'}, 'metadata': {}}
[IPKernelApp] WARNING | WARNING: attempted to send message from fork
{'header': {'msg_id': 'fd7a326f-a48486fe62b30e2d1a6e2874_313865_132', 'msg_type': 'comm_msg', 'username': 'shie44167', 'session': 'fd7a326f-a48486fe62b30e2d1a6e2874', 'date': datetime.datetime(2022, 11, 30, 11, 50, 22, 869393, tzinfo=datetime.timezone.utc), 'version': '5.3'}, 'msg_id': 'fd7a326f-a48486fe62b30e2d1a6e2874_313865_132', 'msg_type': 'comm_msg', 'parent_header': {'msg_id': '9f675616-9c730a781d572d25d31e654f_311606_12', 'msg_type': 'execute_request', 'username': 'shie44167', 'session': '9f675616-9c730a781d572d25d31e654f', 'date': datetime.datetime(2022, 11, 30, 11, 50, 21, 187004, tzinfo=tzutc()), 'version': '5.3'}, 'content': {'data': {'method': 'update', 'state': {'bar_style': 'success'}, 'buffer_paths': []}, 'comm_id': '20cfa664add0427b99ce80d20528a570'}, 'metadata': {}}
[IPKernelApp] WARNING | WARNING: attempted to send message from fork
{'header': {'msg_id': 'fd7a326f-a48486fe62b30e2d1a6e2874_313864_131', 'msg_type': 'comm_msg', 'username': 'shie44167', 'session': 'fd7a326f-a48486fe62b30e2d1a6e2874', 'date': datetime.datetime(2022, 11, 30, 11, 50, 22, 869286, tzinfo=datetime.timezone.utc), 'version': '5.3'}, 'msg_id': 'fd7a326f-a48486fe62b30e2d1a6e2874_313864_131', 'msg_type': 'comm_msg', 'parent_header': {'msg_id': '9f675616-9c730a781d572d25d31e654f_311606_12', 'msg_type': 'execute_request', 'username': 'shie44167', 'session': '9f675616-9c730a781d572d25d31e654f', 'date': datetime.datetime(2022, 11, 30, 11, 50, 21, 187004, tzinfo=tzutc()), 'version': '5.3'}, 'content': {'data': {'method': 'update', 'state': {'value': 'train          Epoch   0:  100%|| 2/2 [00:01&lt;00:00,  3.43ba/s, loss/train/total=2.5453]         '}, 'buffer_paths': []}, 'comm_id': '150a78e67c724579a66c9f9953ffed1b'}, 'metadata': {}}
[IPKernelApp] WARNING | WARNING: attempted to send message from fork
{'header': {'msg_id': 'fd7a326f-a48486fe62b30e2d1a6e2874_313864_132', 'msg_type': 'comm_msg', 'username': 'shie44167', 'session': 'fd7a326f-a48486fe62b30e2d1a6e2874', 'date': datetime.datetime(2022, 11, 30, 11, 50, 22, 869676, tzinfo=datetime.timezone.utc), 'version': '5.3'}, 'msg_id': 'fd7a326f-a48486fe62b30e2d1a6e2874_313864_132', 'msg_type': 'comm_msg', 'parent_header': {'msg_id': '9f675616-9c730a781d572d25d31e654f_311606_12', 'msg_type': 'execute_request', 'username': 'shie44167', 'session': '9f675616-9c730a781d572d25d31e654f', 'date': datetime.datetime(2022, 11, 30, 11, 50, 21, 187004, tzinfo=tzutc()), 'version': '5.3'}, 'content': {'data': {'method': 'update', 'state': {'bar_style': 'success'}, 'buffer_paths': []}, 'comm_id': '20cfa664add0427b99ce80d20528a570'}, 'metadata': {}}
[IPKernelApp] WARNING | WARNING: attempted to send message from fork
{'header': {'msg_id': 'fd7a326f-a48486fe62b30e2d1a6e2874_313875_154', 'msg_type': 'comm_msg', 'username': 'shie44167', 'session': 'fd7a326f-a48486fe62b30e2d1a6e2874', 'date': datetime.datetime(2022, 11, 30, 11, 50, 23, 383874, tzinfo=datetime.timezone.utc), 'version': '5.3'}, 'msg_id': 'fd7a326f-a48486fe62b30e2d1a6e2874_313875_154', 'msg_type': 'comm_msg', 'parent_header': {'msg_id': '9f675616-9c730a781d572d25d31e654f_311606_12', 'msg_type': 'execute_request', 'username': 'shie44167', 'session': '9f675616-9c730a781d572d25d31e654f', 'date': datetime.datetime(2022, 11, 30, 11, 50, 21, 187004, tzinfo=tzutc()), 'version': '5.3'}, 'content': {'data': {'method': 'update', 'state': {'value': 'train          Epoch   0:  100%|| 2/2 [00:01&lt;00:00,  3.43ba/s, loss/train/total=2.5453]         '}, 'buffer_paths': []}, 'comm_id': '150a78e67c724579a66c9f9953ffed1b'}, 'metadata': {}}
[IPKernelApp] WARNING | WARNING: attempted to send message from fork
{'header': {'msg_id': 'fd7a326f-a48486fe62b30e2d1a6e2874_313875_155', 'msg_type': 'comm_msg', 'username': 'shie44167', 'session': 'fd7a326f-a48486fe62b30e2d1a6e2874', 'date': datetime.datetime(2022, 11, 30, 11, 50, 23, 384308, tzinfo=datetime.timezone.utc), 'version': '5.3'}, 'msg_id': 'fd7a326f-a48486fe62b30e2d1a6e2874_313875_155', 'msg_type': 'comm_msg', 'parent_header': {'msg_id': '9f675616-9c730a781d572d25d31e654f_311606_12', 'msg_type': 'execute_request', 'username': 'shie44167', 'session': '9f675616-9c730a781d572d25d31e654f', 'date': datetime.datetime(2022, 11, 30, 11, 50, 21, 187004, tzinfo=tzutc()), 'version': '5.3'}, 'content': {'data': {'method': 'update', 'state': {'bar_style': 'success'}, 'buffer_paths': []}, 'comm_id': '20cfa664add0427b99ce80d20528a570'}, 'metadata': {}}
[IPKernelApp] WARNING | WARNING: attempted to send message from fork
{'header': {'msg_id': 'fd7a326f-a48486fe62b30e2d1a6e2874_313875_156', 'msg_type': 'comm_msg', 'username': 'shie44167', 'session': 'fd7a326f-a48486fe62b30e2d1a6e2874', 'date': datetime.datetime(2022, 11, 30, 11, 50, 23, 384566, tzinfo=datetime.timezone.utc), 'version': '5.3'}, 'msg_id': 'fd7a326f-a48486fe62b30e2d1a6e2874_313875_156', 'msg_type': 'comm_msg', 'parent_header': {'msg_id': '9f675616-9c730a781d572d25d31e654f_311606_12', 'msg_type': 'execute_request', 'username': 'shie44167', 'session': '9f675616-9c730a781d572d25d31e654f', 'date': datetime.datetime(2022, 11, 30, 11, 50, 21, 187004, tzinfo=tzutc()), 'version': '5.3'}, 'content': {'data': {'method': 'update', 'state': {'bar_style': 'success'}, 'buffer_paths': []}, 'comm_id': '4ace1d32dcd8408397690912597ff19c'}, 'metadata': {}}
[IPKernelApp] WARNING | WARNING: attempted to send message from fork
{'header': {'msg_id': 'fd7a326f-a48486fe62b30e2d1a6e2874_313874_154', 'msg_type': 'comm_msg', 'username': 'shie44167', 'session': 'fd7a326f-a48486fe62b30e2d1a6e2874', 'date': datetime.datetime(2022, 11, 30, 11, 50, 23, 387454, tzinfo=datetime.timezone.utc), 'version': '5.3'}, 'msg_id': 'fd7a326f-a48486fe62b30e2d1a6e2874_313874_154', 'msg_type': 'comm_msg', 'parent_header': {'msg_id': '9f675616-9c730a781d572d25d31e654f_311606_12', 'msg_type': 'execute_request', 'username': 'shie44167', 'session': '9f675616-9c730a781d572d25d31e654f', 'date': datetime.datetime(2022, 11, 30, 11, 50, 21, 187004, tzinfo=tzutc()), 'version': '5.3'}, 'content': {'data': {'method': 'update', 'state': {'value': 'train          Epoch   0:  100%|| 2/2 [00:01&lt;00:00,  3.43ba/s, loss/train/total=2.5453]         '}, 'buffer_paths': []}, 'comm_id': '150a78e67c724579a66c9f9953ffed1b'}, 'metadata': {}}
[IPKernelApp] WARNING | WARNING: attempted to send message from fork
{'header': {'msg_id': 'fd7a326f-a48486fe62b30e2d1a6e2874_313874_155', 'msg_type': 'comm_msg', 'username': 'shie44167', 'session': 'fd7a326f-a48486fe62b30e2d1a6e2874', 'date': datetime.datetime(2022, 11, 30, 11, 50, 23, 387854, tzinfo=datetime.timezone.utc), 'version': '5.3'}, 'msg_id': 'fd7a326f-a48486fe62b30e2d1a6e2874_313874_155', 'msg_type': 'comm_msg', 'parent_header': {'msg_id': '9f675616-9c730a781d572d25d31e654f_311606_12', 'msg_type': 'execute_request', 'username': 'shie44167', 'session': '9f675616-9c730a781d572d25d31e654f', 'date': datetime.datetime(2022, 11, 30, 11, 50, 21, 187004, tzinfo=tzutc()), 'version': '5.3'}, 'content': {'data': {'method': 'update', 'state': {'bar_style': 'success'}, 'buffer_paths': []}, 'comm_id': '20cfa664add0427b99ce80d20528a570'}, 'metadata': {}}
[IPKernelApp] WARNING | WARNING: attempted to send message from fork
{'header': {'msg_id': 'fd7a326f-a48486fe62b30e2d1a6e2874_313874_156', 'msg_type': 'comm_msg', 'username': 'shie44167', 'session': 'fd7a326f-a48486fe62b30e2d1a6e2874', 'date': datetime.datetime(2022, 11, 30, 11, 50, 23, 388112, tzinfo=datetime.timezone.utc), 'version': '5.3'}, 'msg_id': 'fd7a326f-a48486fe62b30e2d1a6e2874_313874_156', 'msg_type': 'comm_msg', 'parent_header': {'msg_id': '9f675616-9c730a781d572d25d31e654f_311606_12', 'msg_type': 'execute_request', 'username': 'shie44167', 'session': '9f675616-9c730a781d572d25d31e654f', 'date': datetime.datetime(2022, 11, 30, 11, 50, 21, 187004, tzinfo=tzutc()), 'version': '5.3'}, 'content': {'data': {'method': 'update', 'state': {'bar_style': 'success'}, 'buffer_paths': []}, 'comm_id': '4ace1d32dcd8408397690912597ff19c'}, 'metadata': {}}
_ test_notebook[gpu-/home/shie44167/workspace/1_read_codes/composer/composer/../examples/getting_started.ipynb] _

self = <testbook.client.TestbookNotebookClient object at 0x7f6794abcf10>
cell = [21], kwargs = {}, cell_indexes = [21], executed_cells = [], idx = 21

    def execute_cell(self, cell, **kwargs) -> Union[Dict, List[Dict]]:
        """
        Executes a cell or list of cells
        """
        if isinstance(cell, slice):
            start, stop = self._cell_index(cell.start), self._cell_index(cell.stop)
            if cell.step is not None:
                raise TestbookError('testbook does not support step argument')
    
            cell = range(start, stop + 1)
        elif isinstance(cell, str) or isinstance(cell, int):
            cell = [cell]
    
        cell_indexes = cell
    
        if all(isinstance(x, str) for x in cell):
            cell_indexes = [self._cell_index(tag) for tag in cell]
    
        executed_cells = []
        for idx in cell_indexes:
            try:
>               cell = super().execute_cell(self.nb['cells'][idx], idx, **kwargs)

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/testbook/client.py:133: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<testbook.client.TestbookNotebookClient object at 0x7f6794abcf10>, {'cell_type': 'code', 'execution_count': 11, 'meta...loop in action!\nend_time = time.perf_counter()\nprint(f"It took {end_time - start_time:0.4f} seconds to train")'}, 21)
kwargs = {}, name = 'MainThread'
inner = <coroutine object NotebookClient.async_execute_cell at 0x7f6796a48e40>
loop = <_UnixSelectorEventLoop running=False closed=False debug=False>

    def wrapped(*args, **kwargs):
        name = threading.current_thread().name
        inner = coro(*args, **kwargs)
        try:
            # If a loop is currently running in this thread,
            # use a task runner.
            asyncio.get_running_loop()
            if name not in _runner_map:
                _runner_map[name] = _TaskRunner()
            return _runner_map[name].run(inner)
        except RuntimeError:
            pass
    
        # Run the loop for this thread.
        if name not in _loop_map:
            _loop_map[name] = asyncio.new_event_loop()
        loop = _loop_map[name]
>       return loop.run_until_complete(inner)

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/jupyter_core/utils/__init__.py:160: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_UnixSelectorEventLoop running=False closed=False debug=False>
future = <Task finished name='Task-482' coro=<NotebookClient.async_execute_cell() done, defined at /home/shie44167/miniconda3/e...ing max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n')>

    def run_until_complete(self, future):
        """Run until the Future is done.
    
        If the argument is a coroutine, it is wrapped in a Task.
    
        WARNING: It would be disastrous to call run_until_complete()
        with the same coroutine twice -- it would wrap it in two
        different Tasks and that can't be good.
    
        Return the Future's result, or raise its exception.
        """
        self._check_closed()
        self._check_running()
    
        new_task = not futures.isfuture(future)
        future = tasks.ensure_future(future, loop=self)
        if new_task:
            # An exception is raised if the future didn't complete, so there
            # is no need to log the "destroy pending task" message
            future._log_destroy_pending = False
    
        future.add_done_callback(_run_until_complete_cb)
        try:
            self.run_forever()
        except:
            if new_task and future.done() and not future.cancelled():
                # The coroutine raised a BaseException. Consume the exception
                # to not log a warning, the caller doesn't have access to the
                # local task.
                future.exception()
            raise
        finally:
            future.remove_done_callback(_run_until_complete_cb)
        if not future.done():
            raise RuntimeError('Event loop stopped before Future completed.')
    
>       return future.result()

/home/shie44167/miniconda3/envs/composer/lib/python3.10/asyncio/base_events.py:649: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <testbook.client.TestbookNotebookClient object at 0x7f6794abcf10>
cell = {'cell_type': 'code', 'execution_count': 11, 'metadata': {'execution': {'iopub.status.busy': '2022-11-30T11:51:21.6622...ning loop in action!\nend_time = time.perf_counter()\nprint(f"It took {end_time - start_time:0.4f} seconds to train")'}
cell_index = 21, execution_count = None, store_history = True

    async def async_execute_cell(
        self,
        cell: NotebookNode,
        cell_index: int,
        execution_count: t.Optional[int] = None,
        store_history: bool = True,
    ) -> NotebookNode:
        """
        Executes a single code cell.
    
        To execute all cells see :meth:`execute`.
    
        Parameters
        ----------
        cell : nbformat.NotebookNode
            The cell which is currently being processed.
        cell_index : int
            The position of the cell within the notebook object.
        execution_count : int
            The execution count to be assigned to the cell (default: Use kernel response)
        store_history : bool
            Determines if history should be stored in the kernel (default: False).
            Specific to ipython kernels, which can store command histories.
    
        Returns
        -------
        output : dict
            The execution output payload (or None for no output).
    
        Raises
        ------
        CellExecutionError
            If execution failed and should raise an exception, this will be raised
            with defaults about the failure.
    
        Returns
        -------
        cell : NotebookNode
            The cell which was just processed.
        """
        assert self.kc is not None
    
        await run_hook(self.on_cell_start, cell=cell, cell_index=cell_index)
    
        if cell.cell_type != 'code' or not cell.source.strip():
            self.log.debug("Skipping non-executing cell %s", cell_index)
            return cell
    
        if self.skip_cells_with_tag in cell.metadata.get("tags", []):
            self.log.debug("Skipping tagged cell %s", cell_index)
            return cell
    
        if self.record_timing:  # clear execution metadata prior to execution
            cell['metadata']['execution'] = {}
    
        self.log.debug("Executing cell:\n%s", cell.source)
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors or "raises-exception" in cell.metadata.get("tags", [])
        )
    
        await run_hook(self.on_cell_execute, cell=cell, cell_index=cell_index)
        parent_msg_id = await ensure_async(
            self.kc.execute(
                cell.source, store_history=store_history, stop_on_error=not cell_allows_errors
            )
        )
        await run_hook(self.on_cell_complete, cell=cell, cell_index=cell_index)
        # We launched a code cell to execute
        self.code_cells_executed += 1
        exec_timeout = self._get_timeout(cell)
    
        cell.outputs = []
        self.clear_before_next_output = False
    
        task_poll_kernel_alive = asyncio.ensure_future(self._async_poll_kernel_alive())
        task_poll_output_msg = asyncio.ensure_future(
            self._async_poll_output_msg(parent_msg_id, cell, cell_index)
        )
        self.task_poll_for_reply = asyncio.ensure_future(
            self._async_poll_for_reply(
                parent_msg_id, cell, exec_timeout, task_poll_output_msg, task_poll_kernel_alive
            )
        )
        try:
            exec_reply = await self.task_poll_for_reply
        except asyncio.CancelledError:
            # can only be cancelled by task_poll_kernel_alive when the kernel is dead
            task_poll_output_msg.cancel()
            raise DeadKernelError("Kernel died")
        except Exception as e:
            # Best effort to cancel request if it hasn't been resolved
            try:
                # Check if the task_poll_output is doing the raising for us
                if not isinstance(e, CellControlSignal):
                    task_poll_output_msg.cancel()
            finally:
                raise
    
        if execution_count:
            cell['execution_count'] = execution_count
        await run_hook(
            self.on_cell_executed, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
>       await self._check_raise_for_error(cell, cell_index, exec_reply)

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/nbclient/client.py:1021: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <testbook.client.TestbookNotebookClient object at 0x7f6794abcf10>
cell = {'cell_type': 'code', 'execution_count': 11, 'metadata': {'execution': {'iopub.status.busy': '2022-11-30T11:51:21.6622...ning loop in action!\nend_time = time.perf_counter()\nprint(f"It took {end_time - start_time:0.4f} seconds to train")'}
cell_index = 21
exec_reply = {'buffers': [], 'content': {'ename': 'RuntimeError', 'engine_info': {'engine_id': -1, 'engine_uuid': '29484bac-6750-4d...e, 'engine': '29484bac-6750-4d2e-904f-c3a2e024b434', 'started': '2022-11-30T11:51:21.662340Z', 'status': 'error'}, ...}

    async def _check_raise_for_error(
        self, cell: NotebookNode, cell_index: int, exec_reply: t.Optional[t.Dict]
    ) -> None:
    
        if exec_reply is None:
            return None
    
        exec_reply_content = exec_reply['content']
        if exec_reply_content['status'] != 'error':
            return None
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors
            or exec_reply_content.get('ename') in self.allow_error_names
            or "raises-exception" in cell.metadata.get("tags", [])
        )
        await run_hook(
            self.on_cell_error, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
        if not cell_allows_errors:
>           raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
E           nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
E           ------------------
E           start_time = time.perf_counter()
E           trainer.fit() # <-- Your training loop in action!
E           end_time = time.perf_counter()
E           print(f"It took {end_time - start_time:0.4f} seconds to train")
E           ------------------
E           
E           [0;31m---------------------------------------------------------------------------[0m
E           [0;31mRuntimeError[0m                              Traceback (most recent call last)
E           [0;32m/tmp/ipykernel_315223/1511601347.py[0m in [0;36m<cell line: 2>[0;34m()[0m
E           [1;32m      1[0m [0mstart_time[0m [0;34m=[0m [0mtime[0m[0;34m.[0m[0mperf_counter[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m----> 2[0;31m [0mtrainer[0m[0;34m.[0m[0mfit[0m[0;34m([0m[0;34m)[0m [0;31m# <-- Your training loop in action![0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m      3[0m [0mend_time[0m [0;34m=[0m [0mtime[0m[0;34m.[0m[0mperf_counter[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m      4[0m [0mprint[0m[0;34m([0m[0;34mf"It took {end_time - start_time:0.4f} seconds to train"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m/tmp/ipykernel_315223/4080402710.py[0m in [0;36mnew_fit[0;34m(self, *args, **kwargs)[0m
E           [1;32m     19[0m             [0;32mif[0m [0;34m'eval_subset_num_batches'[0m [0;32mnot[0m [0;32min[0m [0mkwargs[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m     20[0m                 [0mkwargs[0m[0;34m[[0m[0;34m'eval_subset_num_batches'[0m[0;34m][0m [0;34m=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m---> 21[0;31m         [0moriginal_fit[0m[0;34m([0m[0mself[0m[0;34m,[0m [0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m     22[0m [0;34m[0m[0m
E           [1;32m     23[0m     [0mTrainer[0m[0;34m.[0m[0mfit[0m [0;34m=[0m [0mnew_fit[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36mfit[0;34m(self, train_dataloader, train_dataloader_label, train_subset_num_batches, duration, reset_time, schedulers, scale_schedule_ratio, step_schedulers_every_batch, eval_dataloader, eval_subset_num_batches, eval_interval, grad_accum, precision)[0m
E           [1;32m   1632[0m             [0;31m# update scaler since precision was provided[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1633[0m             [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mscaler[0m [0;34m=[0m [0mClosureGradScaler[0m[0;34m([0m[0;34m)[0m [0;32mif[0m [0mself[0m[0;34m.[0m[0m_use_closures[0m[0;34m([0m[0;34m)[0m [0;32melse[0m [0mGradScaler[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m-> 1634[0;31m         [0mself[0m[0;34m.[0m[0m_train_loop[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1635[0m [0;34m[0m[0m
E           [1;32m   1636[0m     [0;32mdef[0m [0mclose[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_loop[0;34m(self)[0m
E           [1;32m   1790[0m                     })
E           [1;32m   1791[0m [0;34m[0m[0m
E           [0;32m-> 1792[0;31m                     [0mtotal_loss_dict[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_train_batch[0m[0;34m([0m[0muse_grad_scaling[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1793[0m [0;34m[0m[0m
E           [1;32m   1794[0m                     [0;32mif[0m [0muse_grad_scaling[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_batch[0;34m(self, use_grad_scaling)[0m
E           [1;32m   1961[0m                                 microbatches, total_loss_dict, **kwargs).item())
E           [1;32m   1962[0m                 [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m-> 1963[0;31m                     [0mself[0m[0;34m.[0m[0m_train_microbatches[0m[0;34m([0m[0mmicrobatches[0m[0;34m,[0m [0mtotal_loss_dict[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1964[0m                     [0;32mif[0m [0;32mnot[0m [0mself[0m[0;34m.[0m[0mdeepspeed_enabled[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1965[0m                         [0;32mfor[0m [0moptimizer[0m [0;32min[0m [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0moptimizers[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_microbatches[0;34m(self, microbatches, total_loss_dict, ddp_sync)[0m
E           [1;32m   2032[0m             [0;32mfor[0m [0mmicrobatch_idx[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mbatch[0m [0;32min[0m [0menumerate[0m[0;34m([0m[0mmicrobatches[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   2033[0m                 [0mis_final_microbatch[0m [0;34m=[0m [0mmicrobatch_idx[0m [0;34m+[0m [0;36m1[0m [0;34m==[0m [0mlen[0m[0;34m([0m[0mmicrobatches[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m-> 2034[0;31m                 [0mmicrobatch_loss_dict[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_train_microbatch[0m[0;34m([0m[0muse_grad_scaling[0m[0;34m,[0m [0mcurrent_batch_size[0m[0;34m,[0m [0mis_final_microbatch[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   2035[0m [0;34m[0m[0m
E           [1;32m   2036[0m                 [0;31m# Aggregate each loss in microbatch_loss_dict into total_loss_dict[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_microbatch[0;34m(self, use_grad_scaling, current_batch_size, is_final_microbatch)[0m
E           [1;32m   2078[0m [0;34m[0m[0m
E           [1;32m   2079[0m             [0;32mwith[0m [0mget_precision_context[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mprecision[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m-> 2080[0;31m                 [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0moutputs[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mmodel[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mbatch[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   2081[0m [0;34m[0m[0m
E           [1;32m   2082[0m             [0mself[0m[0;34m.[0m[0mengine[0m[0;34m.[0m[0mrun_event[0m[0;34m([0m[0mEvent[0m[0;34m.[0m[0mAFTER_FORWARD[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E           [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E           [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E           [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/models/tasks/classification.py[0m in [0;36mforward[0;34m(self, batch)[0m
E           [1;32m    104[0m     [0;32mdef[0m [0mforward[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mbatch[0m[0;34m:[0m [0mTuple[0m[0;34m[[0m[0mTensor[0m[0;34m,[0m [0mAny[0m[0;34m][0m[0;34m)[0m [0;34m->[0m [0mTensor[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m    105[0m         [0minputs[0m[0;34m,[0m [0m_[0m [0;34m=[0m [0mbatch[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m--> 106[0;31m         [0moutputs[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mmodule[0m[0;34m([0m[0minputs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m    107[0m         [0;32mreturn[0m [0moutputs[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m    108[0m [0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E           [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E           [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E           [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/models/resnet_cifar/resnets.py[0m in [0;36mforward[0;34m(self, x)[0m
E           [1;32m     85[0m     [0;32mdef[0m [0mforward[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mx[0m[0;34m:[0m [0mtorch[0m[0;34m.[0m[0mTensor[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m     86[0m         [0mout[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mrelu[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mbn[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mconv[0m[0;34m([0m[0mx[0m[0;34m)[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m---> 87[0;31m         [0mout[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mblocks[0m[0;34m([0m[0mout[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m     88[0m         [0mout[0m [0;34m=[0m [0mF[0m[0;34m.[0m[0mavg_pool2d[0m[0;34m([0m[0mout[0m[0;34m,[0m [0mout[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;36m3[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m     89[0m         [0mout[0m [0;34m=[0m [0mout[0m[0;34m.[0m[0mview[0m[0;34m([0m[0mout[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;36m0[0m[0;34m)[0m[0;34m,[0m [0;34m-[0m[0;36m1[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E           [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E           [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E           [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/container.py[0m in [0;36mforward[0;34m(self, input)[0m
E           [1;32m    137[0m     [0;32mdef[0m [0mforward[0m[0;34m([0m[0mself[0m[0;34m,[0m [0minput[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m    138[0m         [0;32mfor[0m [0mmodule[0m [0;32min[0m [0mself[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m--> 139[0;31m             [0minput[0m [0;34m=[0m [0mmodule[0m[0;34m([0m[0minput[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m    140[0m         [0;32mreturn[0m [0minput[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m    141[0m [0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E           [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E           [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E           [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/models/resnet_cifar/resnets.py[0m in [0;36mforward[0;34m(self, x)[0m
E           [1;32m     49[0m         [0;32mdef[0m [0mforward[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mx[0m[0;34m:[0m [0mtorch[0m[0;34m.[0m[0mTensor[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m     50[0m             [0mout[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mrelu[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mbn1[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mconv1[0m[0;34m([0m[0mx[0m[0;34m)[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m---> 51[0;31m             [0mout[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mbn2[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mconv2[0m[0;34m([0m[0mout[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m     52[0m             [0mout[0m [0;34m+=[0m [0mself[0m[0;34m.[0m[0mshortcut[0m[0;34m([0m[0mx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m     53[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0mrelu[0m[0;34m([0m[0mout[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E           [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E           [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E           [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/conv.py[0m in [0;36mforward[0;34m(self, input)[0m
E           [1;32m    455[0m [0;34m[0m[0m
E           [1;32m    456[0m     [0;32mdef[0m [0mforward[0m[0;34m([0m[0mself[0m[0;34m,[0m [0minput[0m[0;34m:[0m [0mTensor[0m[0;34m)[0m [0;34m->[0m [0mTensor[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m--> 457[0;31m         [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_conv_forward[0m[0;34m([0m[0minput[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0mweight[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0mbias[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m    458[0m [0;34m[0m[0m
E           [1;32m    459[0m [0;32mclass[0m [0mConv3d[0m[0;34m([0m[0m_ConvNd[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/conv.py[0m in [0;36m_conv_forward[0;34m(self, input, weight, bias)[0m
E           [1;32m    451[0m                             [0mweight[0m[0;34m,[0m [0mbias[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0mstride[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m    452[0m                             _pair(0), self.dilation, self.groups)
E           [0;32m--> 453[0;31m         return F.conv2d(input, weight, bias, self.stride,
E           [0m[1;32m    454[0m                         self.padding, self.dilation, self.groups)
E           [1;32m    455[0m [0;34m[0m[0m
E           
E           [0;31mRuntimeError[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 7.76 GiB total capacity; 1.58 GiB already allocated; 53.38 MiB free; 1.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
E           RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 7.76 GiB total capacity; 1.58 GiB already allocated; 53.38 MiB free; 1.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/nbclient/client.py:915: CellExecutionError

During handling of the above exception, another exception occurred:

notebook = '/home/shie44167/workspace/1_read_codes/composer/composer/../examples/getting_started.ipynb'
device = 'gpu', s3_bucket = 'my-bucket'

    @pytest.mark.parametrize('notebook', [_to_pytest_param(notebook) for notebook in NOTEBOOKS])
    @device('cpu', 'gpu')
    @pytest.mark.daily
    def test_notebook(notebook: str, device: str, s3_bucket: str):
        trainer_monkeypatch_code = inspect.getsource(patch_notebooks)
        notebook_name = os.path.split(notebook)[-1][:-len('.ipynb')]
        if notebook_name == 'medical_image_segmentation':
            pytest.xfail('Dataset is only available via kaggle; need to authenticate on ci/cd')
        if notebook_name == 'auto_grad_accum' and device == 'cpu':
            pytest.skip('auto_grad_accum notebook only runs with a gpu')
        if notebook_name == 'TPU_Training_in_composer':
            pytest.skip('The CI does not support tpus')
        if notebook_name == 'ffcv_dataloaders' and device == 'cpu':
            pytest.skip('The FFCV notebook requires CUDA')
        if notebook_name == 'streaming_dataloader_facesynthetics':
            pytest.skip('Jenkins is killing this notebook for some reason, it should work locally')
        if notebook_name == 'training_without_local_storage':
            pytest.skip('Jenkins is not getting the S3 credentials set up properly, it should work locally')
        with testbook.testbook(notebook) as tb:
            tb.inject(trainer_monkeypatch_code)
            tb.inject('patch_notebooks()')
            for i, cell in enumerate(tb.cells):
                if cell['cell_type'] != 'code':
                    continue
                cell['source'] = modify_cell_source(tb,
                                                    notebook_name=notebook_name,
                                                    cell_source=cell['source'],
                                                    s3_bucket=s3_bucket)
>               tb.execute_cell(i)

/home/shie44167/workspace/1_read_codes/composer/tests/test_notebooks.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <testbook.client.TestbookNotebookClient object at 0x7f6794abcf10>
cell = [21], kwargs = {}, cell_indexes = [21], executed_cells = [], idx = 21

    def execute_cell(self, cell, **kwargs) -> Union[Dict, List[Dict]]:
        """
        Executes a cell or list of cells
        """
        if isinstance(cell, slice):
            start, stop = self._cell_index(cell.start), self._cell_index(cell.stop)
            if cell.step is not None:
                raise TestbookError('testbook does not support step argument')
    
            cell = range(start, stop + 1)
        elif isinstance(cell, str) or isinstance(cell, int):
            cell = [cell]
    
        cell_indexes = cell
    
        if all(isinstance(x, str) for x in cell):
            cell_indexes = [self._cell_index(tag) for tag in cell]
    
        executed_cells = []
        for idx in cell_indexes:
            try:
                cell = super().execute_cell(self.nb['cells'][idx], idx, **kwargs)
            except CellExecutionError as ce:
>               raise TestbookRuntimeError(ce.evalue, ce, self._get_error_class(ce.ename))
E               testbook.exceptions.TestbookRuntimeError: An error occurred while executing the following cell:
E               ------------------
E               start_time = time.perf_counter()
E               trainer.fit() # <-- Your training loop in action!
E               end_time = time.perf_counter()
E               print(f"It took {end_time - start_time:0.4f} seconds to train")
E               ------------------
E               
E               [0;31m---------------------------------------------------------------------------[0m
E               [0;31mRuntimeError[0m                              Traceback (most recent call last)
E               [0;32m/tmp/ipykernel_315223/1511601347.py[0m in [0;36m<cell line: 2>[0;34m()[0m
E               [1;32m      1[0m [0mstart_time[0m [0;34m=[0m [0mtime[0m[0;34m.[0m[0mperf_counter[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m----> 2[0;31m [0mtrainer[0m[0;34m.[0m[0mfit[0m[0;34m([0m[0;34m)[0m [0;31m# <-- Your training loop in action![0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m      3[0m [0mend_time[0m [0;34m=[0m [0mtime[0m[0;34m.[0m[0mperf_counter[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m      4[0m [0mprint[0m[0;34m([0m[0;34mf"It took {end_time - start_time:0.4f} seconds to train"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m/tmp/ipykernel_315223/4080402710.py[0m in [0;36mnew_fit[0;34m(self, *args, **kwargs)[0m
E               [1;32m     19[0m             [0;32mif[0m [0;34m'eval_subset_num_batches'[0m [0;32mnot[0m [0;32min[0m [0mkwargs[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m     20[0m                 [0mkwargs[0m[0;34m[[0m[0;34m'eval_subset_num_batches'[0m[0;34m][0m [0;34m=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m---> 21[0;31m         [0moriginal_fit[0m[0;34m([0m[0mself[0m[0;34m,[0m [0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m     22[0m [0;34m[0m[0m
E               [1;32m     23[0m     [0mTrainer[0m[0;34m.[0m[0mfit[0m [0;34m=[0m [0mnew_fit[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36mfit[0;34m(self, train_dataloader, train_dataloader_label, train_subset_num_batches, duration, reset_time, schedulers, scale_schedule_ratio, step_schedulers_every_batch, eval_dataloader, eval_subset_num_batches, eval_interval, grad_accum, precision)[0m
E               [1;32m   1632[0m             [0;31m# update scaler since precision was provided[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1633[0m             [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mscaler[0m [0;34m=[0m [0mClosureGradScaler[0m[0;34m([0m[0;34m)[0m [0;32mif[0m [0mself[0m[0;34m.[0m[0m_use_closures[0m[0;34m([0m[0;34m)[0m [0;32melse[0m [0mGradScaler[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m-> 1634[0;31m         [0mself[0m[0;34m.[0m[0m_train_loop[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1635[0m [0;34m[0m[0m
E               [1;32m   1636[0m     [0;32mdef[0m [0mclose[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_loop[0;34m(self)[0m
E               [1;32m   1790[0m                     })
E               [1;32m   1791[0m [0;34m[0m[0m
E               [0;32m-> 1792[0;31m                     [0mtotal_loss_dict[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_train_batch[0m[0;34m([0m[0muse_grad_scaling[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1793[0m [0;34m[0m[0m
E               [1;32m   1794[0m                     [0;32mif[0m [0muse_grad_scaling[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_batch[0;34m(self, use_grad_scaling)[0m
E               [1;32m   1961[0m                                 microbatches, total_loss_dict, **kwargs).item())
E               [1;32m   1962[0m                 [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m-> 1963[0;31m                     [0mself[0m[0;34m.[0m[0m_train_microbatches[0m[0;34m([0m[0mmicrobatches[0m[0;34m,[0m [0mtotal_loss_dict[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1964[0m                     [0;32mif[0m [0;32mnot[0m [0mself[0m[0;34m.[0m[0mdeepspeed_enabled[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1965[0m                         [0;32mfor[0m [0moptimizer[0m [0;32min[0m [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0moptimizers[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_microbatches[0;34m(self, microbatches, total_loss_dict, ddp_sync)[0m
E               [1;32m   2032[0m             [0;32mfor[0m [0mmicrobatch_idx[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mbatch[0m [0;32min[0m [0menumerate[0m[0;34m([0m[0mmicrobatches[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   2033[0m                 [0mis_final_microbatch[0m [0;34m=[0m [0mmicrobatch_idx[0m [0;34m+[0m [0;36m1[0m [0;34m==[0m [0mlen[0m[0;34m([0m[0mmicrobatches[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m-> 2034[0;31m                 [0mmicrobatch_loss_dict[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_train_microbatch[0m[0;34m([0m[0muse_grad_scaling[0m[0;34m,[0m [0mcurrent_batch_size[0m[0;34m,[0m [0mis_final_microbatch[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   2035[0m [0;34m[0m[0m
E               [1;32m   2036[0m                 [0;31m# Aggregate each loss in microbatch_loss_dict into total_loss_dict[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_microbatch[0;34m(self, use_grad_scaling, current_batch_size, is_final_microbatch)[0m
E               [1;32m   2078[0m [0;34m[0m[0m
E               [1;32m   2079[0m             [0;32mwith[0m [0mget_precision_context[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mprecision[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m-> 2080[0;31m                 [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0moutputs[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mmodel[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mbatch[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   2081[0m [0;34m[0m[0m
E               [1;32m   2082[0m             [0mself[0m[0;34m.[0m[0mengine[0m[0;34m.[0m[0mrun_event[0m[0;34m([0m[0mEvent[0m[0;34m.[0m[0mAFTER_FORWARD[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E               [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E               [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E               [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/models/tasks/classification.py[0m in [0;36mforward[0;34m(self, batch)[0m
E               [1;32m    104[0m     [0;32mdef[0m [0mforward[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mbatch[0m[0;34m:[0m [0mTuple[0m[0;34m[[0m[0mTensor[0m[0;34m,[0m [0mAny[0m[0;34m][0m[0;34m)[0m [0;34m->[0m [0mTensor[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m    105[0m         [0minputs[0m[0;34m,[0m [0m_[0m [0;34m=[0m [0mbatch[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m--> 106[0;31m         [0moutputs[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mmodule[0m[0;34m([0m[0minputs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m    107[0m         [0;32mreturn[0m [0moutputs[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m    108[0m [0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E               [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E               [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E               [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/models/resnet_cifar/resnets.py[0m in [0;36mforward[0;34m(self, x)[0m
E               [1;32m     85[0m     [0;32mdef[0m [0mforward[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mx[0m[0;34m:[0m [0mtorch[0m[0;34m.[0m[0mTensor[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m     86[0m         [0mout[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mrelu[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mbn[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mconv[0m[0;34m([0m[0mx[0m[0;34m)[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m---> 87[0;31m         [0mout[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mblocks[0m[0;34m([0m[0mout[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m     88[0m         [0mout[0m [0;34m=[0m [0mF[0m[0;34m.[0m[0mavg_pool2d[0m[0;34m([0m[0mout[0m[0;34m,[0m [0mout[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0;36m3[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m     89[0m         [0mout[0m [0;34m=[0m [0mout[0m[0;34m.[0m[0mview[0m[0;34m([0m[0mout[0m[0;34m.[0m[0msize[0m[0;34m([0m[0;36m0[0m[0;34m)[0m[0;34m,[0m [0;34m-[0m[0;36m1[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E               [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E               [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E               [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/container.py[0m in [0;36mforward[0;34m(self, input)[0m
E               [1;32m    137[0m     [0;32mdef[0m [0mforward[0m[0;34m([0m[0mself[0m[0;34m,[0m [0minput[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m    138[0m         [0;32mfor[0m [0mmodule[0m [0;32min[0m [0mself[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m--> 139[0;31m             [0minput[0m [0;34m=[0m [0mmodule[0m[0;34m([0m[0minput[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m    140[0m         [0;32mreturn[0m [0minput[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m    141[0m [0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E               [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E               [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E               [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/models/resnet_cifar/resnets.py[0m in [0;36mforward[0;34m(self, x)[0m
E               [1;32m     49[0m         [0;32mdef[0m [0mforward[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mx[0m[0;34m:[0m [0mtorch[0m[0;34m.[0m[0mTensor[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m     50[0m             [0mout[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mrelu[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mbn1[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mconv1[0m[0;34m([0m[0mx[0m[0;34m)[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m---> 51[0;31m             [0mout[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mbn2[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mconv2[0m[0;34m([0m[0mout[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m     52[0m             [0mout[0m [0;34m+=[0m [0mself[0m[0;34m.[0m[0mshortcut[0m[0;34m([0m[0mx[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m     53[0m             [0;32mreturn[0m [0mself[0m[0;34m.[0m[0mrelu[0m[0;34m([0m[0mout[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E               [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E               [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E               [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/conv.py[0m in [0;36mforward[0;34m(self, input)[0m
E               [1;32m    455[0m [0;34m[0m[0m
E               [1;32m    456[0m     [0;32mdef[0m [0mforward[0m[0;34m([0m[0mself[0m[0;34m,[0m [0minput[0m[0;34m:[0m [0mTensor[0m[0;34m)[0m [0;34m->[0m [0mTensor[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m--> 457[0;31m         [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_conv_forward[0m[0;34m([0m[0minput[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0mweight[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0mbias[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m    458[0m [0;34m[0m[0m
E               [1;32m    459[0m [0;32mclass[0m [0mConv3d[0m[0;34m([0m[0m_ConvNd[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/conv.py[0m in [0;36m_conv_forward[0;34m(self, input, weight, bias)[0m
E               [1;32m    451[0m                             [0mweight[0m[0;34m,[0m [0mbias[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0mstride[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m    452[0m                             _pair(0), self.dilation, self.groups)
E               [0;32m--> 453[0;31m         return F.conv2d(input, weight, bias, self.stride,
E               [0m[1;32m    454[0m                         self.padding, self.dilation, self.groups)
E               [1;32m    455[0m [0;34m[0m[0m
E               
E               [0;31mRuntimeError[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 7.76 GiB total capacity; 1.58 GiB already allocated; 53.38 MiB free; 1.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
E               RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 7.76 GiB total capacity; 1.58 GiB already allocated; 53.38 MiB free; 1.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/testbook/client.py:135: TestbookRuntimeError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
_ test_notebook[gpu-/home/shie44167/workspace/1_read_codes/composer/composer/../examples/huggingface_models.ipynb] _

self = <testbook.client.TestbookNotebookClient object at 0x7f6794abd7e0>
cell = [18], kwargs = {}, cell_indexes = [18], executed_cells = [], idx = 18

    def execute_cell(self, cell, **kwargs) -> Union[Dict, List[Dict]]:
        """
        Executes a cell or list of cells
        """
        if isinstance(cell, slice):
            start, stop = self._cell_index(cell.start), self._cell_index(cell.stop)
            if cell.step is not None:
                raise TestbookError('testbook does not support step argument')
    
            cell = range(start, stop + 1)
        elif isinstance(cell, str) or isinstance(cell, int):
            cell = [cell]
    
        cell_indexes = cell
    
        if all(isinstance(x, str) for x in cell):
            cell_indexes = [self._cell_index(tag) for tag in cell]
    
        executed_cells = []
        for idx in cell_indexes:
            try:
>               cell = super().execute_cell(self.nb['cells'][idx], idx, **kwargs)

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/testbook/client.py:133: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<testbook.client.TestbookNotebookClient object at 0x7f6794abd7e0>, {'cell_type': 'code', 'execution_count': 9, 'metad...\',\n    train_subset_num_batches=150,\n    precision=\'fp32\',\n    seed=17\n)\n# Start training\ntrainer.fit()'}, 18)
kwargs = {}, name = 'MainThread'
inner = <coroutine object NotebookClient.async_execute_cell at 0x7f6794ad55b0>
loop = <_UnixSelectorEventLoop running=False closed=False debug=False>

    def wrapped(*args, **kwargs):
        name = threading.current_thread().name
        inner = coro(*args, **kwargs)
        try:
            # If a loop is currently running in this thread,
            # use a task runner.
            asyncio.get_running_loop()
            if name not in _runner_map:
                _runner_map[name] = _TaskRunner()
            return _runner_map[name].run(inner)
        except RuntimeError:
            pass
    
        # Run the loop for this thread.
        if name not in _loop_map:
            _loop_map[name] = asyncio.new_event_loop()
        loop = _loop_map[name]
>       return loop.run_until_complete(inner)

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/jupyter_core/utils/__init__.py:160: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_UnixSelectorEventLoop running=False closed=False debug=False>
future = <Task finished name='Task-524' coro=<NotebookClient.async_execute_cell() done, defined at /home/shie44167/miniconda3/e...ing max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n')>

    def run_until_complete(self, future):
        """Run until the Future is done.
    
        If the argument is a coroutine, it is wrapped in a Task.
    
        WARNING: It would be disastrous to call run_until_complete()
        with the same coroutine twice -- it would wrap it in two
        different Tasks and that can't be good.
    
        Return the Future's result, or raise its exception.
        """
        self._check_closed()
        self._check_running()
    
        new_task = not futures.isfuture(future)
        future = tasks.ensure_future(future, loop=self)
        if new_task:
            # An exception is raised if the future didn't complete, so there
            # is no need to log the "destroy pending task" message
            future._log_destroy_pending = False
    
        future.add_done_callback(_run_until_complete_cb)
        try:
            self.run_forever()
        except:
            if new_task and future.done() and not future.cancelled():
                # The coroutine raised a BaseException. Consume the exception
                # to not log a warning, the caller doesn't have access to the
                # local task.
                future.exception()
            raise
        finally:
            future.remove_done_callback(_run_until_complete_cb)
        if not future.done():
            raise RuntimeError('Event loop stopped before Future completed.')
    
>       return future.result()

/home/shie44167/miniconda3/envs/composer/lib/python3.10/asyncio/base_events.py:649: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <testbook.client.TestbookNotebookClient object at 0x7f6794abd7e0>
cell = {'cell_type': 'code', 'execution_count': 9, 'metadata': {'execution': {'iopub.status.busy': '2022-11-30T11:51:45.42172...\'cpu\',\n    train_subset_num_batches=150,\n    precision=\'fp32\',\n    seed=17\n)\n# Start training\ntrainer.fit()'}
cell_index = 18, execution_count = None, store_history = True

    async def async_execute_cell(
        self,
        cell: NotebookNode,
        cell_index: int,
        execution_count: t.Optional[int] = None,
        store_history: bool = True,
    ) -> NotebookNode:
        """
        Executes a single code cell.
    
        To execute all cells see :meth:`execute`.
    
        Parameters
        ----------
        cell : nbformat.NotebookNode
            The cell which is currently being processed.
        cell_index : int
            The position of the cell within the notebook object.
        execution_count : int
            The execution count to be assigned to the cell (default: Use kernel response)
        store_history : bool
            Determines if history should be stored in the kernel (default: False).
            Specific to ipython kernels, which can store command histories.
    
        Returns
        -------
        output : dict
            The execution output payload (or None for no output).
    
        Raises
        ------
        CellExecutionError
            If execution failed and should raise an exception, this will be raised
            with defaults about the failure.
    
        Returns
        -------
        cell : NotebookNode
            The cell which was just processed.
        """
        assert self.kc is not None
    
        await run_hook(self.on_cell_start, cell=cell, cell_index=cell_index)
    
        if cell.cell_type != 'code' or not cell.source.strip():
            self.log.debug("Skipping non-executing cell %s", cell_index)
            return cell
    
        if self.skip_cells_with_tag in cell.metadata.get("tags", []):
            self.log.debug("Skipping tagged cell %s", cell_index)
            return cell
    
        if self.record_timing:  # clear execution metadata prior to execution
            cell['metadata']['execution'] = {}
    
        self.log.debug("Executing cell:\n%s", cell.source)
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors or "raises-exception" in cell.metadata.get("tags", [])
        )
    
        await run_hook(self.on_cell_execute, cell=cell, cell_index=cell_index)
        parent_msg_id = await ensure_async(
            self.kc.execute(
                cell.source, store_history=store_history, stop_on_error=not cell_allows_errors
            )
        )
        await run_hook(self.on_cell_complete, cell=cell, cell_index=cell_index)
        # We launched a code cell to execute
        self.code_cells_executed += 1
        exec_timeout = self._get_timeout(cell)
    
        cell.outputs = []
        self.clear_before_next_output = False
    
        task_poll_kernel_alive = asyncio.ensure_future(self._async_poll_kernel_alive())
        task_poll_output_msg = asyncio.ensure_future(
            self._async_poll_output_msg(parent_msg_id, cell, cell_index)
        )
        self.task_poll_for_reply = asyncio.ensure_future(
            self._async_poll_for_reply(
                parent_msg_id, cell, exec_timeout, task_poll_output_msg, task_poll_kernel_alive
            )
        )
        try:
            exec_reply = await self.task_poll_for_reply
        except asyncio.CancelledError:
            # can only be cancelled by task_poll_kernel_alive when the kernel is dead
            task_poll_output_msg.cancel()
            raise DeadKernelError("Kernel died")
        except Exception as e:
            # Best effort to cancel request if it hasn't been resolved
            try:
                # Check if the task_poll_output is doing the raising for us
                if not isinstance(e, CellControlSignal):
                    task_poll_output_msg.cancel()
            finally:
                raise
    
        if execution_count:
            cell['execution_count'] = execution_count
        await run_hook(
            self.on_cell_executed, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
>       await self._check_raise_for_error(cell, cell_index, exec_reply)

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/nbclient/client.py:1021: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <testbook.client.TestbookNotebookClient object at 0x7f6794abd7e0>
cell = {'cell_type': 'code', 'execution_count': 9, 'metadata': {'execution': {'iopub.status.busy': '2022-11-30T11:51:45.42172...\'cpu\',\n    train_subset_num_batches=150,\n    precision=\'fp32\',\n    seed=17\n)\n# Start training\ntrainer.fit()'}
cell_index = 18
exec_reply = {'buffers': [], 'content': {'ename': 'RuntimeError', 'engine_info': {'engine_id': -1, 'engine_uuid': '05a8679c-2aff-4b...e, 'engine': '05a8679c-2aff-4b14-bf1f-b83a1d0477d6', 'started': '2022-11-30T11:51:45.421833Z', 'status': 'error'}, ...}

    async def _check_raise_for_error(
        self, cell: NotebookNode, cell_index: int, exec_reply: t.Optional[t.Dict]
    ) -> None:
    
        if exec_reply is None:
            return None
    
        exec_reply_content = exec_reply['content']
        if exec_reply_content['status'] != 'error':
            return None
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors
            or exec_reply_content.get('ename') in self.allow_error_names
            or "raises-exception" in cell.metadata.get("tags", [])
        )
        await run_hook(
            self.on_cell_error, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
        if not cell_allows_errors:
>           raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
E           nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
E           ------------------
E           import torch
E           from composer import Trainer
E           
E           # Create Trainer Object
E           trainer = Trainer(
E               model=composer_model, # This is the model from the HuggingFaceModel wrapper class.
E               train_dataloader=train_dataloader,
E               eval_dataloader=eval_dataloader,
E               max_duration="1ep",
E               optimizers=optimizer,
E               schedulers=[linear_lr_decay],
E               device='gpu' if torch.cuda.is_available() else 'cpu',
E               train_subset_num_batches=150,
E               precision='fp32',
E               seed=17
E           )
E           # Start training
E           trainer.fit()
E           ------------------
E           
E           [0;31m---------------------------------------------------------------------------[0m
E           [0;31mRuntimeError[0m                              Traceback (most recent call last)
E           [0;32m/tmp/ipykernel_315289/1117973756.py[0m in [0;36m<cell line: 18>[0;34m()[0m
E           [1;32m     16[0m )
E           [1;32m     17[0m [0;31m# Start training[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m---> 18[0;31m [0mtrainer[0m[0;34m.[0m[0mfit[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m
E           [0;32m/tmp/ipykernel_315289/4080402710.py[0m in [0;36mnew_fit[0;34m(self, *args, **kwargs)[0m
E           [1;32m     19[0m             [0;32mif[0m [0;34m'eval_subset_num_batches'[0m [0;32mnot[0m [0;32min[0m [0mkwargs[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m     20[0m                 [0mkwargs[0m[0;34m[[0m[0;34m'eval_subset_num_batches'[0m[0;34m][0m [0;34m=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m---> 21[0;31m         [0moriginal_fit[0m[0;34m([0m[0mself[0m[0;34m,[0m [0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m     22[0m [0;34m[0m[0m
E           [1;32m     23[0m     [0mTrainer[0m[0;34m.[0m[0mfit[0m [0;34m=[0m [0mnew_fit[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36mfit[0;34m(self, train_dataloader, train_dataloader_label, train_subset_num_batches, duration, reset_time, schedulers, scale_schedule_ratio, step_schedulers_every_batch, eval_dataloader, eval_subset_num_batches, eval_interval, grad_accum, precision)[0m
E           [1;32m   1632[0m             [0;31m# update scaler since precision was provided[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1633[0m             [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mscaler[0m [0;34m=[0m [0mClosureGradScaler[0m[0;34m([0m[0;34m)[0m [0;32mif[0m [0mself[0m[0;34m.[0m[0m_use_closures[0m[0;34m([0m[0;34m)[0m [0;32melse[0m [0mGradScaler[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m-> 1634[0;31m         [0mself[0m[0;34m.[0m[0m_train_loop[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1635[0m [0;34m[0m[0m
E           [1;32m   1636[0m     [0;32mdef[0m [0mclose[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_loop[0;34m(self)[0m
E           [1;32m   1790[0m                     })
E           [1;32m   1791[0m [0;34m[0m[0m
E           [0;32m-> 1792[0;31m                     [0mtotal_loss_dict[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_train_batch[0m[0;34m([0m[0muse_grad_scaling[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1793[0m [0;34m[0m[0m
E           [1;32m   1794[0m                     [0;32mif[0m [0muse_grad_scaling[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_batch[0;34m(self, use_grad_scaling)[0m
E           [1;32m   1958[0m                                                    _train_microbatches(microbatches, loss_dict, **kwargs))
E           [1;32m   1959[0m                         [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m-> 1960[0;31m                             optimizer.step(closure=lambda **kwargs: self._train_microbatches(
E           [0m[1;32m   1961[0m                                 microbatches, total_loss_dict, **kwargs).item())
E           [1;32m   1962[0m                 [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/optim/lr_scheduler.py[0m in [0;36mwrapper[0;34m(*args, **kwargs)[0m
E           [1;32m     63[0m                 [0minstance[0m[0;34m.[0m[0m_step_count[0m [0;34m+=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m     64[0m                 [0mwrapped[0m [0;34m=[0m [0mfunc[0m[0;34m.[0m[0m__get__[0m[0;34m([0m[0minstance[0m[0;34m,[0m [0mcls[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m---> 65[0;31m                 [0;32mreturn[0m [0mwrapped[0m[0;34m([0m[0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m     66[0m [0;34m[0m[0m
E           [1;32m     67[0m             [0;31m# Note that the returned function here is no longer a bound method,[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/optim/optimizer.py[0m in [0;36mwrapper[0;34m(*args, **kwargs)[0m
E           [1;32m    111[0m                 [0mprofile_name[0m [0;34m=[0m [0;34m"Optimizer.step#{}.step"[0m[0;34m.[0m[0mformat[0m[0;34m([0m[0mobj[0m[0;34m.[0m[0m__class__[0m[0;34m.[0m[0m__name__[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m    112[0m                 [0;32mwith[0m [0mtorch[0m[0;34m.[0m[0mautograd[0m[0;34m.[0m[0mprofiler[0m[0;34m.[0m[0mrecord_function[0m[0;34m([0m[0mprofile_name[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m--> 113[0;31m                     [0;32mreturn[0m [0mfunc[0m[0;34m([0m[0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m    114[0m             [0;32mreturn[0m [0mwrapper[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m    115[0m [0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/autograd/grad_mode.py[0m in [0;36mdecorate_context[0;34m(*args, **kwargs)[0m
E           [1;32m     25[0m         [0;32mdef[0m [0mdecorate_context[0m[0;34m([0m[0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m     26[0m             [0;32mwith[0m [0mself[0m[0;34m.[0m[0mclone[0m[0;34m([0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m---> 27[0;31m                 [0;32mreturn[0m [0mfunc[0m[0;34m([0m[0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m     28[0m         [0;32mreturn[0m [0mcast[0m[0;34m([0m[0mF[0m[0;34m,[0m [0mdecorate_context[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m     29[0m [0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/optim/adamw.py[0m in [0;36mstep[0;34m(self, closure)[0m
E           [1;32m    117[0m         [0;32mif[0m [0mclosure[0m [0;32mis[0m [0;32mnot[0m [0;32mNone[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m    118[0m             [0;32mwith[0m [0mtorch[0m[0;34m.[0m[0menable_grad[0m[0;34m([0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m--> 119[0;31m                 [0mloss[0m [0;34m=[0m [0mclosure[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m    120[0m [0;34m[0m[0m
E           [1;32m    121[0m         [0;32mfor[0m [0mgroup[0m [0;32min[0m [0mself[0m[0;34m.[0m[0mparam_groups[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m<lambda>[0;34m(**kwargs)[0m
E           [1;32m   1958[0m                                                    _train_microbatches(microbatches, loss_dict, **kwargs))
E           [1;32m   1959[0m                         [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m-> 1960[0;31m                             optimizer.step(closure=lambda **kwargs: self._train_microbatches(
E           [0m[1;32m   1961[0m                                 microbatches, total_loss_dict, **kwargs).item())
E           [1;32m   1962[0m                 [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_microbatches[0;34m(self, microbatches, total_loss_dict, ddp_sync)[0m
E           [1;32m   2032[0m             [0;32mfor[0m [0mmicrobatch_idx[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mbatch[0m [0;32min[0m [0menumerate[0m[0;34m([0m[0mmicrobatches[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   2033[0m                 [0mis_final_microbatch[0m [0;34m=[0m [0mmicrobatch_idx[0m [0;34m+[0m [0;36m1[0m [0;34m==[0m [0mlen[0m[0;34m([0m[0mmicrobatches[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m-> 2034[0;31m                 [0mmicrobatch_loss_dict[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_train_microbatch[0m[0;34m([0m[0muse_grad_scaling[0m[0;34m,[0m [0mcurrent_batch_size[0m[0;34m,[0m [0mis_final_microbatch[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   2035[0m [0;34m[0m[0m
E           [1;32m   2036[0m                 [0;31m# Aggregate each loss in microbatch_loss_dict into total_loss_dict[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_microbatch[0;34m(self, use_grad_scaling, current_batch_size, is_final_microbatch)[0m
E           [1;32m   2078[0m [0;34m[0m[0m
E           [1;32m   2079[0m             [0;32mwith[0m [0mget_precision_context[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mprecision[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m-> 2080[0;31m                 [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0moutputs[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mmodel[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mbatch[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   2081[0m [0;34m[0m[0m
E           [1;32m   2082[0m             [0mself[0m[0;34m.[0m[0mengine[0m[0;34m.[0m[0mrun_event[0m[0;34m([0m[0mEvent[0m[0;34m.[0m[0mAFTER_FORWARD[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E           [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E           [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E           [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/models/huggingface.py[0m in [0;36mforward[0;34m(self, batch)[0m
E           [1;32m     75[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mbatch[0m[0;34m,[0m [0mdict[0m[0;34m)[0m [0;32mor[0m [0misinstance[0m[0;34m([0m[0mbatch[0m[0;34m,[0m [0mUserDict[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m     76[0m             [0;31m# Further input validation is left to the huggingface forward call[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m---> 77[0;31m             [0moutput[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mmodel[0m[0;34m([0m[0;34m**[0m[0mbatch[0m[0;34m)[0m  [0;31m# type: ignore (thirdparty)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m     78[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m     79[0m             raise ValueError(
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E           [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E           [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E           [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py[0m in [0;36mforward[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)[0m
E           [1;32m   1550[0m         [0mreturn_dict[0m [0;34m=[0m [0mreturn_dict[0m [0;32mif[0m [0mreturn_dict[0m [0;32mis[0m [0;32mnot[0m [0;32mNone[0m [0;32melse[0m [0mself[0m[0;34m.[0m[0mconfig[0m[0;34m.[0m[0muse_return_dict[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1551[0m [0;34m[0m[0m
E           [0;32m-> 1552[0;31m         outputs = self.bert(
E           [0m[1;32m   1553[0m             [0minput_ids[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1554[0m             [0mattention_mask[0m[0;34m=[0m[0mattention_mask[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E           [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E           [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E           [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py[0m in [0;36mforward[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)[0m
E           [1;32m   1012[0m             [0mpast_key_values_length[0m[0;34m=[0m[0mpast_key_values_length[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1013[0m         )
E           [0;32m-> 1014[0;31m         encoder_outputs = self.encoder(
E           [0m[1;32m   1015[0m             [0membedding_output[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1016[0m             [0mattention_mask[0m[0;34m=[0m[0mextended_attention_mask[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E           [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E           [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E           [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py[0m in [0;36mforward[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)[0m
E           [1;32m    601[0m                 )
E           [1;32m    602[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m--> 603[0;31m                 layer_outputs = layer_module(
E           [0m[1;32m    604[0m                     [0mhidden_states[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m    605[0m                     [0mattention_mask[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E           [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E           [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E           [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py[0m in [0;36mforward[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)[0m
E           [1;32m    487[0m         [0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m    488[0m         [0mself_attn_past_key_value[0m [0;34m=[0m [0mpast_key_value[0m[0;34m[[0m[0;34m:[0m[0;36m2[0m[0;34m][0m [0;32mif[0m [0mpast_key_value[0m [0;32mis[0m [0;32mnot[0m [0;32mNone[0m [0;32melse[0m [0;32mNone[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m--> 489[0;31m         self_attention_outputs = self.attention(
E           [0m[1;32m    490[0m             [0mhidden_states[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m    491[0m             [0mattention_mask[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E           [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E           [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E           [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py[0m in [0;36mforward[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)[0m
E           [1;32m    417[0m         [0moutput_attentions[0m[0;34m:[0m [0mOptional[0m[0;34m[[0m[0mbool[0m[0;34m][0m [0;34m=[0m [0;32mFalse[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m    418[0m     ) -> Tuple[torch.Tensor]:
E           [0;32m--> 419[0;31m         self_outputs = self.self(
E           [0m[1;32m    420[0m             [0mhidden_states[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m    421[0m             [0mattention_mask[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E           [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E           [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E           [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E           [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py[0m in [0;36mforward[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)[0m
E           [1;32m    321[0m [0;34m[0m[0m
E           [1;32m    322[0m         [0;31m# Take the dot product between "query" and "key" to get the raw attention scores.[0m[0;34m[0m[0;34m[0m[0m
E           [0;32m--> 323[0;31m         [0mattention_scores[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mmatmul[0m[0;34m([0m[0mquery_layer[0m[0;34m,[0m [0mkey_layer[0m[0;34m.[0m[0mtranspose[0m[0;34m([0m[0;34m-[0m[0;36m1[0m[0;34m,[0m [0;34m-[0m[0;36m2[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E           [0m[1;32m    324[0m [0;34m[0m[0m
E           [1;32m    325[0m         [0;32mif[0m [0mself[0m[0;34m.[0m[0mposition_embedding_type[0m [0;34m==[0m [0;34m"relative_key"[0m [0;32mor[0m [0mself[0m[0;34m.[0m[0mposition_embedding_type[0m [0;34m==[0m [0;34m"relative_key_query"[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E           
E           [0;31mRuntimeError[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 7.76 GiB total capacity; 1.68 GiB already allocated; 48.25 MiB free; 1.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
E           RuntimeError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 7.76 GiB total capacity; 1.68 GiB already allocated; 48.25 MiB free; 1.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/nbclient/client.py:915: CellExecutionError

During handling of the above exception, another exception occurred:

notebook = '/home/shie44167/workspace/1_read_codes/composer/composer/../examples/huggingface_models.ipynb'
device = 'gpu', s3_bucket = 'my-bucket'

    @pytest.mark.parametrize('notebook', [_to_pytest_param(notebook) for notebook in NOTEBOOKS])
    @device('cpu', 'gpu')
    @pytest.mark.daily
    def test_notebook(notebook: str, device: str, s3_bucket: str):
        trainer_monkeypatch_code = inspect.getsource(patch_notebooks)
        notebook_name = os.path.split(notebook)[-1][:-len('.ipynb')]
        if notebook_name == 'medical_image_segmentation':
            pytest.xfail('Dataset is only available via kaggle; need to authenticate on ci/cd')
        if notebook_name == 'auto_grad_accum' and device == 'cpu':
            pytest.skip('auto_grad_accum notebook only runs with a gpu')
        if notebook_name == 'TPU_Training_in_composer':
            pytest.skip('The CI does not support tpus')
        if notebook_name == 'ffcv_dataloaders' and device == 'cpu':
            pytest.skip('The FFCV notebook requires CUDA')
        if notebook_name == 'streaming_dataloader_facesynthetics':
            pytest.skip('Jenkins is killing this notebook for some reason, it should work locally')
        if notebook_name == 'training_without_local_storage':
            pytest.skip('Jenkins is not getting the S3 credentials set up properly, it should work locally')
        with testbook.testbook(notebook) as tb:
            tb.inject(trainer_monkeypatch_code)
            tb.inject('patch_notebooks()')
            for i, cell in enumerate(tb.cells):
                if cell['cell_type'] != 'code':
                    continue
                cell['source'] = modify_cell_source(tb,
                                                    notebook_name=notebook_name,
                                                    cell_source=cell['source'],
                                                    s3_bucket=s3_bucket)
>               tb.execute_cell(i)

/home/shie44167/workspace/1_read_codes/composer/tests/test_notebooks.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <testbook.client.TestbookNotebookClient object at 0x7f6794abd7e0>
cell = [18], kwargs = {}, cell_indexes = [18], executed_cells = [], idx = 18

    def execute_cell(self, cell, **kwargs) -> Union[Dict, List[Dict]]:
        """
        Executes a cell or list of cells
        """
        if isinstance(cell, slice):
            start, stop = self._cell_index(cell.start), self._cell_index(cell.stop)
            if cell.step is not None:
                raise TestbookError('testbook does not support step argument')
    
            cell = range(start, stop + 1)
        elif isinstance(cell, str) or isinstance(cell, int):
            cell = [cell]
    
        cell_indexes = cell
    
        if all(isinstance(x, str) for x in cell):
            cell_indexes = [self._cell_index(tag) for tag in cell]
    
        executed_cells = []
        for idx in cell_indexes:
            try:
                cell = super().execute_cell(self.nb['cells'][idx], idx, **kwargs)
            except CellExecutionError as ce:
>               raise TestbookRuntimeError(ce.evalue, ce, self._get_error_class(ce.ename))
E               testbook.exceptions.TestbookRuntimeError: An error occurred while executing the following cell:
E               ------------------
E               import torch
E               from composer import Trainer
E               
E               # Create Trainer Object
E               trainer = Trainer(
E                   model=composer_model, # This is the model from the HuggingFaceModel wrapper class.
E                   train_dataloader=train_dataloader,
E                   eval_dataloader=eval_dataloader,
E                   max_duration="1ep",
E                   optimizers=optimizer,
E                   schedulers=[linear_lr_decay],
E                   device='gpu' if torch.cuda.is_available() else 'cpu',
E                   train_subset_num_batches=150,
E                   precision='fp32',
E                   seed=17
E               )
E               # Start training
E               trainer.fit()
E               ------------------
E               
E               [0;31m---------------------------------------------------------------------------[0m
E               [0;31mRuntimeError[0m                              Traceback (most recent call last)
E               [0;32m/tmp/ipykernel_315289/1117973756.py[0m in [0;36m<cell line: 18>[0;34m()[0m
E               [1;32m     16[0m )
E               [1;32m     17[0m [0;31m# Start training[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m---> 18[0;31m [0mtrainer[0m[0;34m.[0m[0mfit[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m
E               [0;32m/tmp/ipykernel_315289/4080402710.py[0m in [0;36mnew_fit[0;34m(self, *args, **kwargs)[0m
E               [1;32m     19[0m             [0;32mif[0m [0;34m'eval_subset_num_batches'[0m [0;32mnot[0m [0;32min[0m [0mkwargs[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m     20[0m                 [0mkwargs[0m[0;34m[[0m[0;34m'eval_subset_num_batches'[0m[0;34m][0m [0;34m=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m---> 21[0;31m         [0moriginal_fit[0m[0;34m([0m[0mself[0m[0;34m,[0m [0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m     22[0m [0;34m[0m[0m
E               [1;32m     23[0m     [0mTrainer[0m[0;34m.[0m[0mfit[0m [0;34m=[0m [0mnew_fit[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36mfit[0;34m(self, train_dataloader, train_dataloader_label, train_subset_num_batches, duration, reset_time, schedulers, scale_schedule_ratio, step_schedulers_every_batch, eval_dataloader, eval_subset_num_batches, eval_interval, grad_accum, precision)[0m
E               [1;32m   1632[0m             [0;31m# update scaler since precision was provided[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1633[0m             [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mscaler[0m [0;34m=[0m [0mClosureGradScaler[0m[0;34m([0m[0;34m)[0m [0;32mif[0m [0mself[0m[0;34m.[0m[0m_use_closures[0m[0;34m([0m[0;34m)[0m [0;32melse[0m [0mGradScaler[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m-> 1634[0;31m         [0mself[0m[0;34m.[0m[0m_train_loop[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1635[0m [0;34m[0m[0m
E               [1;32m   1636[0m     [0;32mdef[0m [0mclose[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_loop[0;34m(self)[0m
E               [1;32m   1790[0m                     })
E               [1;32m   1791[0m [0;34m[0m[0m
E               [0;32m-> 1792[0;31m                     [0mtotal_loss_dict[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_train_batch[0m[0;34m([0m[0muse_grad_scaling[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1793[0m [0;34m[0m[0m
E               [1;32m   1794[0m                     [0;32mif[0m [0muse_grad_scaling[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_batch[0;34m(self, use_grad_scaling)[0m
E               [1;32m   1958[0m                                                    _train_microbatches(microbatches, loss_dict, **kwargs))
E               [1;32m   1959[0m                         [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m-> 1960[0;31m                             optimizer.step(closure=lambda **kwargs: self._train_microbatches(
E               [0m[1;32m   1961[0m                                 microbatches, total_loss_dict, **kwargs).item())
E               [1;32m   1962[0m                 [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/optim/lr_scheduler.py[0m in [0;36mwrapper[0;34m(*args, **kwargs)[0m
E               [1;32m     63[0m                 [0minstance[0m[0;34m.[0m[0m_step_count[0m [0;34m+=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m     64[0m                 [0mwrapped[0m [0;34m=[0m [0mfunc[0m[0;34m.[0m[0m__get__[0m[0;34m([0m[0minstance[0m[0;34m,[0m [0mcls[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m---> 65[0;31m                 [0;32mreturn[0m [0mwrapped[0m[0;34m([0m[0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m     66[0m [0;34m[0m[0m
E               [1;32m     67[0m             [0;31m# Note that the returned function here is no longer a bound method,[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/optim/optimizer.py[0m in [0;36mwrapper[0;34m(*args, **kwargs)[0m
E               [1;32m    111[0m                 [0mprofile_name[0m [0;34m=[0m [0;34m"Optimizer.step#{}.step"[0m[0;34m.[0m[0mformat[0m[0;34m([0m[0mobj[0m[0;34m.[0m[0m__class__[0m[0;34m.[0m[0m__name__[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m    112[0m                 [0;32mwith[0m [0mtorch[0m[0;34m.[0m[0mautograd[0m[0;34m.[0m[0mprofiler[0m[0;34m.[0m[0mrecord_function[0m[0;34m([0m[0mprofile_name[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m--> 113[0;31m                     [0;32mreturn[0m [0mfunc[0m[0;34m([0m[0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m    114[0m             [0;32mreturn[0m [0mwrapper[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m    115[0m [0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/autograd/grad_mode.py[0m in [0;36mdecorate_context[0;34m(*args, **kwargs)[0m
E               [1;32m     25[0m         [0;32mdef[0m [0mdecorate_context[0m[0;34m([0m[0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m     26[0m             [0;32mwith[0m [0mself[0m[0;34m.[0m[0mclone[0m[0;34m([0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m---> 27[0;31m                 [0;32mreturn[0m [0mfunc[0m[0;34m([0m[0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m     28[0m         [0;32mreturn[0m [0mcast[0m[0;34m([0m[0mF[0m[0;34m,[0m [0mdecorate_context[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m     29[0m [0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/optim/adamw.py[0m in [0;36mstep[0;34m(self, closure)[0m
E               [1;32m    117[0m         [0;32mif[0m [0mclosure[0m [0;32mis[0m [0;32mnot[0m [0;32mNone[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m    118[0m             [0;32mwith[0m [0mtorch[0m[0;34m.[0m[0menable_grad[0m[0;34m([0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m--> 119[0;31m                 [0mloss[0m [0;34m=[0m [0mclosure[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m    120[0m [0;34m[0m[0m
E               [1;32m    121[0m         [0;32mfor[0m [0mgroup[0m [0;32min[0m [0mself[0m[0;34m.[0m[0mparam_groups[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m<lambda>[0;34m(**kwargs)[0m
E               [1;32m   1958[0m                                                    _train_microbatches(microbatches, loss_dict, **kwargs))
E               [1;32m   1959[0m                         [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m-> 1960[0;31m                             optimizer.step(closure=lambda **kwargs: self._train_microbatches(
E               [0m[1;32m   1961[0m                                 microbatches, total_loss_dict, **kwargs).item())
E               [1;32m   1962[0m                 [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_microbatches[0;34m(self, microbatches, total_loss_dict, ddp_sync)[0m
E               [1;32m   2032[0m             [0;32mfor[0m [0mmicrobatch_idx[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mbatch[0m [0;32min[0m [0menumerate[0m[0;34m([0m[0mmicrobatches[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   2033[0m                 [0mis_final_microbatch[0m [0;34m=[0m [0mmicrobatch_idx[0m [0;34m+[0m [0;36m1[0m [0;34m==[0m [0mlen[0m[0;34m([0m[0mmicrobatches[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m-> 2034[0;31m                 [0mmicrobatch_loss_dict[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_train_microbatch[0m[0;34m([0m[0muse_grad_scaling[0m[0;34m,[0m [0mcurrent_batch_size[0m[0;34m,[0m [0mis_final_microbatch[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   2035[0m [0;34m[0m[0m
E               [1;32m   2036[0m                 [0;31m# Aggregate each loss in microbatch_loss_dict into total_loss_dict[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/trainer/trainer.py[0m in [0;36m_train_microbatch[0;34m(self, use_grad_scaling, current_batch_size, is_final_microbatch)[0m
E               [1;32m   2078[0m [0;34m[0m[0m
E               [1;32m   2079[0m             [0;32mwith[0m [0mget_precision_context[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mprecision[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m-> 2080[0;31m                 [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0moutputs[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mmodel[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mstate[0m[0;34m.[0m[0mbatch[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   2081[0m [0;34m[0m[0m
E               [1;32m   2082[0m             [0mself[0m[0;34m.[0m[0mengine[0m[0;34m.[0m[0mrun_event[0m[0;34m([0m[0mEvent[0m[0;34m.[0m[0mAFTER_FORWARD[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E               [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E               [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E               [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/composer/models/huggingface.py[0m in [0;36mforward[0;34m(self, batch)[0m
E               [1;32m     75[0m         [0;32mif[0m [0misinstance[0m[0;34m([0m[0mbatch[0m[0;34m,[0m [0mdict[0m[0;34m)[0m [0;32mor[0m [0misinstance[0m[0;34m([0m[0mbatch[0m[0;34m,[0m [0mUserDict[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m     76[0m             [0;31m# Further input validation is left to the huggingface forward call[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m---> 77[0;31m             [0moutput[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mmodel[0m[0;34m([0m[0;34m**[0m[0mbatch[0m[0;34m)[0m  [0;31m# type: ignore (thirdparty)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m     78[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m     79[0m             raise ValueError(
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E               [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E               [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E               [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py[0m in [0;36mforward[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)[0m
E               [1;32m   1550[0m         [0mreturn_dict[0m [0;34m=[0m [0mreturn_dict[0m [0;32mif[0m [0mreturn_dict[0m [0;32mis[0m [0;32mnot[0m [0;32mNone[0m [0;32melse[0m [0mself[0m[0;34m.[0m[0mconfig[0m[0;34m.[0m[0muse_return_dict[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1551[0m [0;34m[0m[0m
E               [0;32m-> 1552[0;31m         outputs = self.bert(
E               [0m[1;32m   1553[0m             [0minput_ids[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1554[0m             [0mattention_mask[0m[0;34m=[0m[0mattention_mask[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E               [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E               [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E               [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py[0m in [0;36mforward[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)[0m
E               [1;32m   1012[0m             [0mpast_key_values_length[0m[0;34m=[0m[0mpast_key_values_length[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1013[0m         )
E               [0;32m-> 1014[0;31m         encoder_outputs = self.encoder(
E               [0m[1;32m   1015[0m             [0membedding_output[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1016[0m             [0mattention_mask[0m[0;34m=[0m[0mextended_attention_mask[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E               [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E               [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E               [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py[0m in [0;36mforward[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)[0m
E               [1;32m    601[0m                 )
E               [1;32m    602[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m--> 603[0;31m                 layer_outputs = layer_module(
E               [0m[1;32m    604[0m                     [0mhidden_states[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m    605[0m                     [0mattention_mask[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E               [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E               [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E               [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py[0m in [0;36mforward[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)[0m
E               [1;32m    487[0m         [0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m    488[0m         [0mself_attn_past_key_value[0m [0;34m=[0m [0mpast_key_value[0m[0;34m[[0m[0;34m:[0m[0;36m2[0m[0;34m][0m [0;32mif[0m [0mpast_key_value[0m [0;32mis[0m [0;32mnot[0m [0;32mNone[0m [0;32melse[0m [0;32mNone[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m--> 489[0;31m         self_attention_outputs = self.attention(
E               [0m[1;32m    490[0m             [0mhidden_states[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m    491[0m             [0mattention_mask[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E               [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E               [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E               [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py[0m in [0;36mforward[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)[0m
E               [1;32m    417[0m         [0moutput_attentions[0m[0;34m:[0m [0mOptional[0m[0;34m[[0m[0mbool[0m[0;34m][0m [0;34m=[0m [0;32mFalse[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m    418[0m     ) -> Tuple[torch.Tensor]:
E               [0;32m--> 419[0;31m         self_outputs = self.self(
E               [0m[1;32m    420[0m             [0mhidden_states[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m    421[0m             [0mattention_mask[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py[0m in [0;36m_call_impl[0;34m(self, *input, **kwargs)[0m
E               [1;32m   1128[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
E               [1;32m   1129[0m                 or _global_forward_hooks or _global_forward_pre_hooks):
E               [0;32m-> 1130[0;31m             [0;32mreturn[0m [0mforward_call[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m   1131[0m         [0;31m# Do not call functions when jit is used[0m[0;34m[0m[0;34m[0m[0m
E               [1;32m   1132[0m         [0mfull_backward_hooks[0m[0;34m,[0m [0mnon_full_backward_hooks[0m [0;34m=[0m [0;34m[[0m[0;34m][0m[0;34m,[0m [0;34m[[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;32m~/miniconda3/envs/composer/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py[0m in [0;36mforward[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)[0m
E               [1;32m    321[0m [0;34m[0m[0m
E               [1;32m    322[0m         [0;31m# Take the dot product between "query" and "key" to get the raw attention scores.[0m[0;34m[0m[0;34m[0m[0m
E               [0;32m--> 323[0;31m         [0mattention_scores[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mmatmul[0m[0;34m([0m[0mquery_layer[0m[0;34m,[0m [0mkey_layer[0m[0;34m.[0m[0mtranspose[0m[0;34m([0m[0;34m-[0m[0;36m1[0m[0;34m,[0m [0;34m-[0m[0;36m2[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
E               [0m[1;32m    324[0m [0;34m[0m[0m
E               [1;32m    325[0m         [0;32mif[0m [0mself[0m[0;34m.[0m[0mposition_embedding_type[0m [0;34m==[0m [0;34m"relative_key"[0m [0;32mor[0m [0mself[0m[0;34m.[0m[0mposition_embedding_type[0m [0;34m==[0m [0;34m"relative_key_query"[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
E               
E               [0;31mRuntimeError[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 7.76 GiB total capacity; 1.68 GiB already allocated; 48.25 MiB free; 1.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
E               RuntimeError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 7.76 GiB total capacity; 1.68 GiB already allocated; 48.25 MiB free; 1.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/testbook/client.py:135: TestbookRuntimeError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
__________________ test_train_precision_memory[Precision.AMP] __________________

precision = <Precision.AMP: 'amp'>

    @pytest.mark.gpu
    @pytest.mark.parametrize('precision', [Precision.AMP, Precision.BF16])
    def test_train_precision_memory(precision: Precision):
>       memory_fp32 = fit_and_measure_memory(Precision.FP32)

/home/shie44167/workspace/1_read_codes/composer/tests/test_precision.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/tests/test_precision.py:43: in fit_and_measure_memory
    trainer.fit()
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1634: in fit
    self._train_loop()
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1792: in _train_loop
    total_loss_dict = self._train_batch(use_grad_scaling)
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1960: in _train_batch
    optimizer.step(closure=lambda **kwargs: self._train_microbatches(
/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/torch/optim/optimizer.py:113: in wrapper
    return func(*args, **kwargs)
/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27: in decorate_context
    return func(*args, **kwargs)
/home/shie44167/workspace/1_read_codes/composer/composer/optim/decoupled_weight_decay.py:120: in step
    loss = closure()
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1960: in <lambda>
    optimizer.step(closure=lambda **kwargs: self._train_microbatches(
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:2034: in _train_microbatches
    microbatch_loss_dict = self._train_microbatch(use_grad_scaling, current_batch_size, is_final_microbatch)
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:2080: in _train_microbatch
    self.state.outputs = self.state.model(self.state.batch)
/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py:1130: in _call_impl
    return forward_call(*input, **kwargs)
/home/shie44167/workspace/1_read_codes/composer/composer/models/tasks/classification.py:106: in forward
    outputs = self.module(inputs)
/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py:1130: in _call_impl
    return forward_call(*input, **kwargs)
/home/shie44167/workspace/1_read_codes/composer/composer/models/resnet_cifar/resnets.py:87: in forward
    out = self.blocks(out)
/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py:1130: in _call_impl
    return forward_call(*input, **kwargs)
/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/container.py:139: in forward
    input = module(input)
/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py:1130: in _call_impl
    return forward_call(*input, **kwargs)
/home/shie44167/workspace/1_read_codes/composer/composer/models/resnet_cifar/resnets.py:50: in forward
    out = self.relu(self.bn1(self.conv1(x)))
/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/module.py:1130: in _call_impl
    return forward_call(*input, **kwargs)
/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:168: in forward
    return F.batch_norm(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[-3.0997e-01, -1.1503e+00, -3.5244e-01,  ..., -2.9498e+00,
           -2.5441e-01, -8.0366e-01],
          [...00,  ..., -3.0588e-01,
           -2.0010e-02, -2.0230e-02]]]], device='cuda:0',
       grad_fn=<ConvolutionBackward0>)
running_mean = tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
running_var = tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0')
weight = Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True)
bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
training = True, momentum = 0.1, eps = 1e-05

    def batch_norm(
        input: Tensor,
        running_mean: Optional[Tensor],
        running_var: Optional[Tensor],
        weight: Optional[Tensor] = None,
        bias: Optional[Tensor] = None,
        training: bool = False,
        momentum: float = 0.1,
        eps: float = 1e-5,
    ) -> Tensor:
        r"""Applies Batch Normalization for each channel across a batch of data.
    
        See :class:`~torch.nn.BatchNorm1d`, :class:`~torch.nn.BatchNorm2d`,
        :class:`~torch.nn.BatchNorm3d` for details.
        """
        if has_torch_function_variadic(input, running_mean, running_var, weight, bias):
            return handle_torch_function(
                batch_norm,
                (input, running_mean, running_var, weight, bias),
                input,
                running_mean,
                running_var,
                weight=weight,
                bias=bias,
                training=training,
                momentum=momentum,
                eps=eps,
            )
        if training:
            _verify_batch_size(input.size())
    
>       return torch.batch_norm(
            input, weight, bias, running_mean, running_var, training, momentum, eps, torch.backends.cudnn.enabled
        )
E       RuntimeError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 7.76 GiB total capacity; 4.41 GiB already allocated; 51.25 MiB free; 4.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/torch/nn/functional.py:2438: RuntimeError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
----------------------------- Captured stderr call -----------------------------
******************************
Config:
num_gpus_per_node: 1
num_nodes: 1
rank_zero_seed: 0

******************************
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809140-sandy-chupacabra
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
INFO     composer.trainer.trainer:trainer.py:1274 Setting seed to 0
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:1734 Using precision Precision.FP32
DEBUG    composer.trainer.trainer:trainer.py:1693 Spinning the dataloaders
_________________ test_train_precision_memory[Precision.BF16] __________________

precision = <Precision.BF16: 'bf16'>

    @pytest.mark.gpu
    @pytest.mark.parametrize('precision', [Precision.AMP, Precision.BF16])
    def test_train_precision_memory(precision: Precision):
>       memory_fp32 = fit_and_measure_memory(Precision.FP32)

/home/shie44167/workspace/1_read_codes/composer/tests/test_precision.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/tests/test_precision.py:43: in fit_and_measure_memory
    trainer.fit()
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1634: in fit
    self._train_loop()
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1776: in _train_loop
    self.state.batch = self._device.batch_to_device(self.state.batch)
/home/shie44167/workspace/1_read_codes/composer/composer/devices/device.py:76: in batch_to_device
    return _map_batch(batch, _to_device)
/home/shie44167/workspace/1_read_codes/composer/composer/devices/device.py:110: in _map_batch
    return type(batch)(_map_batch(x, map_fn) for x in batch)  # type: ignore
/home/shie44167/workspace/1_read_codes/composer/composer/devices/device.py:110: in <genexpr>
    return type(batch)(_map_batch(x, map_fn) for x in batch)  # type: ignore
/home/shie44167/workspace/1_read_codes/composer/composer/devices/device.py:113: in _map_batch
    return map_fn(batch)
/home/shie44167/workspace/1_read_codes/composer/composer/devices/device.py:73: in _to_device
    return self.tensor_to_device(x)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.devices.device_gpu.DeviceGPU object at 0x7f679686c520>
tensor = tensor([[[[ 1.3901e+00, -1.5976e+00,  8.0591e-01,  ...,  1.7496e+00,
            9.8463e-01, -1.9750e-02],
          [...503e+00],
          [-1.1551e+00,  1.9572e+00,  1.9014e-01,  ..., -1.1449e+00,
            7.8410e-01,  1.2129e-01]]]])

    def tensor_to_device(self, tensor: torch.Tensor) -> torch.Tensor:
>       return tensor.to(self._device, non_blocking=True)
E       RuntimeError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 7.76 GiB total capacity; 4.42 GiB already allocated; 51.25 MiB free; 4.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

/home/shie44167/workspace/1_read_codes/composer/composer/devices/device_gpu.py:58: RuntimeError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
----------------------------- Captured stderr call -----------------------------
******************************
Config:
num_gpus_per_node: 1
num_nodes: 1
rank_zero_seed: 0

******************************
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809140-hairy-galago
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
INFO     composer.trainer.trainer:trainer.py:1274 Setting seed to 0
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:1734 Using precision Precision.FP32
DEBUG    composer.trainer.trainer:trainer.py:1693 Spinning the dataloaders
__________________ test_eval_precision_memory[Precision.AMP] ___________________

precision = <Precision.AMP: 'amp'>

    @pytest.mark.gpu
    @pytest.mark.parametrize('precision', [Precision.AMP, Precision.BF16])
    def test_eval_precision_memory(precision: Precision):
>       memory_fp32 = eval_and_measure_memory(Precision.FP32)

/home/shie44167/workspace/1_read_codes/composer/tests/test_precision.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/tests/test_precision.py:53: in eval_and_measure_memory
    trainer.eval()
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:2422: in eval
    self._eval_loop(
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:2492: in _eval_loop
    self.state.batch = self._device.batch_to_device(self.state.batch)
/home/shie44167/workspace/1_read_codes/composer/composer/devices/device.py:76: in batch_to_device
    return _map_batch(batch, _to_device)
/home/shie44167/workspace/1_read_codes/composer/composer/devices/device.py:110: in _map_batch
    return type(batch)(_map_batch(x, map_fn) for x in batch)  # type: ignore
/home/shie44167/workspace/1_read_codes/composer/composer/devices/device.py:110: in <genexpr>
    return type(batch)(_map_batch(x, map_fn) for x in batch)  # type: ignore
/home/shie44167/workspace/1_read_codes/composer/composer/devices/device.py:113: in _map_batch
    return map_fn(batch)
/home/shie44167/workspace/1_read_codes/composer/composer/devices/device.py:73: in _to_device
    return self.tensor_to_device(x)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.devices.device_gpu.DeviceGPU object at 0x7f679687d2a0>
tensor = tensor([[[[-0.2506, -0.4339,  0.8487,  ..., -0.8437,  0.9318,  1.2590],
          [-0.4927,  0.2484,  0.4397,  ..., -0..., -1.0550,  ..., -0.8729,  0.9798, -0.4471],
          [ 1.1968, -0.7578, -0.3038,  ..., -0.3201,  0.2257,  0.7018]]]])

    def tensor_to_device(self, tensor: torch.Tensor) -> torch.Tensor:
>       return tensor.to(self._device, non_blocking=True)
E       RuntimeError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 7.76 GiB total capacity; 4.42 GiB already allocated; 51.25 MiB free; 4.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

/home/shie44167/workspace/1_read_codes/composer/composer/devices/device_gpu.py:58: RuntimeError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
----------------------------- Captured stderr call -----------------------------
******************************
Config:
num_gpus_per_node: 1
num_nodes: 1
rank_zero_seed: 0

******************************
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809140-puzzling-otter
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
INFO     composer.trainer.trainer:trainer.py:1274 Setting seed to 0
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
--------------------------- Captured stderr teardown ---------------------------




__________________ test_eval_precision_memory[Precision.BF16] __________________

precision = <Precision.BF16: 'bf16'>

    @pytest.mark.gpu
    @pytest.mark.parametrize('precision', [Precision.AMP, Precision.BF16])
    def test_eval_precision_memory(precision: Precision):
        memory_fp32 = eval_and_measure_memory(Precision.FP32)
>       memory_half = eval_and_measure_memory(precision)

/home/shie44167/workspace/1_read_codes/composer/tests/test_precision.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/tests/test_precision.py:53: in eval_and_measure_memory
    trainer.eval()
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:2422: in eval
    self._eval_loop(
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:2514: in _eval_loop
    with get_precision_context(self.state.precision):
/home/shie44167/miniconda3/envs/composer/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/home/shie44167/workspace/1_read_codes/composer/composer/core/precision.py:65: in get_precision_context
    with torch.cuda.amp.autocast(enabled=True, dtype=torch.bfloat16):
/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/torch/cuda/amp/autocast_mode.py:25: in __init__
    super().__init__("cuda", enabled=enabled, dtype=dtype, cache_enabled=cache_enabled)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <torch.cuda.amp.autocast_mode.autocast object at 0x7f6796a11c60>
device_type = 'cuda', dtype = torch.bfloat16, enabled = True
cache_enabled = True

    def __init__(self, device_type : str,
                 dtype : Optional[_dtype] = None,
                 enabled : bool = True,
                 cache_enabled : Optional[bool] = None):
        if torch._jit_internal.is_scripting():
            self._enabled = enabled
            self.device = device_type
            self.fast_dtype = dtype
            # TODO: support get_autocast_gpu/cpu_dtype
            assert dtype is not None
            return
        self.device = device_type
        if self.device == 'cuda':
            self.fast_dtype = torch.get_autocast_gpu_dtype()
        elif self.device == 'cpu':
            self.fast_dtype = torch.get_autocast_cpu_dtype()
        elif self.device == 'xpu':
            self.fast_dtype = torch.xpu.get_autocast_xpu_dtype()  # type: ignore[attr-defined]
        else:
            raise RuntimeError('User specified autocast device_type must be \'cuda\' or \'cpu\'')
        self._cache_enabled = torch.is_autocast_cache_enabled()
        if torch.cuda.amp.common.amp_definitely_not_available() and self.device == 'cuda':
            warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
            enabled = False
        if dtype is not None:
            self.fast_dtype = dtype
        if cache_enabled is not None:
            self._cache_enabled = cache_enabled
    
        if self.device == 'cpu':
            supported_dtype = [torch.bfloat16]
            if self.fast_dtype not in supported_dtype:
                error_message = 'In CPU autocast, but the target dtype is not supported. Disabling autocast.\n'
                error_message += 'CPU Autocast only supports dtype of torch.bfloat16 currently.'
                warnings.warn(error_message)
                enabled = False
        if self.device == 'xpu':
            supported_dtype = [torch.bfloat16, torch.float16]
            if self.fast_dtype not in supported_dtype:
                error_message = 'In XPU autocast, but the target dtype is not supported. Disabling autocast.\n'
                error_message += 'XPU Autocast only supports dtype of torch.bfloat16 currently.'
                warnings.warn(error_message)
                enabled = False
        if self.device == 'cuda':
            if self.fast_dtype == torch.bfloat16 and not torch.cuda.is_bf16_supported():
>               raise RuntimeError('Current CUDA Device does not support bfloat16. Please switch dtype to float16.')
E               RuntimeError: Current CUDA Device does not support bfloat16. Please switch dtype to float16.

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/torch/amp/autocast_mode.py:221: RuntimeError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
----------------------------- Captured stderr call -----------------------------
******************************
Config:
num_gpus_per_node: 1
num_nodes: 1
rank_zero_seed: 0

******************************


******************************
Config:
num_gpus_per_node: 1
num_nodes: 1
rank_zero_seed: 0

******************************
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809141-judicious-chipmunk
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
INFO     composer.trainer.trainer:trainer.py:1274 Setting seed to 0
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
DEBUG    composer.core.engine:engine.py:484 Closing the engine
DEBUG    composer.core.engine:engine.py:488 Closing callback ProgressBarLogger
DEBUG    composer.core.engine:engine.py:502 Post-closing callback ProgressBarLogger
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809142-sexy-shrew
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
INFO     composer.trainer.trainer:trainer.py:1274 Setting seed to 0
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
________________ test_predict_precision_memory[Precision.BF16] _________________

precision = <Precision.BF16: 'bf16'>

    @pytest.mark.gpu
    @pytest.mark.parametrize('precision', [Precision.AMP, Precision.BF16])
    def test_predict_precision_memory(precision: Precision):
        memory_fp32 = predict_and_measure_memory(Precision.FP32)
>       memory_half = predict_and_measure_memory(precision)

/home/shie44167/workspace/1_read_codes/composer/tests/test_precision.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/tests/test_precision.py:63: in predict_and_measure_memory
    trainer.predict(dataloader=trainer.state.evaluators[0].dataloader)
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:2258: in predict
    with get_precision_context(self.state.precision):
/home/shie44167/miniconda3/envs/composer/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/home/shie44167/workspace/1_read_codes/composer/composer/core/precision.py:65: in get_precision_context
    with torch.cuda.amp.autocast(enabled=True, dtype=torch.bfloat16):
/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/torch/cuda/amp/autocast_mode.py:25: in __init__
    super().__init__("cuda", enabled=enabled, dtype=dtype, cache_enabled=cache_enabled)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <torch.cuda.amp.autocast_mode.autocast object at 0x7f6796bd7d00>
device_type = 'cuda', dtype = torch.bfloat16, enabled = True
cache_enabled = True

    def __init__(self, device_type : str,
                 dtype : Optional[_dtype] = None,
                 enabled : bool = True,
                 cache_enabled : Optional[bool] = None):
        if torch._jit_internal.is_scripting():
            self._enabled = enabled
            self.device = device_type
            self.fast_dtype = dtype
            # TODO: support get_autocast_gpu/cpu_dtype
            assert dtype is not None
            return
        self.device = device_type
        if self.device == 'cuda':
            self.fast_dtype = torch.get_autocast_gpu_dtype()
        elif self.device == 'cpu':
            self.fast_dtype = torch.get_autocast_cpu_dtype()
        elif self.device == 'xpu':
            self.fast_dtype = torch.xpu.get_autocast_xpu_dtype()  # type: ignore[attr-defined]
        else:
            raise RuntimeError('User specified autocast device_type must be \'cuda\' or \'cpu\'')
        self._cache_enabled = torch.is_autocast_cache_enabled()
        if torch.cuda.amp.common.amp_definitely_not_available() and self.device == 'cuda':
            warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
            enabled = False
        if dtype is not None:
            self.fast_dtype = dtype
        if cache_enabled is not None:
            self._cache_enabled = cache_enabled
    
        if self.device == 'cpu':
            supported_dtype = [torch.bfloat16]
            if self.fast_dtype not in supported_dtype:
                error_message = 'In CPU autocast, but the target dtype is not supported. Disabling autocast.\n'
                error_message += 'CPU Autocast only supports dtype of torch.bfloat16 currently.'
                warnings.warn(error_message)
                enabled = False
        if self.device == 'xpu':
            supported_dtype = [torch.bfloat16, torch.float16]
            if self.fast_dtype not in supported_dtype:
                error_message = 'In XPU autocast, but the target dtype is not supported. Disabling autocast.\n'
                error_message += 'XPU Autocast only supports dtype of torch.bfloat16 currently.'
                warnings.warn(error_message)
                enabled = False
        if self.device == 'cuda':
            if self.fast_dtype == torch.bfloat16 and not torch.cuda.is_bf16_supported():
>               raise RuntimeError('Current CUDA Device does not support bfloat16. Please switch dtype to float16.')
E               RuntimeError: Current CUDA Device does not support bfloat16. Please switch dtype to float16.

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/torch/amp/autocast_mode.py:221: RuntimeError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
----------------------------- Captured stderr call -----------------------------
******************************
Config:
num_gpus_per_node: 1
num_nodes: 1
rank_zero_seed: 0

******************************

******************************
Config:
num_gpus_per_node: 1
num_nodes: 1
rank_zero_seed: 0

******************************
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809144-nice-meerkat
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
INFO     composer.trainer.trainer:trainer.py:1274 Setting seed to 0
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
DEBUG    composer.core.engine:engine.py:484 Closing the engine
DEBUG    composer.core.engine:engine.py:488 Closing callback ProgressBarLogger
DEBUG    composer.core.engine:engine.py:502 Post-closing callback ProgressBarLogger
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809145-slim-wallaby
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
INFO     composer.trainer.trainer:trainer.py:1274 Setting seed to 0
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
__________________ test_algorithm_resumption[FusedLayerNorm] ___________________

tmp_path = PosixPath('/tmp/pytest-of-shie44167/pytest-3/test_algorithm_resumption_Fuse0')
alg_cls = <class 'composer.algorithms.fused_layernorm.fused_layernorm.FusedLayerNorm'>

    @pytest.mark.gpu
    @pytest.mark.parametrize('alg_cls', get_algs_with_marks())
    def test_algorithm_resumption(
        tmp_path: pathlib.Path,
        alg_cls: Type[Algorithm],
    ):
        folder1 = os.path.join(tmp_path, 'folder1')
        folder2 = os.path.join(tmp_path, 'folder2')
    
        model = get_alg_model(alg_cls)
        alg_kwargs = get_alg_kwargs(alg_cls)
    
        copied_model = copy.deepcopy(model)  # copy the model so the params will start from the same point
    
        if alg_cls is LayerFreezing:
            pytest.xfail('Known issues')
    
        if alg_cls in (SAM, StochasticDepth):
            pytest.xfail('Mismatch in weights when resuming from a checkpoint.')
    
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)
    
        shared_config = {
            'max_duration': '2ep',
            'save_filename': 'ep{epoch}-rank{rank}',
            'train_subset_num_batches': 2,
            'precision': 'amp',
        }
    
        # train model once, saving checkpoints every epoch
        trainer1 = Trainer(
            model=model,
            train_dataloader=get_alg_dataloader(alg_cls),
            optimizers=optimizer,
            schedulers=scheduler,
            save_folder=folder1,
>           algorithms=alg_cls(**alg_kwargs),
            **shared_config,
        )

/home/shie44167/workspace/1_read_codes/composer/tests/algorithms/test_algorithm_resumption.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/composer/algorithms/fused_layernorm/fused_layernorm.py:98: in __init__
    check_if_apex_installed()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def check_if_apex_installed():
        if not APEX_INSTALLED:
>           raise ImportError(
                'https://github.com/NVIDIA/apex is not installed. The Fused LayerNorm algorithm cannot be applied. The MosaicML Docker Images (https://hub.docker.com/r/mosaicml/pytorch) contain a copy of APEX for easy use.'
            )
E           ImportError: https://github.com/NVIDIA/apex is not installed. The Fused LayerNorm algorithm cannot be applied. The MosaicML Docker Images (https://hub.docker.com/r/mosaicml/pytorch) contain a copy of APEX for easy use.

/home/shie44167/workspace/1_read_codes/composer/composer/algorithms/fused_layernorm/fused_layernorm.py:30: ImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
----------------------------- Captured stdout call -----------------------------






----------------------------- Captured stderr call -----------------------------
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00, 24.17ba/s]
------------------------------ Captured log call -------------------------------
INFO     composer.datasets.lm_dataset:lm_dataset.py:226 LM datasets: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 100
})
INFO     composer.datasets.lm_dataset:lm_dataset.py:227 Subsample ratio: 1.0
INFO     composer.datasets.lm_dataset:lm_dataset.py:228 Total number of samples: 1.000000e+02
INFO     composer.datasets.lm_dataset:lm_dataset.py:229 Total number of tokens: 1.280000e+04
____________________ test_algorithm_trains[FusedLayerNorm] _____________________

alg_cls = <class 'composer.algorithms.fused_layernorm.fused_layernorm.FusedLayerNorm'>

    @pytest.mark.gpu
    @pytest.mark.parametrize('alg_cls', get_algs_with_marks())
    def test_algorithm_trains(alg_cls: Type[Algorithm]):
        alg_kwargs = get_alg_kwargs(alg_cls)
        model = get_alg_model(alg_cls)
        dataloader = get_alg_dataloader(alg_cls)
        trainer = Trainer(
            model=model,
            train_dataloader=dataloader,
            max_duration='2ep',
>           algorithms=alg_cls(**alg_kwargs),
        )

/home/shie44167/workspace/1_read_codes/composer/tests/algorithms/test_algorithms_train.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/composer/algorithms/fused_layernorm/fused_layernorm.py:98: in __init__
    check_if_apex_installed()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def check_if_apex_installed():
        if not APEX_INSTALLED:
>           raise ImportError(
                'https://github.com/NVIDIA/apex is not installed. The Fused LayerNorm algorithm cannot be applied. The MosaicML Docker Images (https://hub.docker.com/r/mosaicml/pytorch) contain a copy of APEX for easy use.'
            )
E           ImportError: https://github.com/NVIDIA/apex is not installed. The Fused LayerNorm algorithm cannot be applied. The MosaicML Docker Images (https://hub.docker.com/r/mosaicml/pytorch) contain a copy of APEX for easy use.

/home/shie44167/workspace/1_read_codes/composer/composer/algorithms/fused_layernorm/fused_layernorm.py:30: ImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
----------------------------- Captured stdout call -----------------------------






----------------------------- Captured stderr call -----------------------------
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00, 24.65ba/s]
------------------------------ Captured log call -------------------------------
INFO     composer.datasets.lm_dataset:lm_dataset.py:226 LM datasets: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 100
})
INFO     composer.datasets.lm_dataset:lm_dataset.py:227 Subsample ratio: 1.0
INFO     composer.datasets.lm_dataset:lm_dataset.py:228 Total number of samples: 1.000000e+02
INFO     composer.datasets.lm_dataset:lm_dataset.py:229 Total number of tokens: 1.280000e+04
_____________________ test_fused_layernorm_functional[gpu] _____________________

synthetic_bert_state = (<composer.core.state.State object at 0x7f67b16d71f0>, HuggingFaceModel(
  (model): BertForMaskedLM(
    (bert): BertM...4, out_features=297, bias=True)
      )
    )
  )
), <torch.utils.data.dataloader.DataLoader object at 0x7f67b16d6b30>)
device = 'gpu'

    @device('gpu')
    def test_fused_layernorm_functional(synthetic_bert_state: Tuple, device: str):
        state, _, _ = synthetic_bert_state
>       apply_fused_layernorm(state.model, state.optimizers)

/home/shie44167/workspace/1_read_codes/composer/tests/algorithms/test_fused_layernorm.py:42: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/composer/algorithms/fused_layernorm/fused_layernorm.py:49: in apply_fused_layernorm
    check_if_apex_installed()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def check_if_apex_installed():
        if not APEX_INSTALLED:
>           raise ImportError(
                'https://github.com/NVIDIA/apex is not installed. The Fused LayerNorm algorithm cannot be applied. The MosaicML Docker Images (https://hub.docker.com/r/mosaicml/pytorch) contain a copy of APEX for easy use.'
            )
E           ImportError: https://github.com/NVIDIA/apex is not installed. The Fused LayerNorm algorithm cannot be applied. The MosaicML Docker Images (https://hub.docker.com/r/mosaicml/pytorch) contain a copy of APEX for easy use.

/home/shie44167/workspace/1_read_codes/composer/composer/algorithms/fused_layernorm/fused_layernorm.py:30: ImportError
---------------------------- Captured stdout setup -----------------------------






---------------------------- Captured stderr setup -----------------------------
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00, 24.88ba/s]
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.datasets.lm_dataset:lm_dataset.py:226 LM datasets: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 100
})
INFO     composer.datasets.lm_dataset:lm_dataset.py:227 Subsample ratio: 1.0
INFO     composer.datasets.lm_dataset:lm_dataset.py:228 Total number of samples: 1.000000e+02
INFO     composer.datasets.lm_dataset:lm_dataset.py:229 Total number of tokens: 1.280000e+04
_____________________ test_fused_layernorm_algorithm[gpu] ______________________

synthetic_bert_state = (<composer.core.state.State object at 0x7f67b151b460>, HuggingFaceModel(
  (model): BertForMaskedLM(
    (bert): BertM...4, out_features=297, bias=True)
      )
    )
  )
), <torch.utils.data.dataloader.DataLoader object at 0x7f67b15182b0>)
empty_logger = <composer.loggers.logger.Logger object at 0x7f67b151ae30>
device = 'gpu'

    @device('gpu')
    def test_fused_layernorm_algorithm(synthetic_bert_state: Tuple, empty_logger: Logger, device: str):
        pytest.importorskip('transformers')
        from transformers import BertForMaskedLM, BertForSequenceClassification
    
        state, _, _ = synthetic_bert_state
>       fused_layernorm = FusedLayerNorm()

/home/shie44167/workspace/1_read_codes/composer/tests/algorithms/test_fused_layernorm.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/composer/algorithms/fused_layernorm/fused_layernorm.py:98: in __init__
    check_if_apex_installed()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def check_if_apex_installed():
        if not APEX_INSTALLED:
>           raise ImportError(
                'https://github.com/NVIDIA/apex is not installed. The Fused LayerNorm algorithm cannot be applied. The MosaicML Docker Images (https://hub.docker.com/r/mosaicml/pytorch) contain a copy of APEX for easy use.'
            )
E           ImportError: https://github.com/NVIDIA/apex is not installed. The Fused LayerNorm algorithm cannot be applied. The MosaicML Docker Images (https://hub.docker.com/r/mosaicml/pytorch) contain a copy of APEX for easy use.

/home/shie44167/workspace/1_read_codes/composer/composer/algorithms/fused_layernorm/fused_layernorm.py:30: ImportError
---------------------------- Captured stdout setup -----------------------------






---------------------------- Captured stderr setup -----------------------------
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00, 24.54ba/s]
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.datasets.lm_dataset:lm_dataset.py:226 LM datasets: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 100
})
INFO     composer.datasets.lm_dataset:lm_dataset.py:227 Subsample ratio: 1.0
INFO     composer.datasets.lm_dataset:lm_dataset.py:228 Total number of samples: 1.000000e+02
INFO     composer.datasets.lm_dataset:lm_dataset.py:229 Total number of tokens: 1.280000e+04
_ TestCheckpointResumption.test_resumption[None-1ep-ep{epoch}-rank{rank}.pt-ep1-rank{rank}.pt-latest-rank{rank}.pt-deepspeed-zero1-1] _

self = <composer.trainer.trainer.Trainer object at 0x7f67b1332fe0>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
>               import deepspeed
E               ModuleNotFoundError: No module named 'deepspeed'

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1159: ModuleNotFoundError

The above exception was the direct cause of the following exception:

self = <tests.trainer.test_checkpoint.TestCheckpointResumption object at 0x7f6921900280>
device = 'gpu', world_size = 1, deepspeed_zero_stage = 1, save_interval = '1ep'
save_filename = 'ep{epoch}-rank{rank}.pt', resume_file = 'ep1-rank{rank}.pt.tar'
final_checkpoint = 'latest-rank{rank}.pt.tar', seed = None
tmp_path = PosixPath('/tmp/pytest-of-shie44167/pytest-3/test_resumption_None_1ep_ep_ep2')

    @pytest.mark.parametrize('world_size', [
        pytest.param(1),
        pytest.param(2, marks=pytest.mark.world_size(2)),
    ])
    @pytest.mark.parametrize('device,deepspeed_zero_stage', [
        pytest.param('cpu', None, id='cpu-ddp'),
        pytest.param('gpu', None, id='gpu-ddp', marks=pytest.mark.gpu),
        pytest.param('gpu', 0, id='deepspeed-zero0', marks=pytest.mark.gpu),
        pytest.param('gpu', 1, id='deepspeed-zero1', marks=pytest.mark.gpu),
        pytest.param('gpu', 2, id='deepspeed-zero2', marks=pytest.mark.gpu),
    ])
    @pytest.mark.parametrize(
        'seed,save_interval,save_filename,resume_file,final_checkpoint',
        [
            [None, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'latest-rank{rank}.pt'
            ],  # test randomized seed saving and symlinking
            [42, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'ep2-rank{rank}.pt'],  # test save at epoch end
            [42, '1ep', 'ep{epoch}-rank{rank}.tgz', 'ep1-rank{rank}.tgz', 'ep2-rank{rank}.tgz'
            ],  # test tarball with compression
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba4-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch in partial epoch
            [42, '1ba', 'ba{batch}-rank{rank}.pt', 'ba5-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch at epoch end
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba6-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch after complete epoch
        ],
    )
    def test_resumption(
        self,
        device: str,
        world_size: int,
        deepspeed_zero_stage: Optional[int],
        save_interval: str,
        save_filename: str,
        resume_file: str,
        final_checkpoint: str,
        seed: Optional[int],
        tmp_path: pathlib.Path,
    ):
    
        # all ranks use rank 0 folder
        tmp_paths = dist.all_gather_object(os.path.abspath(tmp_path))
        save_folder = pathlib.Path(tmp_paths[0])
    
        if deepspeed_zero_stage:
            deepspeed_config = {'zero_optimization': {'stage': deepspeed_zero_stage}}
    
            # save_checkpoint appends .tar for deepspeed
            if not is_tar(resume_file):
                resume_file += '.tar'
            if not is_tar(final_checkpoint):
                final_checkpoint += '.tar'
        else:
            deepspeed_config = None
    
>       trainer_1 = self.get_trainer(
            save_folder=os.path.join(save_folder, 'first'),
            save_filename=save_filename,
            save_interval=save_interval,
            eval_interval=save_interval,
            deepspeed_config=deepspeed_config,
            seed=seed,
            device=device,
        )

/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:606: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:529: in get_trainer
    return Trainer(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.trainer.trainer.Trainer object at 0x7f67b1332fe0>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
                import deepspeed
            except ImportError as e:
>               raise MissingConditionalImportError(
                    extra_deps_group='deepspeed',
                    conda_package='deepspeed>=0.5.5',
                    conda_channel=None,
                ) from e
E               composer.utils.import_helpers.MissingConditionalImportError: Composer was installed without deepspeed support. To use deepspeed related packages, with Composer, run `pip install 'mosaicml[deepspeed]'` if using pip or `pip install deepspeed>=0.5.5` if using Anaconda.

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1161: MissingConditionalImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809266-upbeat-dingo
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
_ TestCheckpointResumption.test_resumption[None-1ep-ep{epoch}-rank{rank}.pt-ep1-rank{rank}.pt-latest-rank{rank}.pt-deepspeed-zero2-1] _

self = <composer.trainer.trainer.Trainer object at 0x7f67b137b9d0>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
>               import deepspeed
E               ModuleNotFoundError: No module named 'deepspeed'

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1159: ModuleNotFoundError

The above exception was the direct cause of the following exception:

self = <tests.trainer.test_checkpoint.TestCheckpointResumption object at 0x7f69219003a0>
device = 'gpu', world_size = 1, deepspeed_zero_stage = 2, save_interval = '1ep'
save_filename = 'ep{epoch}-rank{rank}.pt', resume_file = 'ep1-rank{rank}.pt.tar'
final_checkpoint = 'latest-rank{rank}.pt.tar', seed = None
tmp_path = PosixPath('/tmp/pytest-of-shie44167/pytest-3/test_resumption_None_1ep_ep_ep3')

    @pytest.mark.parametrize('world_size', [
        pytest.param(1),
        pytest.param(2, marks=pytest.mark.world_size(2)),
    ])
    @pytest.mark.parametrize('device,deepspeed_zero_stage', [
        pytest.param('cpu', None, id='cpu-ddp'),
        pytest.param('gpu', None, id='gpu-ddp', marks=pytest.mark.gpu),
        pytest.param('gpu', 0, id='deepspeed-zero0', marks=pytest.mark.gpu),
        pytest.param('gpu', 1, id='deepspeed-zero1', marks=pytest.mark.gpu),
        pytest.param('gpu', 2, id='deepspeed-zero2', marks=pytest.mark.gpu),
    ])
    @pytest.mark.parametrize(
        'seed,save_interval,save_filename,resume_file,final_checkpoint',
        [
            [None, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'latest-rank{rank}.pt'
            ],  # test randomized seed saving and symlinking
            [42, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'ep2-rank{rank}.pt'],  # test save at epoch end
            [42, '1ep', 'ep{epoch}-rank{rank}.tgz', 'ep1-rank{rank}.tgz', 'ep2-rank{rank}.tgz'
            ],  # test tarball with compression
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba4-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch in partial epoch
            [42, '1ba', 'ba{batch}-rank{rank}.pt', 'ba5-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch at epoch end
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba6-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch after complete epoch
        ],
    )
    def test_resumption(
        self,
        device: str,
        world_size: int,
        deepspeed_zero_stage: Optional[int],
        save_interval: str,
        save_filename: str,
        resume_file: str,
        final_checkpoint: str,
        seed: Optional[int],
        tmp_path: pathlib.Path,
    ):
    
        # all ranks use rank 0 folder
        tmp_paths = dist.all_gather_object(os.path.abspath(tmp_path))
        save_folder = pathlib.Path(tmp_paths[0])
    
        if deepspeed_zero_stage:
            deepspeed_config = {'zero_optimization': {'stage': deepspeed_zero_stage}}
    
            # save_checkpoint appends .tar for deepspeed
            if not is_tar(resume_file):
                resume_file += '.tar'
            if not is_tar(final_checkpoint):
                final_checkpoint += '.tar'
        else:
            deepspeed_config = None
    
>       trainer_1 = self.get_trainer(
            save_folder=os.path.join(save_folder, 'first'),
            save_filename=save_filename,
            save_interval=save_interval,
            eval_interval=save_interval,
            deepspeed_config=deepspeed_config,
            seed=seed,
            device=device,
        )

/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:606: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:529: in get_trainer
    return Trainer(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.trainer.trainer.Trainer object at 0x7f67b137b9d0>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
                import deepspeed
            except ImportError as e:
>               raise MissingConditionalImportError(
                    extra_deps_group='deepspeed',
                    conda_package='deepspeed>=0.5.5',
                    conda_channel=None,
                ) from e
E               composer.utils.import_helpers.MissingConditionalImportError: Composer was installed without deepspeed support. To use deepspeed related packages, with Composer, run `pip install 'mosaicml[deepspeed]'` if using pip or `pip install deepspeed>=0.5.5` if using Anaconda.

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1161: MissingConditionalImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809266-pragmatic-silkworm
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
_ TestCheckpointResumption.test_resumption[42-1ep-ep{epoch}-rank{rank}.pt-ep1-rank{rank}.pt-ep2-rank{rank}.pt-deepspeed-zero1-1] _

self = <composer.trainer.trainer.Trainer object at 0x7f67b137b9a0>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
>               import deepspeed
E               ModuleNotFoundError: No module named 'deepspeed'

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1159: ModuleNotFoundError

The above exception was the direct cause of the following exception:

self = <tests.trainer.test_checkpoint.TestCheckpointResumption object at 0x7f6921900820>
device = 'gpu', world_size = 1, deepspeed_zero_stage = 1, save_interval = '1ep'
save_filename = 'ep{epoch}-rank{rank}.pt', resume_file = 'ep1-rank{rank}.pt.tar'
final_checkpoint = 'ep2-rank{rank}.pt.tar', seed = 42
tmp_path = PosixPath('/tmp/pytest-of-shie44167/pytest-3/test_resumption_42_1ep_ep_epoc2')

    @pytest.mark.parametrize('world_size', [
        pytest.param(1),
        pytest.param(2, marks=pytest.mark.world_size(2)),
    ])
    @pytest.mark.parametrize('device,deepspeed_zero_stage', [
        pytest.param('cpu', None, id='cpu-ddp'),
        pytest.param('gpu', None, id='gpu-ddp', marks=pytest.mark.gpu),
        pytest.param('gpu', 0, id='deepspeed-zero0', marks=pytest.mark.gpu),
        pytest.param('gpu', 1, id='deepspeed-zero1', marks=pytest.mark.gpu),
        pytest.param('gpu', 2, id='deepspeed-zero2', marks=pytest.mark.gpu),
    ])
    @pytest.mark.parametrize(
        'seed,save_interval,save_filename,resume_file,final_checkpoint',
        [
            [None, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'latest-rank{rank}.pt'
            ],  # test randomized seed saving and symlinking
            [42, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'ep2-rank{rank}.pt'],  # test save at epoch end
            [42, '1ep', 'ep{epoch}-rank{rank}.tgz', 'ep1-rank{rank}.tgz', 'ep2-rank{rank}.tgz'
            ],  # test tarball with compression
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba4-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch in partial epoch
            [42, '1ba', 'ba{batch}-rank{rank}.pt', 'ba5-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch at epoch end
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba6-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch after complete epoch
        ],
    )
    def test_resumption(
        self,
        device: str,
        world_size: int,
        deepspeed_zero_stage: Optional[int],
        save_interval: str,
        save_filename: str,
        resume_file: str,
        final_checkpoint: str,
        seed: Optional[int],
        tmp_path: pathlib.Path,
    ):
    
        # all ranks use rank 0 folder
        tmp_paths = dist.all_gather_object(os.path.abspath(tmp_path))
        save_folder = pathlib.Path(tmp_paths[0])
    
        if deepspeed_zero_stage:
            deepspeed_config = {'zero_optimization': {'stage': deepspeed_zero_stage}}
    
            # save_checkpoint appends .tar for deepspeed
            if not is_tar(resume_file):
                resume_file += '.tar'
            if not is_tar(final_checkpoint):
                final_checkpoint += '.tar'
        else:
            deepspeed_config = None
    
>       trainer_1 = self.get_trainer(
            save_folder=os.path.join(save_folder, 'first'),
            save_filename=save_filename,
            save_interval=save_interval,
            eval_interval=save_interval,
            deepspeed_config=deepspeed_config,
            seed=seed,
            device=device,
        )

/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:606: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:529: in get_trainer
    return Trainer(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.trainer.trainer.Trainer object at 0x7f67b137b9a0>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
                import deepspeed
            except ImportError as e:
>               raise MissingConditionalImportError(
                    extra_deps_group='deepspeed',
                    conda_package='deepspeed>=0.5.5',
                    conda_channel=None,
                ) from e
E               composer.utils.import_helpers.MissingConditionalImportError: Composer was installed without deepspeed support. To use deepspeed related packages, with Composer, run `pip install 'mosaicml[deepspeed]'` if using pip or `pip install deepspeed>=0.5.5` if using Anaconda.

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1161: MissingConditionalImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 42
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809274-thankful-seahorse
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
_ TestCheckpointResumption.test_resumption[42-1ep-ep{epoch}-rank{rank}.pt-ep1-rank{rank}.pt-ep2-rank{rank}.pt-deepspeed-zero2-1] _

self = <composer.trainer.trainer.Trainer object at 0x7f67b1019f60>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
>               import deepspeed
E               ModuleNotFoundError: No module named 'deepspeed'

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1159: ModuleNotFoundError

The above exception was the direct cause of the following exception:

self = <tests.trainer.test_checkpoint.TestCheckpointResumption object at 0x7f6921900940>
device = 'gpu', world_size = 1, deepspeed_zero_stage = 2, save_interval = '1ep'
save_filename = 'ep{epoch}-rank{rank}.pt', resume_file = 'ep1-rank{rank}.pt.tar'
final_checkpoint = 'ep2-rank{rank}.pt.tar', seed = 42
tmp_path = PosixPath('/tmp/pytest-of-shie44167/pytest-3/test_resumption_42_1ep_ep_epoc3')

    @pytest.mark.parametrize('world_size', [
        pytest.param(1),
        pytest.param(2, marks=pytest.mark.world_size(2)),
    ])
    @pytest.mark.parametrize('device,deepspeed_zero_stage', [
        pytest.param('cpu', None, id='cpu-ddp'),
        pytest.param('gpu', None, id='gpu-ddp', marks=pytest.mark.gpu),
        pytest.param('gpu', 0, id='deepspeed-zero0', marks=pytest.mark.gpu),
        pytest.param('gpu', 1, id='deepspeed-zero1', marks=pytest.mark.gpu),
        pytest.param('gpu', 2, id='deepspeed-zero2', marks=pytest.mark.gpu),
    ])
    @pytest.mark.parametrize(
        'seed,save_interval,save_filename,resume_file,final_checkpoint',
        [
            [None, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'latest-rank{rank}.pt'
            ],  # test randomized seed saving and symlinking
            [42, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'ep2-rank{rank}.pt'],  # test save at epoch end
            [42, '1ep', 'ep{epoch}-rank{rank}.tgz', 'ep1-rank{rank}.tgz', 'ep2-rank{rank}.tgz'
            ],  # test tarball with compression
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba4-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch in partial epoch
            [42, '1ba', 'ba{batch}-rank{rank}.pt', 'ba5-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch at epoch end
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba6-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch after complete epoch
        ],
    )
    def test_resumption(
        self,
        device: str,
        world_size: int,
        deepspeed_zero_stage: Optional[int],
        save_interval: str,
        save_filename: str,
        resume_file: str,
        final_checkpoint: str,
        seed: Optional[int],
        tmp_path: pathlib.Path,
    ):
    
        # all ranks use rank 0 folder
        tmp_paths = dist.all_gather_object(os.path.abspath(tmp_path))
        save_folder = pathlib.Path(tmp_paths[0])
    
        if deepspeed_zero_stage:
            deepspeed_config = {'zero_optimization': {'stage': deepspeed_zero_stage}}
    
            # save_checkpoint appends .tar for deepspeed
            if not is_tar(resume_file):
                resume_file += '.tar'
            if not is_tar(final_checkpoint):
                final_checkpoint += '.tar'
        else:
            deepspeed_config = None
    
>       trainer_1 = self.get_trainer(
            save_folder=os.path.join(save_folder, 'first'),
            save_filename=save_filename,
            save_interval=save_interval,
            eval_interval=save_interval,
            deepspeed_config=deepspeed_config,
            seed=seed,
            device=device,
        )

/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:606: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:529: in get_trainer
    return Trainer(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.trainer.trainer.Trainer object at 0x7f67b1019f60>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
                import deepspeed
            except ImportError as e:
>               raise MissingConditionalImportError(
                    extra_deps_group='deepspeed',
                    conda_package='deepspeed>=0.5.5',
                    conda_channel=None,
                ) from e
E               composer.utils.import_helpers.MissingConditionalImportError: Composer was installed without deepspeed support. To use deepspeed related packages, with Composer, run `pip install 'mosaicml[deepspeed]'` if using pip or `pip install deepspeed>=0.5.5` if using Anaconda.

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1161: MissingConditionalImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 42
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809274-holistic-gorilla
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
_ TestCheckpointResumption.test_resumption[42-1ep-ep{epoch}-rank{rank}.tgz-ep1-rank{rank}.tgz-ep2-rank{rank}.tgz-deepspeed-zero1-1] _

self = <composer.trainer.trainer.Trainer object at 0x7f67b1141ab0>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
>               import deepspeed
E               ModuleNotFoundError: No module named 'deepspeed'

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1159: ModuleNotFoundError

The above exception was the direct cause of the following exception:

self = <tests.trainer.test_checkpoint.TestCheckpointResumption object at 0x7f6921900dc0>
device = 'gpu', world_size = 1, deepspeed_zero_stage = 1, save_interval = '1ep'
save_filename = 'ep{epoch}-rank{rank}.tgz', resume_file = 'ep1-rank{rank}.tgz'
final_checkpoint = 'ep2-rank{rank}.tgz', seed = 42
tmp_path = PosixPath('/tmp/pytest-of-shie44167/pytest-3/test_resumption_42_1ep_ep_epoc6')

    @pytest.mark.parametrize('world_size', [
        pytest.param(1),
        pytest.param(2, marks=pytest.mark.world_size(2)),
    ])
    @pytest.mark.parametrize('device,deepspeed_zero_stage', [
        pytest.param('cpu', None, id='cpu-ddp'),
        pytest.param('gpu', None, id='gpu-ddp', marks=pytest.mark.gpu),
        pytest.param('gpu', 0, id='deepspeed-zero0', marks=pytest.mark.gpu),
        pytest.param('gpu', 1, id='deepspeed-zero1', marks=pytest.mark.gpu),
        pytest.param('gpu', 2, id='deepspeed-zero2', marks=pytest.mark.gpu),
    ])
    @pytest.mark.parametrize(
        'seed,save_interval,save_filename,resume_file,final_checkpoint',
        [
            [None, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'latest-rank{rank}.pt'
            ],  # test randomized seed saving and symlinking
            [42, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'ep2-rank{rank}.pt'],  # test save at epoch end
            [42, '1ep', 'ep{epoch}-rank{rank}.tgz', 'ep1-rank{rank}.tgz', 'ep2-rank{rank}.tgz'
            ],  # test tarball with compression
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba4-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch in partial epoch
            [42, '1ba', 'ba{batch}-rank{rank}.pt', 'ba5-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch at epoch end
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba6-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch after complete epoch
        ],
    )
    def test_resumption(
        self,
        device: str,
        world_size: int,
        deepspeed_zero_stage: Optional[int],
        save_interval: str,
        save_filename: str,
        resume_file: str,
        final_checkpoint: str,
        seed: Optional[int],
        tmp_path: pathlib.Path,
    ):
    
        # all ranks use rank 0 folder
        tmp_paths = dist.all_gather_object(os.path.abspath(tmp_path))
        save_folder = pathlib.Path(tmp_paths[0])
    
        if deepspeed_zero_stage:
            deepspeed_config = {'zero_optimization': {'stage': deepspeed_zero_stage}}
    
            # save_checkpoint appends .tar for deepspeed
            if not is_tar(resume_file):
                resume_file += '.tar'
            if not is_tar(final_checkpoint):
                final_checkpoint += '.tar'
        else:
            deepspeed_config = None
    
>       trainer_1 = self.get_trainer(
            save_folder=os.path.join(save_folder, 'first'),
            save_filename=save_filename,
            save_interval=save_interval,
            eval_interval=save_interval,
            deepspeed_config=deepspeed_config,
            seed=seed,
            device=device,
        )

/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:606: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:529: in get_trainer
    return Trainer(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.trainer.trainer.Trainer object at 0x7f67b1141ab0>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
                import deepspeed
            except ImportError as e:
>               raise MissingConditionalImportError(
                    extra_deps_group='deepspeed',
                    conda_package='deepspeed>=0.5.5',
                    conda_channel=None,
                ) from e
E               composer.utils.import_helpers.MissingConditionalImportError: Composer was installed without deepspeed support. To use deepspeed related packages, with Composer, run `pip install 'mosaicml[deepspeed]'` if using pip or `pip install deepspeed>=0.5.5` if using Anaconda.

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1161: MissingConditionalImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 42
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809281-dangerous-otter
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
_ TestCheckpointResumption.test_resumption[42-1ep-ep{epoch}-rank{rank}.tgz-ep1-rank{rank}.tgz-ep2-rank{rank}.tgz-deepspeed-zero2-1] _

self = <composer.trainer.trainer.Trainer object at 0x7f67b10a7e20>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
>               import deepspeed
E               ModuleNotFoundError: No module named 'deepspeed'

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1159: ModuleNotFoundError

The above exception was the direct cause of the following exception:

self = <tests.trainer.test_checkpoint.TestCheckpointResumption object at 0x7f6921900ee0>
device = 'gpu', world_size = 1, deepspeed_zero_stage = 2, save_interval = '1ep'
save_filename = 'ep{epoch}-rank{rank}.tgz', resume_file = 'ep1-rank{rank}.tgz'
final_checkpoint = 'ep2-rank{rank}.tgz', seed = 42
tmp_path = PosixPath('/tmp/pytest-of-shie44167/pytest-3/test_resumption_42_1ep_ep_epoc7')

    @pytest.mark.parametrize('world_size', [
        pytest.param(1),
        pytest.param(2, marks=pytest.mark.world_size(2)),
    ])
    @pytest.mark.parametrize('device,deepspeed_zero_stage', [
        pytest.param('cpu', None, id='cpu-ddp'),
        pytest.param('gpu', None, id='gpu-ddp', marks=pytest.mark.gpu),
        pytest.param('gpu', 0, id='deepspeed-zero0', marks=pytest.mark.gpu),
        pytest.param('gpu', 1, id='deepspeed-zero1', marks=pytest.mark.gpu),
        pytest.param('gpu', 2, id='deepspeed-zero2', marks=pytest.mark.gpu),
    ])
    @pytest.mark.parametrize(
        'seed,save_interval,save_filename,resume_file,final_checkpoint',
        [
            [None, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'latest-rank{rank}.pt'
            ],  # test randomized seed saving and symlinking
            [42, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'ep2-rank{rank}.pt'],  # test save at epoch end
            [42, '1ep', 'ep{epoch}-rank{rank}.tgz', 'ep1-rank{rank}.tgz', 'ep2-rank{rank}.tgz'
            ],  # test tarball with compression
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba4-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch in partial epoch
            [42, '1ba', 'ba{batch}-rank{rank}.pt', 'ba5-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch at epoch end
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba6-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch after complete epoch
        ],
    )
    def test_resumption(
        self,
        device: str,
        world_size: int,
        deepspeed_zero_stage: Optional[int],
        save_interval: str,
        save_filename: str,
        resume_file: str,
        final_checkpoint: str,
        seed: Optional[int],
        tmp_path: pathlib.Path,
    ):
    
        # all ranks use rank 0 folder
        tmp_paths = dist.all_gather_object(os.path.abspath(tmp_path))
        save_folder = pathlib.Path(tmp_paths[0])
    
        if deepspeed_zero_stage:
            deepspeed_config = {'zero_optimization': {'stage': deepspeed_zero_stage}}
    
            # save_checkpoint appends .tar for deepspeed
            if not is_tar(resume_file):
                resume_file += '.tar'
            if not is_tar(final_checkpoint):
                final_checkpoint += '.tar'
        else:
            deepspeed_config = None
    
>       trainer_1 = self.get_trainer(
            save_folder=os.path.join(save_folder, 'first'),
            save_filename=save_filename,
            save_interval=save_interval,
            eval_interval=save_interval,
            deepspeed_config=deepspeed_config,
            seed=seed,
            device=device,
        )

/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:606: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:529: in get_trainer
    return Trainer(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.trainer.trainer.Trainer object at 0x7f67b10a7e20>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
                import deepspeed
            except ImportError as e:
>               raise MissingConditionalImportError(
                    extra_deps_group='deepspeed',
                    conda_package='deepspeed>=0.5.5',
                    conda_channel=None,
                ) from e
E               composer.utils.import_helpers.MissingConditionalImportError: Composer was installed without deepspeed support. To use deepspeed related packages, with Composer, run `pip install 'mosaicml[deepspeed]'` if using pip or `pip install deepspeed>=0.5.5` if using Anaconda.

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1161: MissingConditionalImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 42
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809281-delicate-hawk
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
_ TestCheckpointResumption.test_resumption[42-2ba-ba{batch}-rank{rank}.pt-ba4-rank{rank}.pt-ba8-rank{rank}.pt-deepspeed-zero1-1] _

self = <composer.trainer.trainer.Trainer object at 0x7f67b1230400>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
>               import deepspeed
E               ModuleNotFoundError: No module named 'deepspeed'

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1159: ModuleNotFoundError

The above exception was the direct cause of the following exception:

self = <tests.trainer.test_checkpoint.TestCheckpointResumption object at 0x7f6921901360>
device = 'gpu', world_size = 1, deepspeed_zero_stage = 1, save_interval = '2ba'
save_filename = 'ba{batch}-rank{rank}.pt', resume_file = 'ba4-rank{rank}.pt.tar'
final_checkpoint = 'ba8-rank{rank}.pt.tar', seed = 42
tmp_path = PosixPath('/tmp/pytest-of-shie44167/pytest-3/test_resumption_42_2ba_ba_batc2')

    @pytest.mark.parametrize('world_size', [
        pytest.param(1),
        pytest.param(2, marks=pytest.mark.world_size(2)),
    ])
    @pytest.mark.parametrize('device,deepspeed_zero_stage', [
        pytest.param('cpu', None, id='cpu-ddp'),
        pytest.param('gpu', None, id='gpu-ddp', marks=pytest.mark.gpu),
        pytest.param('gpu', 0, id='deepspeed-zero0', marks=pytest.mark.gpu),
        pytest.param('gpu', 1, id='deepspeed-zero1', marks=pytest.mark.gpu),
        pytest.param('gpu', 2, id='deepspeed-zero2', marks=pytest.mark.gpu),
    ])
    @pytest.mark.parametrize(
        'seed,save_interval,save_filename,resume_file,final_checkpoint',
        [
            [None, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'latest-rank{rank}.pt'
            ],  # test randomized seed saving and symlinking
            [42, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'ep2-rank{rank}.pt'],  # test save at epoch end
            [42, '1ep', 'ep{epoch}-rank{rank}.tgz', 'ep1-rank{rank}.tgz', 'ep2-rank{rank}.tgz'
            ],  # test tarball with compression
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba4-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch in partial epoch
            [42, '1ba', 'ba{batch}-rank{rank}.pt', 'ba5-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch at epoch end
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba6-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch after complete epoch
        ],
    )
    def test_resumption(
        self,
        device: str,
        world_size: int,
        deepspeed_zero_stage: Optional[int],
        save_interval: str,
        save_filename: str,
        resume_file: str,
        final_checkpoint: str,
        seed: Optional[int],
        tmp_path: pathlib.Path,
    ):
    
        # all ranks use rank 0 folder
        tmp_paths = dist.all_gather_object(os.path.abspath(tmp_path))
        save_folder = pathlib.Path(tmp_paths[0])
    
        if deepspeed_zero_stage:
            deepspeed_config = {'zero_optimization': {'stage': deepspeed_zero_stage}}
    
            # save_checkpoint appends .tar for deepspeed
            if not is_tar(resume_file):
                resume_file += '.tar'
            if not is_tar(final_checkpoint):
                final_checkpoint += '.tar'
        else:
            deepspeed_config = None
    
>       trainer_1 = self.get_trainer(
            save_folder=os.path.join(save_folder, 'first'),
            save_filename=save_filename,
            save_interval=save_interval,
            eval_interval=save_interval,
            deepspeed_config=deepspeed_config,
            seed=seed,
            device=device,
        )

/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:606: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:529: in get_trainer
    return Trainer(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.trainer.trainer.Trainer object at 0x7f67b1230400>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
                import deepspeed
            except ImportError as e:
>               raise MissingConditionalImportError(
                    extra_deps_group='deepspeed',
                    conda_package='deepspeed>=0.5.5',
                    conda_channel=None,
                ) from e
E               composer.utils.import_helpers.MissingConditionalImportError: Composer was installed without deepspeed support. To use deepspeed related packages, with Composer, run `pip install 'mosaicml[deepspeed]'` if using pip or `pip install deepspeed>=0.5.5` if using Anaconda.

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1161: MissingConditionalImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 42
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809299-interesting-trout
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
_ TestCheckpointResumption.test_resumption[42-2ba-ba{batch}-rank{rank}.pt-ba4-rank{rank}.pt-ba8-rank{rank}.pt-deepspeed-zero2-1] _

self = <composer.trainer.trainer.Trainer object at 0x7f67b10a4a90>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
>               import deepspeed
E               ModuleNotFoundError: No module named 'deepspeed'

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1159: ModuleNotFoundError

The above exception was the direct cause of the following exception:

self = <tests.trainer.test_checkpoint.TestCheckpointResumption object at 0x7f6921901480>
device = 'gpu', world_size = 1, deepspeed_zero_stage = 2, save_interval = '2ba'
save_filename = 'ba{batch}-rank{rank}.pt', resume_file = 'ba4-rank{rank}.pt.tar'
final_checkpoint = 'ba8-rank{rank}.pt.tar', seed = 42
tmp_path = PosixPath('/tmp/pytest-of-shie44167/pytest-3/test_resumption_42_2ba_ba_batc3')

    @pytest.mark.parametrize('world_size', [
        pytest.param(1),
        pytest.param(2, marks=pytest.mark.world_size(2)),
    ])
    @pytest.mark.parametrize('device,deepspeed_zero_stage', [
        pytest.param('cpu', None, id='cpu-ddp'),
        pytest.param('gpu', None, id='gpu-ddp', marks=pytest.mark.gpu),
        pytest.param('gpu', 0, id='deepspeed-zero0', marks=pytest.mark.gpu),
        pytest.param('gpu', 1, id='deepspeed-zero1', marks=pytest.mark.gpu),
        pytest.param('gpu', 2, id='deepspeed-zero2', marks=pytest.mark.gpu),
    ])
    @pytest.mark.parametrize(
        'seed,save_interval,save_filename,resume_file,final_checkpoint',
        [
            [None, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'latest-rank{rank}.pt'
            ],  # test randomized seed saving and symlinking
            [42, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'ep2-rank{rank}.pt'],  # test save at epoch end
            [42, '1ep', 'ep{epoch}-rank{rank}.tgz', 'ep1-rank{rank}.tgz', 'ep2-rank{rank}.tgz'
            ],  # test tarball with compression
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba4-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch in partial epoch
            [42, '1ba', 'ba{batch}-rank{rank}.pt', 'ba5-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch at epoch end
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba6-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch after complete epoch
        ],
    )
    def test_resumption(
        self,
        device: str,
        world_size: int,
        deepspeed_zero_stage: Optional[int],
        save_interval: str,
        save_filename: str,
        resume_file: str,
        final_checkpoint: str,
        seed: Optional[int],
        tmp_path: pathlib.Path,
    ):
    
        # all ranks use rank 0 folder
        tmp_paths = dist.all_gather_object(os.path.abspath(tmp_path))
        save_folder = pathlib.Path(tmp_paths[0])
    
        if deepspeed_zero_stage:
            deepspeed_config = {'zero_optimization': {'stage': deepspeed_zero_stage}}
    
            # save_checkpoint appends .tar for deepspeed
            if not is_tar(resume_file):
                resume_file += '.tar'
            if not is_tar(final_checkpoint):
                final_checkpoint += '.tar'
        else:
            deepspeed_config = None
    
>       trainer_1 = self.get_trainer(
            save_folder=os.path.join(save_folder, 'first'),
            save_filename=save_filename,
            save_interval=save_interval,
            eval_interval=save_interval,
            deepspeed_config=deepspeed_config,
            seed=seed,
            device=device,
        )

/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:606: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:529: in get_trainer
    return Trainer(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.trainer.trainer.Trainer object at 0x7f67b10a4a90>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
                import deepspeed
            except ImportError as e:
>               raise MissingConditionalImportError(
                    extra_deps_group='deepspeed',
                    conda_package='deepspeed>=0.5.5',
                    conda_channel=None,
                ) from e
E               composer.utils.import_helpers.MissingConditionalImportError: Composer was installed without deepspeed support. To use deepspeed related packages, with Composer, run `pip install 'mosaicml[deepspeed]'` if using pip or `pip install deepspeed>=0.5.5` if using Anaconda.

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1161: MissingConditionalImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 42
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809299-vigorous-marten
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
_ TestCheckpointResumption.test_resumption[42-1ba-ba{batch}-rank{rank}.pt-ba5-rank{rank}.pt-ba8-rank{rank}.pt-deepspeed-zero1-1] _

self = <composer.trainer.trainer.Trainer object at 0x7f67b11a2530>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
>               import deepspeed
E               ModuleNotFoundError: No module named 'deepspeed'

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1159: ModuleNotFoundError

The above exception was the direct cause of the following exception:

self = <tests.trainer.test_checkpoint.TestCheckpointResumption object at 0x7f6921901900>
device = 'gpu', world_size = 1, deepspeed_zero_stage = 1, save_interval = '1ba'
save_filename = 'ba{batch}-rank{rank}.pt', resume_file = 'ba5-rank{rank}.pt.tar'
final_checkpoint = 'ba8-rank{rank}.pt.tar', seed = 42
tmp_path = PosixPath('/tmp/pytest-of-shie44167/pytest-3/test_resumption_42_1ba_ba_batc2')

    @pytest.mark.parametrize('world_size', [
        pytest.param(1),
        pytest.param(2, marks=pytest.mark.world_size(2)),
    ])
    @pytest.mark.parametrize('device,deepspeed_zero_stage', [
        pytest.param('cpu', None, id='cpu-ddp'),
        pytest.param('gpu', None, id='gpu-ddp', marks=pytest.mark.gpu),
        pytest.param('gpu', 0, id='deepspeed-zero0', marks=pytest.mark.gpu),
        pytest.param('gpu', 1, id='deepspeed-zero1', marks=pytest.mark.gpu),
        pytest.param('gpu', 2, id='deepspeed-zero2', marks=pytest.mark.gpu),
    ])
    @pytest.mark.parametrize(
        'seed,save_interval,save_filename,resume_file,final_checkpoint',
        [
            [None, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'latest-rank{rank}.pt'
            ],  # test randomized seed saving and symlinking
            [42, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'ep2-rank{rank}.pt'],  # test save at epoch end
            [42, '1ep', 'ep{epoch}-rank{rank}.tgz', 'ep1-rank{rank}.tgz', 'ep2-rank{rank}.tgz'
            ],  # test tarball with compression
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba4-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch in partial epoch
            [42, '1ba', 'ba{batch}-rank{rank}.pt', 'ba5-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch at epoch end
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba6-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch after complete epoch
        ],
    )
    def test_resumption(
        self,
        device: str,
        world_size: int,
        deepspeed_zero_stage: Optional[int],
        save_interval: str,
        save_filename: str,
        resume_file: str,
        final_checkpoint: str,
        seed: Optional[int],
        tmp_path: pathlib.Path,
    ):
    
        # all ranks use rank 0 folder
        tmp_paths = dist.all_gather_object(os.path.abspath(tmp_path))
        save_folder = pathlib.Path(tmp_paths[0])
    
        if deepspeed_zero_stage:
            deepspeed_config = {'zero_optimization': {'stage': deepspeed_zero_stage}}
    
            # save_checkpoint appends .tar for deepspeed
            if not is_tar(resume_file):
                resume_file += '.tar'
            if not is_tar(final_checkpoint):
                final_checkpoint += '.tar'
        else:
            deepspeed_config = None
    
>       trainer_1 = self.get_trainer(
            save_folder=os.path.join(save_folder, 'first'),
            save_filename=save_filename,
            save_interval=save_interval,
            eval_interval=save_interval,
            deepspeed_config=deepspeed_config,
            seed=seed,
            device=device,
        )

/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:606: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:529: in get_trainer
    return Trainer(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.trainer.trainer.Trainer object at 0x7f67b11a2530>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
                import deepspeed
            except ImportError as e:
>               raise MissingConditionalImportError(
                    extra_deps_group='deepspeed',
                    conda_package='deepspeed>=0.5.5',
                    conda_channel=None,
                ) from e
E               composer.utils.import_helpers.MissingConditionalImportError: Composer was installed without deepspeed support. To use deepspeed related packages, with Composer, run `pip install 'mosaicml[deepspeed]'` if using pip or `pip install deepspeed>=0.5.5` if using Anaconda.

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1161: MissingConditionalImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 42
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809333-jolly-kagu
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
_ TestCheckpointResumption.test_resumption[42-1ba-ba{batch}-rank{rank}.pt-ba5-rank{rank}.pt-ba8-rank{rank}.pt-deepspeed-zero2-1] _

self = <composer.trainer.trainer.Trainer object at 0x7f67b1a35a50>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
>               import deepspeed
E               ModuleNotFoundError: No module named 'deepspeed'

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1159: ModuleNotFoundError

The above exception was the direct cause of the following exception:

self = <tests.trainer.test_checkpoint.TestCheckpointResumption object at 0x7f6921901a20>
device = 'gpu', world_size = 1, deepspeed_zero_stage = 2, save_interval = '1ba'
save_filename = 'ba{batch}-rank{rank}.pt', resume_file = 'ba5-rank{rank}.pt.tar'
final_checkpoint = 'ba8-rank{rank}.pt.tar', seed = 42
tmp_path = PosixPath('/tmp/pytest-of-shie44167/pytest-3/test_resumption_42_1ba_ba_batc3')

    @pytest.mark.parametrize('world_size', [
        pytest.param(1),
        pytest.param(2, marks=pytest.mark.world_size(2)),
    ])
    @pytest.mark.parametrize('device,deepspeed_zero_stage', [
        pytest.param('cpu', None, id='cpu-ddp'),
        pytest.param('gpu', None, id='gpu-ddp', marks=pytest.mark.gpu),
        pytest.param('gpu', 0, id='deepspeed-zero0', marks=pytest.mark.gpu),
        pytest.param('gpu', 1, id='deepspeed-zero1', marks=pytest.mark.gpu),
        pytest.param('gpu', 2, id='deepspeed-zero2', marks=pytest.mark.gpu),
    ])
    @pytest.mark.parametrize(
        'seed,save_interval,save_filename,resume_file,final_checkpoint',
        [
            [None, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'latest-rank{rank}.pt'
            ],  # test randomized seed saving and symlinking
            [42, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'ep2-rank{rank}.pt'],  # test save at epoch end
            [42, '1ep', 'ep{epoch}-rank{rank}.tgz', 'ep1-rank{rank}.tgz', 'ep2-rank{rank}.tgz'
            ],  # test tarball with compression
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba4-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch in partial epoch
            [42, '1ba', 'ba{batch}-rank{rank}.pt', 'ba5-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch at epoch end
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba6-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch after complete epoch
        ],
    )
    def test_resumption(
        self,
        device: str,
        world_size: int,
        deepspeed_zero_stage: Optional[int],
        save_interval: str,
        save_filename: str,
        resume_file: str,
        final_checkpoint: str,
        seed: Optional[int],
        tmp_path: pathlib.Path,
    ):
    
        # all ranks use rank 0 folder
        tmp_paths = dist.all_gather_object(os.path.abspath(tmp_path))
        save_folder = pathlib.Path(tmp_paths[0])
    
        if deepspeed_zero_stage:
            deepspeed_config = {'zero_optimization': {'stage': deepspeed_zero_stage}}
    
            # save_checkpoint appends .tar for deepspeed
            if not is_tar(resume_file):
                resume_file += '.tar'
            if not is_tar(final_checkpoint):
                final_checkpoint += '.tar'
        else:
            deepspeed_config = None
    
>       trainer_1 = self.get_trainer(
            save_folder=os.path.join(save_folder, 'first'),
            save_filename=save_filename,
            save_interval=save_interval,
            eval_interval=save_interval,
            deepspeed_config=deepspeed_config,
            seed=seed,
            device=device,
        )

/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:606: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:529: in get_trainer
    return Trainer(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.trainer.trainer.Trainer object at 0x7f67b1a35a50>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
                import deepspeed
            except ImportError as e:
>               raise MissingConditionalImportError(
                    extra_deps_group='deepspeed',
                    conda_package='deepspeed>=0.5.5',
                    conda_channel=None,
                ) from e
E               composer.utils.import_helpers.MissingConditionalImportError: Composer was installed without deepspeed support. To use deepspeed related packages, with Composer, run `pip install 'mosaicml[deepspeed]'` if using pip or `pip install deepspeed>=0.5.5` if using Anaconda.

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1161: MissingConditionalImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 42
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809333-small-sunfish
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
_ TestCheckpointResumption.test_resumption[42-2ba-ba{batch}-rank{rank}.pt-ba6-rank{rank}.pt-ba8-rank{rank}.pt-deepspeed-zero1-1] _

self = <composer.trainer.trainer.Trainer object at 0x7f67b1189870>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
>               import deepspeed
E               ModuleNotFoundError: No module named 'deepspeed'

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1159: ModuleNotFoundError

The above exception was the direct cause of the following exception:

self = <tests.trainer.test_checkpoint.TestCheckpointResumption object at 0x7f6921901d80>
device = 'gpu', world_size = 1, deepspeed_zero_stage = 1, save_interval = '2ba'
save_filename = 'ba{batch}-rank{rank}.pt', resume_file = 'ba6-rank{rank}.pt.tar'
final_checkpoint = 'ba8-rank{rank}.pt.tar', seed = 42
tmp_path = PosixPath('/tmp/pytest-of-shie44167/pytest-3/test_resumption_42_2ba_ba_batc6')

    @pytest.mark.parametrize('world_size', [
        pytest.param(1),
        pytest.param(2, marks=pytest.mark.world_size(2)),
    ])
    @pytest.mark.parametrize('device,deepspeed_zero_stage', [
        pytest.param('cpu', None, id='cpu-ddp'),
        pytest.param('gpu', None, id='gpu-ddp', marks=pytest.mark.gpu),
        pytest.param('gpu', 0, id='deepspeed-zero0', marks=pytest.mark.gpu),
        pytest.param('gpu', 1, id='deepspeed-zero1', marks=pytest.mark.gpu),
        pytest.param('gpu', 2, id='deepspeed-zero2', marks=pytest.mark.gpu),
    ])
    @pytest.mark.parametrize(
        'seed,save_interval,save_filename,resume_file,final_checkpoint',
        [
            [None, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'latest-rank{rank}.pt'
            ],  # test randomized seed saving and symlinking
            [42, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'ep2-rank{rank}.pt'],  # test save at epoch end
            [42, '1ep', 'ep{epoch}-rank{rank}.tgz', 'ep1-rank{rank}.tgz', 'ep2-rank{rank}.tgz'
            ],  # test tarball with compression
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba4-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch in partial epoch
            [42, '1ba', 'ba{batch}-rank{rank}.pt', 'ba5-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch at epoch end
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba6-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch after complete epoch
        ],
    )
    def test_resumption(
        self,
        device: str,
        world_size: int,
        deepspeed_zero_stage: Optional[int],
        save_interval: str,
        save_filename: str,
        resume_file: str,
        final_checkpoint: str,
        seed: Optional[int],
        tmp_path: pathlib.Path,
    ):
    
        # all ranks use rank 0 folder
        tmp_paths = dist.all_gather_object(os.path.abspath(tmp_path))
        save_folder = pathlib.Path(tmp_paths[0])
    
        if deepspeed_zero_stage:
            deepspeed_config = {'zero_optimization': {'stage': deepspeed_zero_stage}}
    
            # save_checkpoint appends .tar for deepspeed
            if not is_tar(resume_file):
                resume_file += '.tar'
            if not is_tar(final_checkpoint):
                final_checkpoint += '.tar'
        else:
            deepspeed_config = None
    
>       trainer_1 = self.get_trainer(
            save_folder=os.path.join(save_folder, 'first'),
            save_filename=save_filename,
            save_interval=save_interval,
            eval_interval=save_interval,
            deepspeed_config=deepspeed_config,
            seed=seed,
            device=device,
        )

/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:606: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:529: in get_trainer
    return Trainer(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.trainer.trainer.Trainer object at 0x7f67b1189870>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
                import deepspeed
            except ImportError as e:
>               raise MissingConditionalImportError(
                    extra_deps_group='deepspeed',
                    conda_package='deepspeed>=0.5.5',
                    conda_channel=None,
                ) from e
E               composer.utils.import_helpers.MissingConditionalImportError: Composer was installed without deepspeed support. To use deepspeed related packages, with Composer, run `pip install 'mosaicml[deepspeed]'` if using pip or `pip install deepspeed>=0.5.5` if using Anaconda.

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1161: MissingConditionalImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 42
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809349-aboriginal-aardwolf
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
_ TestCheckpointResumption.test_resumption[42-2ba-ba{batch}-rank{rank}.pt-ba6-rank{rank}.pt-ba8-rank{rank}.pt-deepspeed-zero2-1] _

self = <composer.trainer.trainer.Trainer object at 0x7f67b0f02650>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
>               import deepspeed
E               ModuleNotFoundError: No module named 'deepspeed'

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1159: ModuleNotFoundError

The above exception was the direct cause of the following exception:

self = <tests.trainer.test_checkpoint.TestCheckpointResumption object at 0x7f6921901ea0>
device = 'gpu', world_size = 1, deepspeed_zero_stage = 2, save_interval = '2ba'
save_filename = 'ba{batch}-rank{rank}.pt', resume_file = 'ba6-rank{rank}.pt.tar'
final_checkpoint = 'ba8-rank{rank}.pt.tar', seed = 42
tmp_path = PosixPath('/tmp/pytest-of-shie44167/pytest-3/test_resumption_42_2ba_ba_batc7')

    @pytest.mark.parametrize('world_size', [
        pytest.param(1),
        pytest.param(2, marks=pytest.mark.world_size(2)),
    ])
    @pytest.mark.parametrize('device,deepspeed_zero_stage', [
        pytest.param('cpu', None, id='cpu-ddp'),
        pytest.param('gpu', None, id='gpu-ddp', marks=pytest.mark.gpu),
        pytest.param('gpu', 0, id='deepspeed-zero0', marks=pytest.mark.gpu),
        pytest.param('gpu', 1, id='deepspeed-zero1', marks=pytest.mark.gpu),
        pytest.param('gpu', 2, id='deepspeed-zero2', marks=pytest.mark.gpu),
    ])
    @pytest.mark.parametrize(
        'seed,save_interval,save_filename,resume_file,final_checkpoint',
        [
            [None, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'latest-rank{rank}.pt'
            ],  # test randomized seed saving and symlinking
            [42, '1ep', 'ep{epoch}-rank{rank}.pt', 'ep1-rank{rank}.pt', 'ep2-rank{rank}.pt'],  # test save at epoch end
            [42, '1ep', 'ep{epoch}-rank{rank}.tgz', 'ep1-rank{rank}.tgz', 'ep2-rank{rank}.tgz'
            ],  # test tarball with compression
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba4-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch in partial epoch
            [42, '1ba', 'ba{batch}-rank{rank}.pt', 'ba5-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch at epoch end
            [42, '2ba', 'ba{batch}-rank{rank}.pt', 'ba6-rank{rank}.pt', 'ba8-rank{rank}.pt'
            ],  # test save batch after complete epoch
        ],
    )
    def test_resumption(
        self,
        device: str,
        world_size: int,
        deepspeed_zero_stage: Optional[int],
        save_interval: str,
        save_filename: str,
        resume_file: str,
        final_checkpoint: str,
        seed: Optional[int],
        tmp_path: pathlib.Path,
    ):
    
        # all ranks use rank 0 folder
        tmp_paths = dist.all_gather_object(os.path.abspath(tmp_path))
        save_folder = pathlib.Path(tmp_paths[0])
    
        if deepspeed_zero_stage:
            deepspeed_config = {'zero_optimization': {'stage': deepspeed_zero_stage}}
    
            # save_checkpoint appends .tar for deepspeed
            if not is_tar(resume_file):
                resume_file += '.tar'
            if not is_tar(final_checkpoint):
                final_checkpoint += '.tar'
        else:
            deepspeed_config = None
    
>       trainer_1 = self.get_trainer(
            save_folder=os.path.join(save_folder, 'first'),
            save_filename=save_filename,
            save_interval=save_interval,
            eval_interval=save_interval,
            deepspeed_config=deepspeed_config,
            seed=seed,
            device=device,
        )

/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:606: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:529: in get_trainer
    return Trainer(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.trainer.trainer.Trainer object at 0x7f67b0f02650>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
                import deepspeed
            except ImportError as e:
>               raise MissingConditionalImportError(
                    extra_deps_group='deepspeed',
                    conda_package='deepspeed>=0.5.5',
                    conda_channel=None,
                ) from e
E               composer.utils.import_helpers.MissingConditionalImportError: Composer was installed without deepspeed support. To use deepspeed related packages, with Composer, run `pip install 'mosaicml[deepspeed]'` if using pip or `pip install deepspeed>=0.5.5` if using Anaconda.

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1161: MissingConditionalImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 42
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809349-abstract-jacamar
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
__________________ test_rotate_checkpoints[deepspeed-zero0-1] __________________

self = <composer.trainer.trainer.Trainer object at 0x7f67b11100a0>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
>               import deepspeed
E               ModuleNotFoundError: No module named 'deepspeed'

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1159: ModuleNotFoundError

The above exception was the direct cause of the following exception:

world_size = 1, device = 'gpu', deepspeed_enabled = True, zero_stage = 0
tmp_path = PosixPath('/tmp/pytest-of-shie44167/pytest-3/test_rotate_checkpoints_deepsp0')

    @pytest.mark.parametrize('world_size', [
        pytest.param(1),
        pytest.param(2, marks=pytest.mark.world_size(2)),
    ])
    @pytest.mark.parametrize('device,deepspeed_enabled,zero_stage', [
        pytest.param('cpu', False, None, id='cpu-ddp'),
        pytest.param('gpu', False, None, id='gpu-ddp', marks=pytest.mark.gpu),
        pytest.param('gpu', True, 0, id='deepspeed-zero0', marks=pytest.mark.gpu),
        pytest.param('gpu', True, 1, id='deepspeed-zero1', marks=pytest.mark.gpu),
        pytest.param('gpu', True, 2, id='deepspeed-zero2', marks=pytest.mark.gpu),
    ])
    def test_rotate_checkpoints(
        world_size,
        device,
        deepspeed_enabled,
        zero_stage,
        tmp_path: pathlib.Path,
    ):
        num_keep = 5
    
        # all ranks use rank 0 folder
        tmp_paths = dist.all_gather_object(os.path.abspath(tmp_path))
        save_folder = tmp_paths[0]
    
        deepseed_config = None
        if deepspeed_enabled:
            deepseed_config = {'zero_optimization': {'stage': zero_stage}}
    
        train_dataset = RandomImageDataset()
    
>       trainer = Trainer(
            model=SimpleConvModel(),
            train_dataloader=DataLoader(
                dataset=train_dataset,
                sampler=dist.get_sampler(train_dataset),
            ),
            save_folder=str(save_folder),
            save_filename='checkpoint_{rank}_{batch}.pt',
            save_interval='1ba',
            max_duration='10ba',
            save_num_checkpoints_to_keep=num_keep,
            device=device,
            deepspeed_config=deepseed_config,
        )

/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:732: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.trainer.trainer.Trainer object at 0x7f67b11100a0>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
                import deepspeed
            except ImportError as e:
>               raise MissingConditionalImportError(
                    extra_deps_group='deepspeed',
                    conda_package='deepspeed>=0.5.5',
                    conda_channel=None,
                ) from e
E               composer.utils.import_helpers.MissingConditionalImportError: Composer was installed without deepspeed support. To use deepspeed related packages, with Composer, run `pip install 'mosaicml[deepspeed]'` if using pip or `pip install deepspeed>=0.5.5` if using Anaconda.

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1161: MissingConditionalImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809360-violet-dog
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
__________________ test_rotate_checkpoints[deepspeed-zero1-1] __________________

self = <composer.trainer.trainer.Trainer object at 0x7f67b119af50>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
>               import deepspeed
E               ModuleNotFoundError: No module named 'deepspeed'

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1159: ModuleNotFoundError

The above exception was the direct cause of the following exception:

world_size = 1, device = 'gpu', deepspeed_enabled = True, zero_stage = 1
tmp_path = PosixPath('/tmp/pytest-of-shie44167/pytest-3/test_rotate_checkpoints_deepsp1')

    @pytest.mark.parametrize('world_size', [
        pytest.param(1),
        pytest.param(2, marks=pytest.mark.world_size(2)),
    ])
    @pytest.mark.parametrize('device,deepspeed_enabled,zero_stage', [
        pytest.param('cpu', False, None, id='cpu-ddp'),
        pytest.param('gpu', False, None, id='gpu-ddp', marks=pytest.mark.gpu),
        pytest.param('gpu', True, 0, id='deepspeed-zero0', marks=pytest.mark.gpu),
        pytest.param('gpu', True, 1, id='deepspeed-zero1', marks=pytest.mark.gpu),
        pytest.param('gpu', True, 2, id='deepspeed-zero2', marks=pytest.mark.gpu),
    ])
    def test_rotate_checkpoints(
        world_size,
        device,
        deepspeed_enabled,
        zero_stage,
        tmp_path: pathlib.Path,
    ):
        num_keep = 5
    
        # all ranks use rank 0 folder
        tmp_paths = dist.all_gather_object(os.path.abspath(tmp_path))
        save_folder = tmp_paths[0]
    
        deepseed_config = None
        if deepspeed_enabled:
            deepseed_config = {'zero_optimization': {'stage': zero_stage}}
    
        train_dataset = RandomImageDataset()
    
>       trainer = Trainer(
            model=SimpleConvModel(),
            train_dataloader=DataLoader(
                dataset=train_dataset,
                sampler=dist.get_sampler(train_dataset),
            ),
            save_folder=str(save_folder),
            save_filename='checkpoint_{rank}_{batch}.pt',
            save_interval='1ba',
            max_duration='10ba',
            save_num_checkpoints_to_keep=num_keep,
            device=device,
            deepspeed_config=deepseed_config,
        )

/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:732: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.trainer.trainer.Trainer object at 0x7f67b119af50>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
                import deepspeed
            except ImportError as e:
>               raise MissingConditionalImportError(
                    extra_deps_group='deepspeed',
                    conda_package='deepspeed>=0.5.5',
                    conda_channel=None,
                ) from e
E               composer.utils.import_helpers.MissingConditionalImportError: Composer was installed without deepspeed support. To use deepspeed related packages, with Composer, run `pip install 'mosaicml[deepspeed]'` if using pip or `pip install deepspeed>=0.5.5` if using Anaconda.

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1161: MissingConditionalImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809360-chocolate-wolf
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
__________________ test_rotate_checkpoints[deepspeed-zero2-1] __________________

self = <composer.trainer.trainer.Trainer object at 0x7f67b118b580>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
>               import deepspeed
E               ModuleNotFoundError: No module named 'deepspeed'

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1159: ModuleNotFoundError

The above exception was the direct cause of the following exception:

world_size = 1, device = 'gpu', deepspeed_enabled = True, zero_stage = 2
tmp_path = PosixPath('/tmp/pytest-of-shie44167/pytest-3/test_rotate_checkpoints_deepsp2')

    @pytest.mark.parametrize('world_size', [
        pytest.param(1),
        pytest.param(2, marks=pytest.mark.world_size(2)),
    ])
    @pytest.mark.parametrize('device,deepspeed_enabled,zero_stage', [
        pytest.param('cpu', False, None, id='cpu-ddp'),
        pytest.param('gpu', False, None, id='gpu-ddp', marks=pytest.mark.gpu),
        pytest.param('gpu', True, 0, id='deepspeed-zero0', marks=pytest.mark.gpu),
        pytest.param('gpu', True, 1, id='deepspeed-zero1', marks=pytest.mark.gpu),
        pytest.param('gpu', True, 2, id='deepspeed-zero2', marks=pytest.mark.gpu),
    ])
    def test_rotate_checkpoints(
        world_size,
        device,
        deepspeed_enabled,
        zero_stage,
        tmp_path: pathlib.Path,
    ):
        num_keep = 5
    
        # all ranks use rank 0 folder
        tmp_paths = dist.all_gather_object(os.path.abspath(tmp_path))
        save_folder = tmp_paths[0]
    
        deepseed_config = None
        if deepspeed_enabled:
            deepseed_config = {'zero_optimization': {'stage': zero_stage}}
    
        train_dataset = RandomImageDataset()
    
>       trainer = Trainer(
            model=SimpleConvModel(),
            train_dataloader=DataLoader(
                dataset=train_dataset,
                sampler=dist.get_sampler(train_dataset),
            ),
            save_folder=str(save_folder),
            save_filename='checkpoint_{rank}_{batch}.pt',
            save_interval='1ba',
            max_duration='10ba',
            save_num_checkpoints_to_keep=num_keep,
            device=device,
            deepspeed_config=deepseed_config,
        )

/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_checkpoint.py:732: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.trainer.trainer.Trainer object at 0x7f67b118b580>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
                import deepspeed
            except ImportError as e:
>               raise MissingConditionalImportError(
                    extra_deps_group='deepspeed',
                    conda_package='deepspeed>=0.5.5',
                    conda_channel=None,
                ) from e
E               composer.utils.import_helpers.MissingConditionalImportError: Composer was installed without deepspeed support. To use deepspeed related packages, with Composer, run `pip install 'mosaicml[deepspeed]'` if using pip or `pip install deepspeed>=0.5.5` if using Anaconda.

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1161: MissingConditionalImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809361-sticky-kudu
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
--------------------------- Captured stderr teardown ---------------------------






____________________________ test_ddp[1-deepspeed] _____________________________

self = <composer.trainer.trainer.Trainer object at 0x7f67b0f422f0>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
>               import deepspeed
E               ModuleNotFoundError: No module named 'deepspeed'

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1159: ModuleNotFoundError

The above exception was the direct cause of the following exception:

device = 'gpu', world_size = 1, deepspeed = True, fsdp = False
tmp_path = PosixPath('/tmp/pytest-of-shie44167/pytest-3/test_ddp_1_deepspeed_0')

    @pytest.mark.parametrize('device,deepspeed,fsdp', [
        pytest.param('cpu', False, False, id='cpu'),
        pytest.param('gpu', False, False, id='gpu', marks=pytest.mark.gpu),
        pytest.param('gpu', True, False, id='deepspeed', marks=pytest.mark.gpu),
        pytest.param('gpu',
                     False,
                     True,
                     id='fsdp',
                     marks=[
                         pytest.mark.gpu,
                         pytest.mark.skipif(version.parse(torch.__version__) < version.parse('1.12.0'),
                                            reason='requires PyTorch 1.12 or higher')
                     ]),
    ])
    @pytest.mark.parametrize('world_size', [
        pytest.param(1),
        pytest.param(2, marks=pytest.mark.world_size(2)),
    ])
    def test_ddp(device: str, world_size: int, deepspeed: bool, fsdp: bool, tmp_path: pathlib.Path) -> None:
        """test strategy for ddp: 1) Train a dummy model on two gps, for two epochs, using the tracked dataset. 2) The
        tracked dataset should record two -- and only two -- accesses for each sample -- one for each epoch If each sample
        is accessed more than this number of times, then the distributed sampler isn't working properly If each sample is
        accessed less than this number of times, then either the sample pool size isn't a multiple of the batch size (and
        samples are getting dropped), or not all processes are working 3) We use a callback to save the (x, y) for the first
        batch in each epoch on each process.
    
         ({train, eval} * {epoch 1, epoch 2} * {ddp 1, ddp2})
        We assert that each of these tensors are different to ensure that 1) random seeding works properly,
        and 2) each ddp process is indeed getting different data.
        """
    
        model = SimpleModel(num_classes=100)
    
        train_batch_size = 10
        train_subset_num_batches = 3
    
        synthetic_dataset = SyntheticBatchPairDataset(
            num_unique_samples_to_create=train_batch_size * train_subset_num_batches,
            total_dataset_size=10_000,
            data_shape=(model.num_features, 5, 5),
            num_classes=model.num_classes,
        )
        train_dataset = TrackedDataset(
            synthetic_dataset=synthetic_dataset,
            is_train=True,
            tmp_path=tmp_path,
        )
    
        train_dataloader = DataLoader(
            dataset=train_dataset,
            num_workers=0,
            prefetch_factor=2,
            persistent_workers=False,
            pin_memory=False,
            timeout=0.0,
            batch_size=train_batch_size // dist.get_world_size(),
            sampler=dist.get_sampler(
                train_dataset,
                drop_last=False,
                shuffle=True,
            ),
        )
    
        eval_batch_size = 10
        eval_subset_num_batches = 3
    
        eval_dataset = SyntheticBatchPairDataset(
            num_unique_samples_to_create=eval_batch_size * eval_subset_num_batches,
            total_dataset_size=10_000,
            data_shape=(model.num_features, 5, 5),
            num_classes=model.num_classes,
        )
        eval_dataset = TrackedDataset(
            synthetic_dataset=eval_dataset,
            is_train=False,
            tmp_path=tmp_path,
        )
    
        eval_dataloader = DataLoader(
            dataset=eval_dataset,
            batch_size=eval_batch_size // dist.get_world_size(),
            sampler=dist.get_sampler(
                eval_dataset,
                drop_last=False,
                shuffle=True,
            ),
        )
    
        fsdp_config = None
        if fsdp:
            fsdp_config = {
                'sharding_strategy': 'FULL_SHARD',
                'min_params': 1e8,
                'cpu_offload': False,
                'mixed_precision': 'DEFAULT',
                'backward_prefetch': 'BACKWARD_PRE',
                'activation_checkpointing': False,
                'activation_cpu_offload': False,
                'verbose': False
            }
    
        max_epochs = 2
>       trainer = Trainer(model=model,
                          train_dataloader=train_dataloader,
                          eval_dataloader=eval_dataloader,
                          device=device,
                          max_duration=f'{max_epochs}ep',
                          eval_interval='1ep',
                          eval_subset_num_batches=eval_subset_num_batches,
                          train_subset_num_batches=train_subset_num_batches,
                          deepspeed_config={} if deepspeed else None,
                          fsdp_config=fsdp_config,
                          callbacks=[CheckBatch0(tmp_path)])

/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_ddp.py:186: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.trainer.trainer.Trainer object at 0x7f67b0f422f0>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
                import deepspeed
            except ImportError as e:
>               raise MissingConditionalImportError(
                    extra_deps_group='deepspeed',
                    conda_package='deepspeed>=0.5.5',
                    conda_channel=None,
                ) from e
E               composer.utils.import_helpers.MissingConditionalImportError: Composer was installed without deepspeed support. To use deepspeed related packages, with Composer, run `pip install 'mosaicml[deepspeed]'` if using pip or `pip install deepspeed>=0.5.5` if using Anaconda.

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1161: MissingConditionalImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809361-graceful-horse
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
______________ TestTrainerInitOrFit.test_deepspeed[Precision.AMP] ______________

self = <composer.trainer.trainer.Trainer object at 0x7f67b1519ff0>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
>               import deepspeed
E               ModuleNotFoundError: No module named 'deepspeed'

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1159: ModuleNotFoundError

The above exception was the direct cause of the following exception:

self = <tests.trainer.test_trainer.TestTrainerInitOrFit object at 0x7f6921784be0>
model = SimpleModel(
  (train_metrics): Accuracy()
  (val_metrics): MetricCollection(
    (CrossEntropy): CrossEntropy()
    (...)
  (fc1): Linear(in_features=1, out_features=5, bias=True)
  (fc2): Linear(in_features=5, out_features=2, bias=True)
)
precision = <Precision.AMP: 'amp'>, max_duration = Time(1, TimeUnit.EPOCH)
train_dataloader = <torch.utils.data.dataloader.DataLoader object at 0x7f67b1518e50>

    @pytest.mark.gpu
    @pytest.mark.parametrize('precision', list(Precision))
    def test_deepspeed(
        self,
        model: ComposerModel,
        precision: Precision,
        max_duration: Time[int],
        train_dataloader: DataLoader,
    ):
>       trainer = Trainer(
            model=model,
            precision=precision,
            deepspeed_config={},
            max_duration=max_duration,
            train_dataloader=train_dataloader,
        )

/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_trainer.py:398: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.trainer.trainer.Trainer object at 0x7f67b1519ff0>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
                import deepspeed
            except ImportError as e:
>               raise MissingConditionalImportError(
                    extra_deps_group='deepspeed',
                    conda_package='deepspeed>=0.5.5',
                    conda_channel=None,
                ) from e
E               composer.utils.import_helpers.MissingConditionalImportError: Composer was installed without deepspeed support. To use deepspeed related packages, with Composer, run `pip install 'mosaicml[deepspeed]'` if using pip or `pip install deepspeed>=0.5.5` if using Anaconda.

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1161: MissingConditionalImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809361-curvy-chamois
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
_____________ TestTrainerInitOrFit.test_deepspeed[Precision.FP16] ______________

self = <composer.trainer.trainer.Trainer object at 0x7f67b0de0fd0>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
>               import deepspeed
E               ModuleNotFoundError: No module named 'deepspeed'

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1159: ModuleNotFoundError

The above exception was the direct cause of the following exception:

self = <tests.trainer.test_trainer.TestTrainerInitOrFit object at 0x7f6921784c70>
model = SimpleModel(
  (train_metrics): Accuracy()
  (val_metrics): MetricCollection(
    (CrossEntropy): CrossEntropy()
    (...)
  (fc1): Linear(in_features=1, out_features=5, bias=True)
  (fc2): Linear(in_features=5, out_features=2, bias=True)
)
precision = <Precision.FP16: 'fp16'>, max_duration = Time(1, TimeUnit.EPOCH)
train_dataloader = <torch.utils.data.dataloader.DataLoader object at 0x7f67b0de10c0>

    @pytest.mark.gpu
    @pytest.mark.parametrize('precision', list(Precision))
    def test_deepspeed(
        self,
        model: ComposerModel,
        precision: Precision,
        max_duration: Time[int],
        train_dataloader: DataLoader,
    ):
>       trainer = Trainer(
            model=model,
            precision=precision,
            deepspeed_config={},
            max_duration=max_duration,
            train_dataloader=train_dataloader,
        )

/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_trainer.py:398: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.trainer.trainer.Trainer object at 0x7f67b0de0fd0>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
                import deepspeed
            except ImportError as e:
>               raise MissingConditionalImportError(
                    extra_deps_group='deepspeed',
                    conda_package='deepspeed>=0.5.5',
                    conda_channel=None,
                ) from e
E               composer.utils.import_helpers.MissingConditionalImportError: Composer was installed without deepspeed support. To use deepspeed related packages, with Composer, run `pip install 'mosaicml[deepspeed]'` if using pip or `pip install deepspeed>=0.5.5` if using Anaconda.

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1161: MissingConditionalImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809361-fearless-antelope
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
_____________ TestTrainerInitOrFit.test_deepspeed[Precision.FP32] ______________

self = <composer.trainer.trainer.Trainer object at 0x7f67b0cbac20>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
>               import deepspeed
E               ModuleNotFoundError: No module named 'deepspeed'

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1159: ModuleNotFoundError

The above exception was the direct cause of the following exception:

self = <tests.trainer.test_trainer.TestTrainerInitOrFit object at 0x7f6921784d00>
model = SimpleModel(
  (train_metrics): Accuracy()
  (val_metrics): MetricCollection(
    (CrossEntropy): CrossEntropy()
    (...)
  (fc1): Linear(in_features=1, out_features=5, bias=True)
  (fc2): Linear(in_features=5, out_features=2, bias=True)
)
precision = <Precision.FP32: 'fp32'>, max_duration = Time(1, TimeUnit.EPOCH)
train_dataloader = <torch.utils.data.dataloader.DataLoader object at 0x7f67b0cba7d0>

    @pytest.mark.gpu
    @pytest.mark.parametrize('precision', list(Precision))
    def test_deepspeed(
        self,
        model: ComposerModel,
        precision: Precision,
        max_duration: Time[int],
        train_dataloader: DataLoader,
    ):
>       trainer = Trainer(
            model=model,
            precision=precision,
            deepspeed_config={},
            max_duration=max_duration,
            train_dataloader=train_dataloader,
        )

/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_trainer.py:398: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.trainer.trainer.Trainer object at 0x7f67b0cbac20>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
                import deepspeed
            except ImportError as e:
>               raise MissingConditionalImportError(
                    extra_deps_group='deepspeed',
                    conda_package='deepspeed>=0.5.5',
                    conda_channel=None,
                ) from e
E               composer.utils.import_helpers.MissingConditionalImportError: Composer was installed without deepspeed support. To use deepspeed related packages, with Composer, run `pip install 'mosaicml[deepspeed]'` if using pip or `pip install deepspeed>=0.5.5` if using Anaconda.

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1161: MissingConditionalImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809361-crafty-wildebeest
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
_____________ TestTrainerInitOrFit.test_deepspeed[Precision.BF16] ______________

self = <composer.trainer.trainer.Trainer object at 0x7f67b0d07400>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
>               import deepspeed
E               ModuleNotFoundError: No module named 'deepspeed'

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1159: ModuleNotFoundError

The above exception was the direct cause of the following exception:

self = <tests.trainer.test_trainer.TestTrainerInitOrFit object at 0x7f6921784d90>
model = SimpleModel(
  (train_metrics): Accuracy()
  (val_metrics): MetricCollection(
    (CrossEntropy): CrossEntropy()
    (...)
  (fc1): Linear(in_features=1, out_features=5, bias=True)
  (fc2): Linear(in_features=5, out_features=2, bias=True)
)
precision = <Precision.BF16: 'bf16'>, max_duration = Time(1, TimeUnit.EPOCH)
train_dataloader = <torch.utils.data.dataloader.DataLoader object at 0x7f67b0d072b0>

    @pytest.mark.gpu
    @pytest.mark.parametrize('precision', list(Precision))
    def test_deepspeed(
        self,
        model: ComposerModel,
        precision: Precision,
        max_duration: Time[int],
        train_dataloader: DataLoader,
    ):
>       trainer = Trainer(
            model=model,
            precision=precision,
            deepspeed_config={},
            max_duration=max_duration,
            train_dataloader=train_dataloader,
        )

/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_trainer.py:398: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <composer.trainer.trainer.Trainer object at 0x7f67b0d07400>

    def __init__(
        self,
        *,
        # The Model
        model: ComposerModel,
    
        # Train Dataloader
        train_dataloader: Optional[Union[Iterable, DataSpec, Dict[str, Any]]] = None,
        train_dataloader_label: str = 'train',
        train_subset_num_batches: int = -1,
    
        # Stopping Condition
        max_duration: Optional[Union[int, str, Time]] = None,
    
        # Algorithms
        algorithms: Optional[Union[Algorithm, Sequence[Algorithm]]] = None,
    
        # Engine Pass Registration
        algorithm_passes: Optional[Union[AlgorithmPass, Tuple[AlgorithmPass, int],
                                         Sequence[Union[AlgorithmPass, Tuple[AlgorithmPass, int]]]]] = None,
    
        # Optimizers and Scheduling
        optimizers: Optional[torch.optim.Optimizer] = None,
        schedulers: Optional[Union[ComposerScheduler, PyTorchScheduler, Sequence[Union[ComposerScheduler,
                                                                                       PyTorchScheduler]]]] = None,
        scale_schedule_ratio: float = 1.0,
        step_schedulers_every_batch: Optional[bool] = None,
    
        # Evaluators
        eval_dataloader: Optional[Union[Iterable, DataSpec, Evaluator, Sequence[Evaluator]]] = None,
        eval_interval: Union[int, str, Time, Callable[[State, Event], bool]] = 1,
        eval_subset_num_batches: int = -1,
    
        # Callbacks and Logging
        callbacks: Optional[Union[Callback, Sequence[Callback]]] = None,
        loggers: Optional[Union[LoggerDestination, Sequence[LoggerDestination]]] = None,
        run_name: Optional[str] = None,
        progress_bar: bool = True,
        log_to_console: bool = False,
        console_stream: Union[str, TextIO] = 'stderr',
        console_log_interval: Union[int, str, Time] = '1ep',
        log_traces: bool = False,
    
        # Load Checkpoint
        load_path: Optional[str] = None,
        load_object_store: Optional[Union[ObjectStore, LoggerDestination]] = None,
        load_weights_only: bool = False,
        load_strict_model_weights: bool = False,
        load_progress_bar: bool = True,
        load_ignore_keys: Optional[Union[List[str], Callable[[Dict], None]]] = None,
        load_exclude_algorithms: Optional[List[str]] = None,
    
        # Save Checkpoint
        save_folder: Optional[str] = None,
        save_filename: str = 'ep{epoch}-ba{batch}-rank{rank}.pt',
        save_latest_filename: Optional[str] = 'latest-rank{rank}.pt',
        save_overwrite: bool = False,
        save_interval: Union[str, int, Time, Callable[[State, Event], bool]] = '1ep',
        save_weights_only: bool = False,
        save_num_checkpoints_to_keep: int = -1,
    
        # Graceful Resumption
        autoresume: bool = False,
    
        # DeepSpeed
        deepspeed_config: Optional[Dict[str, Any]] = None,
        fsdp_config: Optional[Dict[str, Any]] = None,
    
        # System/Numerics
        device: Optional[Union[str, Device]] = None,
        precision: Optional[Union[str, Precision]] = None,
        grad_accum: Union[int, str] = 1,
    
        # Reproducibility
        seed: Optional[int] = None,
        deterministic_mode: bool = False,
    
        # Distributed Training
        dist_timeout: float = 1800.0,
        ddp_sync_strategy: Optional[Union[str, DDPSyncStrategy]] = None,
    
        # Grad Clip Norm
        grad_clip_norm: float = -1.0,
    
        # Profiling
        profiler: Optional[Profiler] = None,
    
        # Python logging
        python_log_level: Optional[str] = None,
    ):
    
        self.python_log_level = python_log_level
        if self.python_log_level is not None:
            logging.basicConfig(
                # Example of format string
                # 2022-06-29 11:22:26,152: rank0[822018][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.FP32
                # Including the PID and thread name to help with debugging dataloader workers and callbacks that spawn background
                # threads / processes
                format=
                f'%(asctime)s: rank{dist.get_global_rank()}[%(process)d][%(threadName)s]: %(levelname)s: %(name)s: %(message)s'
            )
            logging.getLogger('composer').setLevel(self.python_log_level.upper())
    
        algorithms = list(ensure_tuple(algorithms))
    
        # Device
        self._device = get_device(device)
    
        # Determine whether DeepSpeed and FSDP are enabled
        self.deepspeed_config = deepspeed_config
        self.fsdp_config = fsdp_config
        self.deepspeed_enabled = self.deepspeed_config is not None
        self.fsdp_enabled = self.fsdp_config is not None
    
        # Precision
        if precision is None:
            precision = Precision.AMP if isinstance(self._device, DeviceGPU) else Precision.FP32
        if isinstance(precision, str):
            precision = Precision(precision)
        _validate_precision(precision, self._device, self.deepspeed_enabled)
    
        # Distributed
        if self.deepspeed_enabled or self.fsdp_enabled or dist.get_world_size() > 1:
            # Deepspeed and FSDP both require torch.distributed to be initialized, even if the world size is 1
            # And torch.distributed is always required for multi-rank training
            dist.initialize_dist(self._device, dist_timeout)
    
        # Handle FSDP sharding
        if self.fsdp_config is not None:
            prepare_fsdp_module(model, optimizers, self.fsdp_config, precision)
    
        # Reproducibility
        rank_zero_seed, seed = _distribute_and_get_random_seed(seed, self._device)
        # If hparams is used to create the Trainer this function is called twice
        # which is okay because all runs with the hparams codepath will do this
        reproducibility.seed_all(seed)
        if deterministic_mode:
            reproducibility.configure_deterministic_mode()
    
        # Optimizers and Schedulers
        if not optimizers:
            optimizers = DecoupledSGDW(model.parameters(), lr=0.1)
            # hard-coding the optimizer in the warning, as repr(optimizers) would print an annoying, multi-line warning
            warnings.warn(('No optimizer was specified. Defaulting to '
                           f"{type(optimizers).__name__}(lr={optimizers.defaults['lr']})"))
    
        num_optimizers = len(ensure_tuple(optimizers))
        if num_optimizers != 1:
            raise NotImplementedError(f'Only one optimizer is supported; found {num_optimizers} optimizers')
    
        # Move the model and optimizers to the device
    
        if not (self.deepspeed_enabled or self.fsdp_enabled):
            # check if model is already on tpu
            if isinstance(self._device, DeviceTPU) and 'xla' not in str(next(model.parameters()).device):
                raise ValueError(
                    'Use model.to(xm.xla_device()) to set the model to the TPU before providing to the trainer.')
            else:
                model = self._device.module_to_device(model)
                # Move any remaining optimizer parameters onto the device
                # It is possible that optimizer initialize created some internal tensors on CPU
                # that need to be moved onto GPU.
            optimizers = map_collection(optimizers, self._device.optimizer_to_device)
    
        # Grad Accum
        auto_grad_accum = _is_auto_grad_accum(grad_accum, device=self._device)
        if auto_grad_accum and profiler:
            raise ValueError("`grad_accum='auto'` is not compatible with the profiler. It is recommended to run "
                             "a mini-run with `grad_accum='auto'` to identify the optimal grad_accum value and "
                             'then manually specify that in a second run with profiler.')
        grad_accum = _get_initial_grad_accum(grad_accum)
        eval_batch_split = 1
    
        # Grad Clip Norm
        if grad_clip_norm > 0:
    
            warnings.warn(
                DeprecationWarning((f"Using the 'grad_clip_norm' field in Trainer is deprecated. Please use"
                                    'the GradientClipping Algorithm in composer.algorithms.gradient_clipping.')))
    
            if any(isinstance(alg, GradientClipping) for alg in algorithms):
                warnings.warn(
                    UserWarning(
                        f'The GradientClipping algorithm is already specified. Ignoring grad_clip_norm={grad_clip_norm}'
                    ))
            else:
                algorithms.append(GradientClipping(clipping_type='norm', clipping_threshold=grad_clip_norm))
    
        # Run Name
        if run_name is None:
            if autoresume:
                raise ValueError('When autoresume=True, the `run_name` must be specified.')
            run_name = _generate_run_name()
        log.info('Run name: %s', run_name)
    
        # Create the State
        self.state = State(
            rank_zero_seed=rank_zero_seed,
            algorithms=algorithms,
            model=model,
            callbacks=callbacks,
            grad_accum=grad_accum,
            auto_grad_accum=auto_grad_accum,
            eval_batch_split=eval_batch_split,
            precision=precision,
            optimizers=optimizers,
            run_name=run_name,
            deepspeed_config=deepspeed_config,
        )
    
        # Profiler
        if profiler is not None:
            warnings.warn('The profiler is enabled. Using the profiler adds additional overhead when training.')
            self.state.profiler = profiler
            self.state.profiler.bind_to_state(self.state)
    
        # Console Logging
        loggers = list(ensure_tuple(loggers))
    
        if progress_bar and log_to_console:
            warnings.warn(
                'Setting both `progress_bar` and `log_to_console` both to True is not recommended and will'
                'lead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.'
            )
    
        if any(isinstance(x, ProgressBarLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning(
                    (f'Specifying the {ProgressBarLogger.__name__} via `loggers` is deprecated. Instead, '
                     'please specify `progress_bar`, `console_stream` and `log_traces` arguments when '
                     'constructing the trainer. If specified, these arguments will be ignored, as the '
                     f'{ProgressBarLogger.__name__} was already created.')))
        else:
            if progress_bar:
                loggers.append(ProgressBarLogger(stream=console_stream, log_traces=log_traces))
    
        # Console Logging
        if any(isinstance(x, ConsoleLogger) for x in loggers):
            warnings.warn(
                DeprecationWarning((
                    f'Specifying the {ConsoleLogger.__name__} via `loggers` is deprecated. Instead, '
                    'please specify `log_to_console`, `console_stream`, `console_log_interval`, and `log_traces` arguments when '
                    'constructing the trainer. If specified, these arguments will be ignored, as the '
                    f'{ConsoleLogger.__name__} was already created.')))
        else:
            if log_to_console:
                loggers.append(
                    ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))
    
        if save_folder is not None:
            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)
            if remote_ud is not None:
                loggers.append(remote_ud)
    
        # Logger
        self.logger = Logger(state=self.state, destinations=loggers)
    
        if save_latest_filename is not None:
            remote_ud_has_format_string = [
                isinstance(logger_destination, RemoteUploaderDownloader) and
                logger_destination.file_path_format_string != '{remote_file_name}'
                for logger_destination in self.logger.destinations
            ]
            if any(remote_ud_has_format_string):
                raise ValueError(
                    'Specifying a `file_path_format_string` to a `RemoteUploaderDownloader` is not currently supported while using `save_latest_filename`. '
                    'Please specify the path formatting via `save_folder`, `save_filename`, and `save_latest_filename`')
    
        # Callbacks
        self.state.callbacks[:] = list(cast(List[Callback], loggers)) + self.state.callbacks
    
        # Checkpoint Saving
        self._checkpoint_saver = None
        latest_remote_file_name = None
        if save_folder is not None:
            _, _, parsed_save_folder = parse_uri(save_folder)
    
            # If user passes a URI with s3:// and a bucket_name, but no other
            # path then we assume they just want their checkpoints saved directly in their
            # bucket.
            if parsed_save_folder == '':
                folder = '.'
                remote_file_name = save_filename
                latest_remote_file_name = save_latest_filename
    
            # If they actually specify a path, then we use that for their local save path
            # and we prefix save_filename with that path for remote_file_name.
            else:
                folder = parsed_save_folder
                remote_file_name = str(Path(parsed_save_folder) / Path(save_filename))
                if save_latest_filename is not None:
                    latest_remote_file_name = str(Path(parsed_save_folder) / Path(save_latest_filename))
                else:
                    latest_remote_file_name = None
    
            self._checkpoint_saver = CheckpointSaver(
                folder=folder,
                filename=save_filename,
                remote_file_name=remote_file_name,
                latest_filename=save_latest_filename,
                latest_remote_file_name=latest_remote_file_name,
                overwrite=save_overwrite,
                weights_only=save_weights_only,
                save_interval=save_interval,
                num_checkpoints_to_keep=save_num_checkpoints_to_keep,
            )
            self.state.callbacks.append(self._checkpoint_saver)
    
        # The Engine
        self.engine = Engine(state=self.state, logger=self.logger, algorithm_passes=algorithm_passes)
    
        # Set the logger
        self.state.model.logger = self.logger
    
        # Run Event.INIT
        self.engine.run_event(Event.INIT)
    
        # Log gpus and nodes.
        device_name = self._device.__class__.__name__.lstrip('Device').lower()
        self.logger.log_hyperparameters({
            'num_nodes': int(dist.get_world_size() / dist.get_local_world_size()),
            f'num_{device_name}s_per_node': dist.get_local_world_size(),
        })
    
        if not isinstance(self.state.model, ComposerModel):
            raise ValueError('Provided model should be a subclass of ComposerModel.')
    
        # After running Event.INIT, then set the "optional" elements of state that could be passed in on FIT instead of INIT
        # Setting these attributes here ensures that algorithms do not depend on unavailable attributes during Event.INIT
    
        # Metrics and Evaluators
        # Set state.train_metrics and state.eval_metrics here to allow callbacks / algs to potentially
        # change the model, which could change what metrics are computed
        self.state.train_metrics = deepcopy(self.state.model.get_metrics(is_train=True))
        self.state.eval_metrics = {}
        if eval_dataloader is None:
            evaluators: List[Evaluator] = []
        else:
            eval_metrics = deepcopy(self.state.model.get_metrics(is_train=False))
            model_metric_names = [str(k) for k in eval_metrics.keys()]
    
            evaluators = [
                ensure_evaluator(evaluator, default_metric_names=model_metric_names)
                for evaluator in ensure_tuple(eval_dataloader)
            ]
    
            # match metric names to model metrics
            self.state.eval_metrics = {
                evaluator.label: _filter_metrics(eval_metrics, evaluator.metric_names) for evaluator in evaluators
            }
    
            _set_evaluator_interval_and_subset_num_batches(
                evaluators=evaluators,
                eval_interval=eval_interval,
                subset_num_batches=eval_subset_num_batches,
            )
        if len(evaluators) == 0:
            if eval_subset_num_batches != -1:
                raise ValueError('Specifying `eval_subset_num_batches` without an `eval_dataloader` has no effect.')
            if eval_interval != 1:
                raise ValueError('Specifying `eval_interval` without an `eval_dataloader` has no effect.')
    
        self.state.evaluators = evaluators
    
        # Train Dataloader
        self._train_data_spec = None if train_dataloader is None else ensure_data_spec(train_dataloader)
        if self._train_data_spec is not None:
            self.state.set_dataloader(self._train_data_spec.dataloader, train_dataloader_label,
                                      train_subset_num_batches)
            if isinstance(self._device, DeviceTPU):
                self.state.train_dataloader = pl.MpDeviceLoader(self.state.dataloader, xm.xla_device())
            else:
                self.state.train_dataloader = self.state.dataloader
    
        # Max Duration
        if max_duration is not None:
            self.state.max_duration = ensure_time(max_duration, TimeUnit.EPOCH)
    
        self.logger.log_hyperparameters({'rank_zero_seed': rank_zero_seed})
    
        # Schedulers
        self.state.schedulers = _compile_schedulers(schedulers, self.state, scale_schedule_ratio)
        if scale_schedule_ratio != 1.0:
            if len(self.state.schedulers) == 0:
                raise ValueError('Specifying `scale_schedule_ratio` without `schedulers` has no effect.')
            self.state.max_duration = _scale_max_duration_by_ssr(scale_schedule_ratio, self.state.max_duration)
    
        if step_schedulers_every_batch is None:
            self._scheduler_step_frequency = _get_default_scheduler_frequency(schedulers)
        else:
            self._scheduler_step_frequency = TimeUnit.BATCH if step_schedulers_every_batch else TimeUnit.EPOCH
    
        # Some algorithms require specific settings
        self._backwards_create_graph = any(map(lambda x: x.backwards_create_graph, self.state.algorithms))
        self._find_unused_parameters = any(map(lambda x: x.find_unused_parameters, self.state.algorithms))
        self._ddp_sync_strategy = _get_ddp_sync_strategy(ddp_sync_strategy, self._find_unused_parameters)
    
        # If using DDP or DeepSpeed, we need to wrap the ComposerModel
        # But store a reference to the original model for functions like `eval_forward`, `get_metrics`, etc.
        self._original_model = self.state.model
        if not isinstance(self._original_model, ComposerModel):
            raise ValueError('self.state.model must be a subclass of ComposerModel.')
    
        # Configure Deepspeed
        if self.state.deepspeed_config is not None:
            for callback in self.state.callbacks:
                if isinstance(callback, GradMonitor):
                    raise ValueError('GradMonitor is not supported with DeepSpeed because DeepSpeed clears '
                                     'the gradients before in the last call to .backward see: '
                                     'https://github.com/microsoft/DeepSpeed/issues/2329 for more details.')
    
            try:
                import deepspeed
            except ImportError as e:
>               raise MissingConditionalImportError(
                    extra_deps_group='deepspeed',
                    conda_package='deepspeed>=0.5.5',
                    conda_channel=None,
                ) from e
E               composer.utils.import_helpers.MissingConditionalImportError: Composer was installed without deepspeed support. To use deepspeed related packages, with Composer, run `pip install 'mosaicml[deepspeed]'` if using pip or `pip install deepspeed>=0.5.5` if using Anaconda.

/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1161: MissingConditionalImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809361-awesome-cicada
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
________________ TestTrainerInitOrFit.test_fsdp[Precision.BF16] ________________

self = <tests.trainer.test_trainer.TestTrainerInitOrFit object at 0x7f6921785330>
model = SimpleModel(
  (train_metrics): Accuracy()
  (val_metrics): MetricCollection(
    (CrossEntropy): CrossEntropy()
    (...rapped_module): FlattenParamsWrapper(
      (_fpw_module): Linear(in_features=5, out_features=2, bias=True)
    )
  )
)
precision = <Precision.BF16: 'bf16'>, max_duration = Time(1, TimeUnit.EPOCH)
train_dataloader = <torch.utils.data.dataloader.DataLoader object at 0x7f67b0e8ebc0>

    @pytest.mark.gpu
    @pytest.mark.skipif(version.parse(torch.__version__) < version.parse('1.12.0'),
                        reason='requires PyTorch 1.12 or higher')
    @pytest.mark.parametrize('precision', list(Precision))
    def test_fsdp(
        self,
        model: ComposerModel,
        precision: Precision,
        max_duration: Time[int],
        train_dataloader: DataLoader,
    ):
    
        fsdp_config = {
            'sharding_strategy': 'FULL_SHARD',
            'min_params': 1e8,
            'cpu_offload': False,
            'mixed_precision': 'DEFAULT',
            'backward_prefetch': 'BACKWARD_PRE',
            'activation_checkpointing': False,
            'activation_cpu_offload': False,
            'verbose': False
        }
    
        # Need to catch the case where we try to train
        # with precision FP16.
        ctx = contextlib.nullcontext()
        should_error = False
        if precision == Precision.FP16:
            ctx = pytest.raises(ValueError, match='FP16 precision is only supported when training with DeepSpeed.')
            should_error = True
    
        with ctx:
            trainer = Trainer(
                model=model,
                precision=precision,
                fsdp_config=fsdp_config,
                max_duration=max_duration,
                train_dataloader=train_dataloader,
            )
    
        if not should_error:
            assert is_model_fsdp(trainer.state.model)
    
            assert trainer.state.fsdp_enabled
>           trainer.fit()

/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_trainer.py:455: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1634: in fit
    self._train_loop()
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1792: in _train_loop
    total_loss_dict = self._train_batch(use_grad_scaling)
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1960: in _train_batch
    optimizer.step(closure=lambda **kwargs: self._train_microbatches(
/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/torch/optim/optimizer.py:113: in wrapper
    return func(*args, **kwargs)
/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27: in decorate_context
    return func(*args, **kwargs)
/home/shie44167/workspace/1_read_codes/composer/composer/optim/decoupled_weight_decay.py:120: in step
    loss = closure()
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1960: in <lambda>
    optimizer.step(closure=lambda **kwargs: self._train_microbatches(
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:2034: in _train_microbatches
    microbatch_loss_dict = self._train_microbatch(use_grad_scaling, current_batch_size, is_final_microbatch)
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:2079: in _train_microbatch
    with get_precision_context(self.state.precision):
/home/shie44167/miniconda3/envs/composer/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/home/shie44167/workspace/1_read_codes/composer/composer/core/precision.py:65: in get_precision_context
    with torch.cuda.amp.autocast(enabled=True, dtype=torch.bfloat16):
/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/torch/cuda/amp/autocast_mode.py:25: in __init__
    super().__init__("cuda", enabled=enabled, dtype=dtype, cache_enabled=cache_enabled)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <torch.cuda.amp.autocast_mode.autocast object at 0x7f67b0e8e560>
device_type = 'cuda', dtype = torch.bfloat16, enabled = True
cache_enabled = True

    def __init__(self, device_type : str,
                 dtype : Optional[_dtype] = None,
                 enabled : bool = True,
                 cache_enabled : Optional[bool] = None):
        if torch._jit_internal.is_scripting():
            self._enabled = enabled
            self.device = device_type
            self.fast_dtype = dtype
            # TODO: support get_autocast_gpu/cpu_dtype
            assert dtype is not None
            return
        self.device = device_type
        if self.device == 'cuda':
            self.fast_dtype = torch.get_autocast_gpu_dtype()
        elif self.device == 'cpu':
            self.fast_dtype = torch.get_autocast_cpu_dtype()
        elif self.device == 'xpu':
            self.fast_dtype = torch.xpu.get_autocast_xpu_dtype()  # type: ignore[attr-defined]
        else:
            raise RuntimeError('User specified autocast device_type must be \'cuda\' or \'cpu\'')
        self._cache_enabled = torch.is_autocast_cache_enabled()
        if torch.cuda.amp.common.amp_definitely_not_available() and self.device == 'cuda':
            warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
            enabled = False
        if dtype is not None:
            self.fast_dtype = dtype
        if cache_enabled is not None:
            self._cache_enabled = cache_enabled
    
        if self.device == 'cpu':
            supported_dtype = [torch.bfloat16]
            if self.fast_dtype not in supported_dtype:
                error_message = 'In CPU autocast, but the target dtype is not supported. Disabling autocast.\n'
                error_message += 'CPU Autocast only supports dtype of torch.bfloat16 currently.'
                warnings.warn(error_message)
                enabled = False
        if self.device == 'xpu':
            supported_dtype = [torch.bfloat16, torch.float16]
            if self.fast_dtype not in supported_dtype:
                error_message = 'In XPU autocast, but the target dtype is not supported. Disabling autocast.\n'
                error_message += 'XPU Autocast only supports dtype of torch.bfloat16 currently.'
                warnings.warn(error_message)
                enabled = False
        if self.device == 'cuda':
            if self.fast_dtype == torch.bfloat16 and not torch.cuda.is_bf16_supported():
>               raise RuntimeError('Current CUDA Device does not support bfloat16. Please switch dtype to float16.')
E               RuntimeError: Current CUDA Device does not support bfloat16. Please switch dtype to float16.

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/torch/amp/autocast_mode.py:221: RuntimeError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
----------------------------- Captured stderr call -----------------------------
******************************
Config:
num_gpus_per_node: 1
num_nodes: 1
rank_zero_seed: 0

******************************
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809361-whispering-tortoise
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
INFO     composer.trainer.trainer:trainer.py:1274 Setting seed to 0
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:1734 Using precision Precision.BF16
DEBUG    composer.trainer.trainer:trainer.py:1693 Spinning the dataloaders
--------------------------- Captured stderr teardown ---------------------------






___________ TestTrainerInitOrFit.test_precision[gpu-Precision.BF16] ____________

self = <tests.trainer.test_trainer.TestTrainerInitOrFit object at 0x7f6921786290>
model = SimpleModel(
  (train_metrics): Accuracy()
  (val_metrics): MetricCollection(
    (CrossEntropy): CrossEntropy()
    (...)
  (fc1): Linear(in_features=1, out_features=5, bias=True)
  (fc2): Linear(in_features=5, out_features=2, bias=True)
)
precision = <Precision.BF16: 'bf16'>, device = 'gpu'
train_dataloader = <torch.utils.data.dataloader.DataLoader object at 0x7f67b0edf130>
max_duration = Time(1, TimeUnit.EPOCH)

    @pytest.mark.parametrize('precision', list(Precision))
    @pytest.mark.parametrize('device', ['cpu', pytest.param('gpu', marks=pytest.mark.gpu)])
    def test_precision(
        self,
        model: ComposerModel,
        precision: Precision,
        device: str,
        train_dataloader: DataLoader,
        max_duration: Time[int],
    ):
        # Copy the model so the fit_trainer can start with the same parameter values as the init_trainer
        copied_model = copy.deepcopy(model)
    
        should_error = False
        ctx = contextlib.nullcontext()
        if device == 'cpu' and precision != Precision.FP32:
            ctx = pytest.raises(ValueError, match='not supported for CPU training.')
            should_error = True
        elif precision == Precision.FP16:
            ctx = pytest.raises(ValueError, match='FP16 precision is only supported when training with DeepSpeed.')
            should_error = True
    
        with ctx:
            # Train once with the precision param on Trainer.__init__()
            init_trainer = Trainer(
                model=model,
                max_duration=max_duration,
                train_dataloader=train_dataloader,
                precision=precision,
                device=device,
            )
    
        if not should_error:
    
>           init_trainer.fit()

/home/shie44167/workspace/1_read_codes/composer/tests/trainer/test_trainer.py:530: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1634: in fit
    self._train_loop()
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1792: in _train_loop
    total_loss_dict = self._train_batch(use_grad_scaling)
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1960: in _train_batch
    optimizer.step(closure=lambda **kwargs: self._train_microbatches(
/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/torch/optim/optimizer.py:113: in wrapper
    return func(*args, **kwargs)
/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27: in decorate_context
    return func(*args, **kwargs)
/home/shie44167/workspace/1_read_codes/composer/composer/optim/decoupled_weight_decay.py:120: in step
    loss = closure()
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:1960: in <lambda>
    optimizer.step(closure=lambda **kwargs: self._train_microbatches(
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:2034: in _train_microbatches
    microbatch_loss_dict = self._train_microbatch(use_grad_scaling, current_batch_size, is_final_microbatch)
/home/shie44167/workspace/1_read_codes/composer/composer/trainer/trainer.py:2079: in _train_microbatch
    with get_precision_context(self.state.precision):
/home/shie44167/miniconda3/envs/composer/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/home/shie44167/workspace/1_read_codes/composer/composer/core/precision.py:65: in get_precision_context
    with torch.cuda.amp.autocast(enabled=True, dtype=torch.bfloat16):
/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/torch/cuda/amp/autocast_mode.py:25: in __init__
    super().__init__("cuda", enabled=enabled, dtype=dtype, cache_enabled=cache_enabled)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <torch.cuda.amp.autocast_mode.autocast object at 0x7f67b0edfb80>
device_type = 'cuda', dtype = torch.bfloat16, enabled = True
cache_enabled = True

    def __init__(self, device_type : str,
                 dtype : Optional[_dtype] = None,
                 enabled : bool = True,
                 cache_enabled : Optional[bool] = None):
        if torch._jit_internal.is_scripting():
            self._enabled = enabled
            self.device = device_type
            self.fast_dtype = dtype
            # TODO: support get_autocast_gpu/cpu_dtype
            assert dtype is not None
            return
        self.device = device_type
        if self.device == 'cuda':
            self.fast_dtype = torch.get_autocast_gpu_dtype()
        elif self.device == 'cpu':
            self.fast_dtype = torch.get_autocast_cpu_dtype()
        elif self.device == 'xpu':
            self.fast_dtype = torch.xpu.get_autocast_xpu_dtype()  # type: ignore[attr-defined]
        else:
            raise RuntimeError('User specified autocast device_type must be \'cuda\' or \'cpu\'')
        self._cache_enabled = torch.is_autocast_cache_enabled()
        if torch.cuda.amp.common.amp_definitely_not_available() and self.device == 'cuda':
            warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
            enabled = False
        if dtype is not None:
            self.fast_dtype = dtype
        if cache_enabled is not None:
            self._cache_enabled = cache_enabled
    
        if self.device == 'cpu':
            supported_dtype = [torch.bfloat16]
            if self.fast_dtype not in supported_dtype:
                error_message = 'In CPU autocast, but the target dtype is not supported. Disabling autocast.\n'
                error_message += 'CPU Autocast only supports dtype of torch.bfloat16 currently.'
                warnings.warn(error_message)
                enabled = False
        if self.device == 'xpu':
            supported_dtype = [torch.bfloat16, torch.float16]
            if self.fast_dtype not in supported_dtype:
                error_message = 'In XPU autocast, but the target dtype is not supported. Disabling autocast.\n'
                error_message += 'XPU Autocast only supports dtype of torch.bfloat16 currently.'
                warnings.warn(error_message)
                enabled = False
        if self.device == 'cuda':
            if self.fast_dtype == torch.bfloat16 and not torch.cuda.is_bf16_supported():
>               raise RuntimeError('Current CUDA Device does not support bfloat16. Please switch dtype to float16.')
E               RuntimeError: Current CUDA Device does not support bfloat16. Please switch dtype to float16.

/home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/torch/amp/autocast_mode.py:221: RuntimeError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
----------------------------- Captured stderr call -----------------------------
******************************
Config:
num_gpus_per_node: 1
num_nodes: 1
rank_zero_seed: 0

******************************
------------------------------ Captured log call -------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:940 Run name: 1669809368-literate-goldfish
INFO     composer.trainer.trainer:trainer.py:94 Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
INFO     composer.trainer.trainer:trainer.py:1274 Setting seed to 0
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
INFO     composer.trainer.trainer:trainer.py:1734 Using precision Precision.BF16
DEBUG    composer.trainer.trainer:trainer.py:1693 Spinning the dataloaders
________________ test_gpu_huggingface_export_for_inference_onnx ________________

    @pytest.mark.gpu
    def test_gpu_huggingface_export_for_inference_onnx():
        pytest.importorskip('onnx')
        pytest.importorskip('onnxruntime')
        pytest.importorskip('transformers')
    
        import onnx
        import onnx.checker
        import onnxruntime as ort
        import transformers
    
        from composer.functional import apply_fused_layernorm
        from composer.models import HuggingFaceModel
    
        # HuggingFace Bert Model
        # dummy sequence batch with 2 labels, 32 sequence length, and 30522 (bert) vocab size).
        input_ids = torch.randint(low=0, high=30522, size=(2, 32))
        labels = torch.randint(low=0, high=1, size=(2,))
        token_type_ids = torch.zeros(size=(2, 32), dtype=torch.int64)
        attention_mask = torch.randint(low=0, high=1, size=(2, 32))
        sample_input = {
            'input_ids': input_ids,
            'labels': labels,
            'token_type_ids': token_type_ids,
            'attention_mask': attention_mask,
        }
        dynamic_axes = {
            'input_ids': {
                0: 'batch_size',
                1: 'seq_len'
            },
            'labels': {
                0: 'batch_size'
            },
            'token_type_ids': {
                0: 'batch_size',
                1: 'seq_len'
            },
            'attention_mask': {
                0: 'batch_size',
                1: 'seq_len'
            },
        }
        # non pretrained model to avoid a slow test that downloads the weights.
        config = transformers.AutoConfig.from_pretrained('bert-base-uncased', num_labels=2, hidden_act='gelu_new')
        hf_model = transformers.AutoModelForSequenceClassification.from_config(config)  # type: ignore (thirdparty)
    
        model = HuggingFaceModel(hf_model)
    
        optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
        apply_gated_linear_units(model, optimizer)
>       apply_fused_layernorm(model, optimizer)

/home/shie44167/workspace/1_read_codes/composer/tests/utils/test_inference.py:204: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/shie44167/workspace/1_read_codes/composer/composer/algorithms/fused_layernorm/fused_layernorm.py:49: in apply_fused_layernorm
    check_if_apex_installed()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def check_if_apex_installed():
        if not APEX_INSTALLED:
>           raise ImportError(
                'https://github.com/NVIDIA/apex is not installed. The Fused LayerNorm algorithm cannot be applied. The MosaicML Docker Images (https://hub.docker.com/r/mosaicml/pytorch) contain a copy of APEX for easy use.'
            )
E           ImportError: https://github.com/NVIDIA/apex is not installed. The Fused LayerNorm algorithm cannot be applied. The MosaicML Docker Images (https://hub.docker.com/r/mosaicml/pytorch) contain a copy of APEX for easy use.

/home/shie44167/workspace/1_read_codes/composer/composer/algorithms/fused_layernorm/fused_layernorm.py:30: ImportError
------------------------------ Captured log setup ------------------------------
INFO     composer.utils.reproducibility:reproducibility.py:159 Setting seed to 0
------------------------------ Captured log call -------------------------------
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 0 new parameters to parameter group #0
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 6 new parameters to parameter group #0
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 0 new parameters to parameter group #0
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 6 new parameters to parameter group #0
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 0 new parameters to parameter group #0
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 6 new parameters to parameter group #0
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 0 new parameters to parameter group #0
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 6 new parameters to parameter group #0
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 0 new parameters to parameter group #0
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 6 new parameters to parameter group #0
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 0 new parameters to parameter group #0
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 6 new parameters to parameter group #0
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 0 new parameters to parameter group #0
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 6 new parameters to parameter group #0
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 0 new parameters to parameter group #0
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 6 new parameters to parameter group #0
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 0 new parameters to parameter group #0
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 6 new parameters to parameter group #0
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 0 new parameters to parameter group #0
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 6 new parameters to parameter group #0
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 0 new parameters to parameter group #0
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 6 new parameters to parameter group #0
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 0 new parameters to parameter group #0
DEBUG    composer.utils.module_surgery:module_surgery.py:401 adding 6 new parameters to parameter group #0
INFO     composer.algorithms.gated_linear_units.gated_linear_units:gated_linear_units.py:137 Successfully replaced 24 of BertIntermediate and BertOutput with a GatedLinearUnit.
=============================== warnings summary ===============================
../../../miniconda3/envs/composer/lib/python3.10/site-packages/pytest_codeblocks/main.py:82
  /home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/pytest_codeblocks/main.py:82: DeprecationWarning: pytest-codeblocks:skip is deprecated. Use pytest.mark.skip
    warnings.warn(

../../../miniconda3/envs/composer/lib/python3.10/site-packages/jupyter_client/connect.py:27
  /home/shie44167/miniconda3/envs/composer/lib/python3.10/site-packages/jupyter_client/connect.py:27: DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs
  given by the platformdirs library.  To remove this warning and
  see the appropriate new directories, set the environment variable
  `JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.
  The use of platformdirs will be the default in `jupyter_core` v6
    from jupyter_core.paths import jupyter_data_dir

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED composer/algorithms/fused_layernorm/README.md::line 42
FAILED docs/source/method_cards/fused_layernorm.md::line 42
FAILED tests/test_events.py::TestEventCalls::test_event_calls[1ep-gpu-ddp-1]
FAILED tests/test_events.py::TestEventCalls::test_event_calls[1ba-gpu-ddp-1]
FAILED tests/test_notebooks.py::test_notebook[gpu-/home/shie44167/workspace/1_read_codes/composer/composer/../examples/auto_grad_accum.ipynb]
FAILED tests/test_notebooks.py::test_notebook[gpu-/home/shie44167/workspace/1_read_codes/composer/composer/../examples/early_stopping.ipynb]
FAILED tests/test_notebooks.py::test_notebook[gpu-/home/shie44167/workspace/1_read_codes/composer/composer/../examples/ffcv_dataloaders.ipynb]
FAILED tests/test_notebooks.py::test_notebook[gpu-/home/shie44167/workspace/1_read_codes/composer/composer/../examples/getting_started.ipynb]
FAILED tests/test_notebooks.py::test_notebook[gpu-/home/shie44167/workspace/1_read_codes/composer/composer/../examples/huggingface_models.ipynb]
FAILED tests/test_precision.py::test_train_precision_memory[Precision.AMP] - ...
FAILED tests/test_precision.py::test_train_precision_memory[Precision.BF16]
FAILED tests/test_precision.py::test_eval_precision_memory[Precision.AMP] - R...
FAILED tests/test_precision.py::test_eval_precision_memory[Precision.BF16] - ...
FAILED tests/test_precision.py::test_predict_precision_memory[Precision.BF16]
FAILED tests/algorithms/test_algorithm_resumption.py::test_algorithm_resumption[FusedLayerNorm]
FAILED tests/algorithms/test_algorithms_train.py::test_algorithm_trains[FusedLayerNorm]
FAILED tests/algorithms/test_fused_layernorm.py::test_fused_layernorm_functional[gpu]
FAILED tests/algorithms/test_fused_layernorm.py::test_fused_layernorm_algorithm[gpu]
FAILED tests/trainer/test_checkpoint.py::TestCheckpointResumption::test_resumption[None-1ep-ep{epoch}-rank{rank}.pt-ep1-rank{rank}.pt-latest-rank{rank}.pt-deepspeed-zero1-1]
FAILED tests/trainer/test_checkpoint.py::TestCheckpointResumption::test_resumption[None-1ep-ep{epoch}-rank{rank}.pt-ep1-rank{rank}.pt-latest-rank{rank}.pt-deepspeed-zero2-1]
FAILED tests/trainer/test_checkpoint.py::TestCheckpointResumption::test_resumption[42-1ep-ep{epoch}-rank{rank}.pt-ep1-rank{rank}.pt-ep2-rank{rank}.pt-deepspeed-zero1-1]
FAILED tests/trainer/test_checkpoint.py::TestCheckpointResumption::test_resumption[42-1ep-ep{epoch}-rank{rank}.pt-ep1-rank{rank}.pt-ep2-rank{rank}.pt-deepspeed-zero2-1]
FAILED tests/trainer/test_checkpoint.py::TestCheckpointResumption::test_resumption[42-1ep-ep{epoch}-rank{rank}.tgz-ep1-rank{rank}.tgz-ep2-rank{rank}.tgz-deepspeed-zero1-1]
FAILED tests/trainer/test_checkpoint.py::TestCheckpointResumption::test_resumption[42-1ep-ep{epoch}-rank{rank}.tgz-ep1-rank{rank}.tgz-ep2-rank{rank}.tgz-deepspeed-zero2-1]
FAILED tests/trainer/test_checkpoint.py::TestCheckpointResumption::test_resumption[42-2ba-ba{batch}-rank{rank}.pt-ba4-rank{rank}.pt-ba8-rank{rank}.pt-deepspeed-zero1-1]
FAILED tests/trainer/test_checkpoint.py::TestCheckpointResumption::test_resumption[42-2ba-ba{batch}-rank{rank}.pt-ba4-rank{rank}.pt-ba8-rank{rank}.pt-deepspeed-zero2-1]
FAILED tests/trainer/test_checkpoint.py::TestCheckpointResumption::test_resumption[42-1ba-ba{batch}-rank{rank}.pt-ba5-rank{rank}.pt-ba8-rank{rank}.pt-deepspeed-zero1-1]
FAILED tests/trainer/test_checkpoint.py::TestCheckpointResumption::test_resumption[42-1ba-ba{batch}-rank{rank}.pt-ba5-rank{rank}.pt-ba8-rank{rank}.pt-deepspeed-zero2-1]
FAILED tests/trainer/test_checkpoint.py::TestCheckpointResumption::test_resumption[42-2ba-ba{batch}-rank{rank}.pt-ba6-rank{rank}.pt-ba8-rank{rank}.pt-deepspeed-zero1-1]
FAILED tests/trainer/test_checkpoint.py::TestCheckpointResumption::test_resumption[42-2ba-ba{batch}-rank{rank}.pt-ba6-rank{rank}.pt-ba8-rank{rank}.pt-deepspeed-zero2-1]
FAILED tests/trainer/test_checkpoint.py::test_rotate_checkpoints[deepspeed-zero0-1]
FAILED tests/trainer/test_checkpoint.py::test_rotate_checkpoints[deepspeed-zero1-1]
FAILED tests/trainer/test_checkpoint.py::test_rotate_checkpoints[deepspeed-zero2-1]
FAILED tests/trainer/test_ddp.py::test_ddp[1-deepspeed] - composer.utils.impo...
FAILED tests/trainer/test_trainer.py::TestTrainerInitOrFit::test_deepspeed[Precision.AMP]
FAILED tests/trainer/test_trainer.py::TestTrainerInitOrFit::test_deepspeed[Precision.FP16]
FAILED tests/trainer/test_trainer.py::TestTrainerInitOrFit::test_deepspeed[Precision.FP32]
FAILED tests/trainer/test_trainer.py::TestTrainerInitOrFit::test_deepspeed[Precision.BF16]
FAILED tests/trainer/test_trainer.py::TestTrainerInitOrFit::test_fsdp[Precision.BF16]
FAILED tests/trainer/test_trainer.py::TestTrainerInitOrFit::test_precision[gpu-Precision.BF16]
FAILED tests/utils/test_inference.py::test_gpu_huggingface_export_for_inference_onnx
ERROR tests/test_notebooks.py::test_notebook[gpu-/home/shie44167/workspace/1_read_codes/composer/composer/../examples/training_without_local_storage.ipynb]
ERROR tests/trainer/test_trainer.py::TestFFCVDataloaders::test_ffcv[gpu-amp]
= 41 failed, 179 passed, 10 skipped, 2892 deselected, 11 xfailed, 2 warnings, 2 errors in 668.32s (0:11:08) =
make: *** [Makefile:14: test-gpu] Error 1
